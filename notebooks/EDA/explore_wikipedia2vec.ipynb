{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install necessary packages\n",
    "- !pip install gensim\n",
    "- !pip install wikipedia2vec\n",
    "- !pip install pyemd # Necessary for one of Gensim's functions\n",
    "\n",
    "**Update**: these are installed via Poetry, so if you set up your environment per the root readme and are correctly using that environment as your Jupyter Notebook kernel, the packages will be available to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Gensim and Word2Vec\n",
    "\n",
    "Following complete API documentation here:\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import test texts and Word2Vec pre-loaded model\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Word2Vec\n",
    "# See parameter explanations here:\n",
    "# https://github.com/kavgan/nlp-in-practice/blob/master/word2vec/Word2Vec.ipynb\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save word_vectors to disk\n",
    "# from gensim.test.utils import get_tmpfile\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# fname = get_tmpfile('vectors.kv')\n",
    "# word_vectors.save(fname)\n",
    "# word_vectors = KeyedVectors.load(fname, mmap='r')\n",
    "\n",
    "# # Load word_vectors from disk\n",
    "# # Possible for Wikipedia2Vec\n",
    "# from gensim.test.utils import datapath\n",
    "# wv_from_bin = KeyedVectors.load_word2vec_format(datapath(\"euclidean_vectors.bin\", binary=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Use API to download Glove wiki word embeddings\n",
    "import gensim.downloader as api\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483),\n",
       " ('monarch', 0.6843380928039551),\n",
       " ('throne', 0.6755735874176025),\n",
       " ('daughter', 0.6594556570053101),\n",
       " ('princess', 0.6520534753799438),\n",
       " ('prince', 0.6517034769058228),\n",
       " ('elizabeth', 0.6464517712593079),\n",
       " ('mother', 0.6311717629432678),\n",
       " ('emperor', 0.6106470823287964),\n",
       " ('wife', 0.6098655462265015)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar word with positive and negative inputs\n",
    "result = word_vectors.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8964555859565735),\n",
       " ('monarch', 0.8495979309082031),\n",
       " ('throne', 0.8447030782699585),\n",
       " ('princess', 0.8371668457984924),\n",
       " ('elizabeth', 0.835679292678833),\n",
       " ('daughter', 0.8348594307899475),\n",
       " ('prince', 0.8230059742927551),\n",
       " ('mother', 0.8154449462890625),\n",
       " ('margaret', 0.8147734999656677),\n",
       " ('father', 0.8100855350494385)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds most similar word using \"multiplicative combination objective\"\n",
    "# Less susceptible to one large distance dominating calculation\n",
    "result = word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannes/entity-disambiguation/.venv/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "# Returns least non-matching word of input words\n",
    "# Caution: If you input \"breakfast lunch dinner\", it will say \"lunch\"\n",
    "# is least similar so no ability to say \"All similar\"\n",
    "print(word_vectors.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8323495"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates word similarity score\n",
    "similarity = word_vectors.similarity('woman', 'man')\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.8798074722290039),\n",
       " ('rabbit', 0.7424426674842834),\n",
       " ('cats', 0.7323004007339478),\n",
       " ('monkey', 0.7288709878921509),\n",
       " ('pet', 0.7190139889717102),\n",
       " ('dogs', 0.7163872718811035),\n",
       " ('mouse', 0.6915250420570374),\n",
       " ('puppy', 0.6800068020820618),\n",
       " ('rat', 0.6641027331352234),\n",
       " ('spider', 0.6501135230064392)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns most similar words\n",
    "result = word_vectors.similar_by_word(\"cat\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4892687395218687"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computes \"Word Mover's Distance\" between two documents\n",
    "sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
    "similarity = word_vectors.wmdistance(sentence_obama, sentence_president)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.960464477539063e-08"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computes cosine distance between two words\n",
    "distance = word_vectors.distance('media', 'media')\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2534049153327942"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance = word_vectors.distance('media', 'press')\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6921039819717407"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance = word_vectors.distance('media', 'mob')\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7066633"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computes cosine similarity between two sets of words\n",
    "sim = word_vectors.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant'])\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "First 10\n",
      "[-0.16298   0.30141   0.57978   0.066548  0.45835  -0.15329   0.43258\n",
      " -0.89215   0.57747   0.36375 ]\n"
     ]
    }
   ],
   "source": [
    "# Each word has 100 length numbers as representation\n",
    "print(word_vectors['computer'].shape)\n",
    "print(\"First 10\")\n",
    "print(word_vectors['computer'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Wikipedia2Vec\n",
    "\n",
    "Following API usage here:\n",
    "https://wikipedia2vec.github.io/wikipedia2vec/usage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download word embeddings\n",
    "You must first download the word embeddings binary file (3.3GB)\n",
    "\n",
    "https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "\n",
    "Use *enwiki_20180420* - 100d(bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikipedia2vec import Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unzipped pkl file\n",
    "wiki2vec = Wikipedia2Vec.load(\"../../embeddings/enwiki_20180420_100d.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[-0.20645621  0.3541138  -0.0294481  -0.33744335 -0.08076815  0.31424952\n",
      "  0.19943002 -0.45241427  0.1767314   0.08421883  0.18743327 -0.15441522\n",
      "  0.7555824  -0.3163147  -0.1272997  -0.16718598  0.26394138 -0.12993027\n",
      "  0.26369524 -0.15656409  0.31780142 -0.21851811 -0.246363    0.09481585\n",
      " -0.824227   -0.32929772 -0.08541542 -0.42382434  0.25865236  0.48900202\n",
      "  0.2663292   0.30553964 -0.24619317  0.4986544  -0.15859959 -0.05226322\n",
      "  0.15892026  0.21789268  0.1005047  -0.04396052 -0.13266805  0.14376648\n",
      " -0.07025098  0.07240359  0.26175997  0.2744427  -0.05514829  0.3889919\n",
      " -0.15309823  0.14552292  0.16997342  0.44799    -0.19526581 -0.10786294\n",
      " -0.32927316  0.3289106   0.02902091  0.08773877 -0.41468772  0.26678678\n",
      " -0.1374677  -0.33667436  0.05344973  0.27092975  0.38892925 -0.43767536\n",
      "  0.28642192 -0.09463934  0.02864233 -0.04812264 -0.5708012   0.48663247\n",
      " -0.07153141  0.5161     -0.29984742  0.12222655  0.06087735  0.09465893\n",
      " -0.32330158  0.00459747 -0.29446656 -0.80294424  0.33227766  0.26280716\n",
      "  0.14379331  0.3225326   0.6658606  -0.44036752  0.5713877  -0.34318823\n",
      "  0.26279572  0.5080736   0.14139684  0.38665375 -0.4247191  -0.39772087\n",
      " -0.15295443  0.21362954 -0.6706313   0.00557745]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve word vector\n",
    "wv = wiki2vec.get_word_vector(\"royalty\")\n",
    "print(len(wv))\n",
    "print(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[-0.20736943 -0.1413487   0.44806942 -0.15780485  0.03426249 -1.496996\n",
      "  0.6636046   1.0935112  -0.28068435 -1.7422425   0.28946415 -0.1224269\n",
      "  1.359793   -2.1443315  -1.2891554   0.5883908  -1.4934471   0.28643036\n",
      " -0.5680678   0.06121244  0.14584215 -0.5493008  -0.5274965  -0.21286193\n",
      " -2.531544   -1.0664226   0.6852317  -0.6560998   1.0964793   1.7911379\n",
      " -0.88538927 -0.6312805  -0.579102    0.2503235   0.581396   -1.0452244\n",
      "  0.99863523 -1.1343025   1.5228372  -0.34984437 -0.30327943 -0.69956243\n",
      " -0.27598628  0.8382855   1.158086   -0.19607222 -0.23079026  0.7163011\n",
      " -0.02645348 -0.53512233  0.8909849   0.33066076 -0.09964236 -0.59745216\n",
      " -0.70949936  0.24142893  0.28429118  0.13206743 -1.3893279  -0.00942595\n",
      " -0.16754963 -0.6323121  -0.41165853  0.28699186  0.63819945 -1.1207265\n",
      "  0.57338834 -0.12862757 -0.57689637  0.32655638 -0.5370033   0.31795335\n",
      " -0.24858339  0.17761594 -0.53856456 -0.717328    0.29654366 -0.18517432\n",
      " -0.38363308  0.10567519 -0.41610453 -1.4170125  -0.06065467 -0.3591297\n",
      "  1.0985152  -1.0076818  -0.48925364 -0.43357962 -0.62458825 -0.00557246\n",
      " -0.65517527  0.3928027   0.43840042  0.8373779  -0.19576903 -0.06179139\n",
      " -0.87699264  1.3264351  -0.02215926 -0.3786939 ]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve entity vector\n",
    "wv = wiki2vec.get_entity_vector(\"Queen Elizabeth II\")\n",
    "print(len(wv))\n",
    "print(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Word royalty>\n",
      "<class 'wikipedia2vec.dictionary.Word'>\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a word\n",
    "print(wiki2vec.get_word('royalty'))\n",
    "print(type(wiki2vec.get_word('royalty')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Entity Metropolitan Museum of Art>\n",
      "<class 'wikipedia2vec.dictionary.Entity'>\n"
     ]
    }
   ],
   "source": [
    "# Retrieve an entity\n",
    "print(wiki2vec.get_entity('Metropolitan Museum'))\n",
    "print(type(wiki2vec.get_entity('Metropolitan Museum')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Entity Harvard University>\n",
      "<class 'wikipedia2vec.dictionary.Entity'>\n"
     ]
    }
   ],
   "source": [
    "# Retrieve an entity\n",
    "print(wiki2vec.get_entity('Harvard University'))\n",
    "print(type(wiki2vec.get_entity('Metropolitan Museum')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.92 s, sys: 4.1 s, total: 8.02 s\n",
      "Wall time: 15.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<Word yoda>, 0.99999994),\n",
       " (<Word kenobi>, 0.7954376),\n",
       " (<Word darth>, 0.7514157),\n",
       " (<Entity Yoda>, 0.7504513),\n",
       " (<Word yularen>, 0.74208754),\n",
       " (<Word saesee>, 0.7390899),\n",
       " (<Word jedi>, 0.7362622),\n",
       " (<Word sidious>, 0.73391825),\n",
       " (<Word anakin>, 0.7304383),\n",
       " (<Word zarbon>, 0.7281125),\n",
       " (<Word ackbar>, 0.7240917),\n",
       " (<Word watto>, 0.7238953),\n",
       " (<Word unduli>, 0.7219176),\n",
       " (<Word numberman>, 0.7216795),\n",
       " (<Word yarael>, 0.7216082),\n",
       " (<Word mystel>, 0.7211916),\n",
       " (<Word kaito>, 0.7205416),\n",
       " (<Word vilmarh>, 0.7194575),\n",
       " (<Word threepio>, 0.71926695),\n",
       " (<Word solusar>, 0.7167008),\n",
       " (<Word echuu>, 0.71605474),\n",
       " (<Word yoshi>, 0.71546626),\n",
       " (<Word jorus>, 0.715434),\n",
       " (<Entity X'Ting>, 0.71265334),\n",
       " (<Word joruus>, 0.71257347),\n",
       " (<Entity Qui-Gon Jinn>, 0.71158594),\n",
       " (<Word 3po>, 0.71024007),\n",
       " (<Word jabba>, 0.70992076),\n",
       " (<Word pandabubba>, 0.70945776),\n",
       " (<Word ghaleon>, 0.7088256),\n",
       " (<Entity C-3PO>, 0.70879513),\n",
       " (<Word yaddle>, 0.70696616),\n",
       " (<Word benjirou>, 0.7062665),\n",
       " (<Word dagobah>, 0.7049252),\n",
       " (<Word yajirobe>, 0.7042612),\n",
       " (<Entity Mace Windu>, 0.70330495),\n",
       " (<Word takeshi>, 0.701803),\n",
       " (<Word wishmaker>, 0.7016454),\n",
       " (<Word goutetsu>, 0.7015391),\n",
       " (<Word calrissian>, 0.700937),\n",
       " (<Word luffy>, 0.7008289),\n",
       " (<Word goku>, 0.7001841),\n",
       " (<Word asajj>, 0.69976586),\n",
       " (<Word kimijima>, 0.69953686),\n",
       " (<Entity List of Cowboy Bebop characters#Jet Black>, 0.6984393),\n",
       " (<Word shinichi>, 0.69780266),\n",
       " (<Word saleucami>, 0.6976341),\n",
       " (<Word kadokura>, 0.69756985),\n",
       " (<Word jonouchi>, 0.69744647),\n",
       " (<Word kaisuke>, 0.69723153),\n",
       " (<Word obi>, 0.6968853),\n",
       " (<Word kouji>, 0.6965228),\n",
       " (<Word tiin>, 0.6960073),\n",
       " (<Word tenjuro>, 0.69595736),\n",
       " (<Word shouhei>, 0.6954689),\n",
       " (<Word utahoshi>, 0.6951125),\n",
       " (<Word aayla>, 0.6948383),\n",
       " (<Word nightbrother>, 0.6947813),\n",
       " (<Word ryo>, 0.69388497),\n",
       " (<Word lamunade>, 0.69372207),\n",
       " (<Word vader>, 0.6929206),\n",
       " (<Word buraiking>, 0.69271183),\n",
       " (<Word boushh>, 0.6919546),\n",
       " (<Word pikkon>, 0.6918757),\n",
       " (<Word kotaro>, 0.69174206),\n",
       " (<Word avdol>, 0.69173867),\n",
       " (<Word makuro>, 0.69162965),\n",
       " (<Word muunilinst>, 0.69130003),\n",
       " (<Word itsuji>, 0.69103235),\n",
       " (<Entity Obi-Wan Kenobi>, 0.6908472),\n",
       " (<Word luminara>, 0.690564),\n",
       " (<Word fukuide>, 0.6905604),\n",
       " (<Word hatenko>, 0.690179),\n",
       " (<Word luuke>, 0.6900398),\n",
       " (<Word wataru>, 0.68998224),\n",
       " (<Word raidÅ>, 0.6899461),\n",
       " (<Word eekway>, 0.68979466),\n",
       " (<Word pipotchi>, 0.68974864),\n",
       " (<Word chewbaboy>, 0.68973744),\n",
       " (<Word praxeum>, 0.68868935),\n",
       " (<Entity Jedi#Rank structure>, 0.68857116),\n",
       " (<Word kubariet>, 0.688451),\n",
       " (<Word tendou>, 0.68831927),\n",
       " (<Word dooku>, 0.68821967),\n",
       " (<Word matsuhiro>, 0.6876099),\n",
       " (<Word gosterro>, 0.6875092),\n",
       " (<Word buriburizaemon>, 0.68750566),\n",
       " (<Entity Star Wars Racer Revenge>, 0.686929),\n",
       " (<Word jouji>, 0.6864088),\n",
       " (<Word tarosuke>, 0.68634504),\n",
       " (<Word cybuster>, 0.68576837),\n",
       " (<Word zuckuss>, 0.68561566),\n",
       " (<Word jheese>, 0.68535066),\n",
       " (<Word hiro>, 0.6849714),\n",
       " (<Word iwagoro>, 0.6846497),\n",
       " (<Word brakiss>, 0.684547),\n",
       " (<Word tauntaun>, 0.68446106),\n",
       " (<Word roboboss>, 0.68431675),\n",
       " (<Word wesell>, 0.6840966),\n",
       " (<Entity R2-D2>, 0.6838638)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get most similar word\n",
    "similar_yoda = wiki2vec.most_similar(wiki2vec.get_word('yoda'), 100)\n",
    "similar_yoda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.1 s, sys: 1.49 s, total: 4.59 s\n",
      "Wall time: 4.52 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<Entity Harvard University>, 1.0000001),\n",
       " (<Entity John F. Kennedy School of Government>, 0.81384987),\n",
       " (<Word harvard>, 0.80841565),\n",
       " (<Entity Harvard College>, 0.7746782),\n",
       " (<Entity Signet society>, 0.75375855),\n",
       " (<Entity List of Harvard College freshman dormitories>, 0.7442793),\n",
       " (<Entity Harvard Faculty of Arts and Sciences>, 0.7426085),\n",
       " (<Entity Radcliffe Institute for Advanced Study>, 0.7415956),\n",
       " (<Entity Harvard Society of Fellows>, 0.7386711),\n",
       " (<Entity Cambridge, Massachusetts>, 0.73181486)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get most similar entity\n",
    "similar_harvard = wiki2vec.most_similar(wiki2vec.get_entity('Harvard University'), 10)\n",
    "similar_harvard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikipedia2vec.dictionary import Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<Entity Yoda>, 0.7504513),\n",
       " (<Entity X'Ting>, 0.71265334),\n",
       " (<Entity Qui-Gon Jinn>, 0.71158594)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve only entities from word\n",
    "yoda_entities = []\n",
    "for i in similar_yoda:\n",
    "#     print(type(i[0]))\n",
    "    if isinstance(i[0], Entity):\n",
    "        yoda_entities.append(i)\n",
    "    if len(yoda_entities) == 3:\n",
    "        break\n",
    "yoda_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gensim & Wikipedia2Vec Together\n",
    "\n",
    "You can use gensim's `load_word2vec_format` to work with wikipedia2vec directly. However, you have to use the `(txt)` file from wikipedia2vec to do this, not the `(bin)` file. Given the greater number of modules provided by gensim, this is likely the preferred path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models type\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 15s, sys: 16.7 s, total: 8min 32s\n",
      "Wall time: 9min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Use model type to load txt file\n",
    "w2v = KeyedVectors.load_word2vec_format(\"../../embeddings/enwiki_20180420_100d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent gensim versions you can load a subset starting from the front of the file using the optional limit parameter to load_word2vec_format(). So use limit=500000 to get the most-frequent 500,000 words' vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('saving', 0.7900023460388184),\n",
       " ('saved', 0.7763535976409912),\n",
       " ('reclaim', 0.7728390097618103),\n",
       " ('recover', 0.7695224285125732),\n",
       " ('help', 0.7515880465507507),\n",
       " ('redeem', 0.7505150437355042),\n",
       " ('restore', 0.7452740669250488),\n",
       " ('bring', 0.7452179789543152),\n",
       " ('destroy', 0.7413591742515564),\n",
       " ('keep', 0.740227997303009)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = w2v.most_similar(\"save\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8306491374969482),\n",
       " ('monarch', 0.7416261434555054),\n",
       " ('ENTITY/Queen_consort', 0.7348717451095581),\n",
       " ('laungshe', 0.7347309589385986),\n",
       " ('regnant', 0.7243735194206238),\n",
       " ('chelna', 0.7236213684082031),\n",
       " ('consort', 0.720160722732544),\n",
       " ('indlovukati', 0.7181541919708252),\n",
       " ('kamamalu', 0.7178552150726318),\n",
       " ('indlovukazi', 0.714848518371582)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = w2v.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7687104"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates word similarity score\n",
    "similarity = w2v.similarity('woman', 'man')\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5174522697925568"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate word distance\n",
    "distance = w2v.distance('media', 'press')\n",
    "distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
