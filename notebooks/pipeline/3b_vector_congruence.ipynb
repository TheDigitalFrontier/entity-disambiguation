{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Congruence via Entity Vector Similarity\n",
    "\n",
    "In this notebook, we calculate the cosine similarity measure between vectors of each candidate. The vectors will be retrieved from Wikipedia2vec's pre-trained API, which creates vectors for the entire Wikipedia page including its relational links. Comparing two vectors in this way thus lets us make a statement about similar pages and update our prior confidence based on that.\n",
    "\n",
    "#### Using Prior Confidence\n",
    "\n",
    "In Phase 3, we calculate congruence between candidates in pools for mentions in the same sentence. We then update these congruence metrics using prior confidence from Phase 2. This should adjust our pool by discounting congruence using the prior knowledge we have of the calculated similarity. In this notebook, we import prior knowledge generated from Anchor Link statistics and weight those with our numerical calculations from congruence. We demonstrate the performance gains of directly incorporating prior confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 21347120, 9477, 1882861]</td>\n",
       "      <td>[458, 46, 211593, 1396, 363404]</td>\n",
       "      <td>['European_Union', 'Europe', 'Eu,_Seine-Mariti...</td>\n",
       "      <td>[0.9227799, 0.024651, 0.020196, 0.005346, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 11884, 152735, 21212, 12674]</td>\n",
       "      <td>[183, 188, 42884, 7318, 43287]</td>\n",
       "      <td>['Germany', 'German_language', 'Germans', 'Naz...</td>\n",
       "      <td>[0.4192066, 0.2893363, 0.1470461, 0.03832, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>british</td>\n",
       "      <td>[31717, 19097669, 13530298, 4721, 158019]</td>\n",
       "      <td>[145, 842438, 23666, 8680, 161885]</td>\n",
       "      <td>['United_Kingdom', 'British_people', 'Great_Br...</td>\n",
       "      <td>[0.6101256, 0.1146913, 0.0681775, 0.0366451, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.5, 0.3, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.5, 0.3, 0.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention     full_mention                                wikipedia_URL  \\\n",
       "0       B               EU                                          NaN   \n",
       "1       B           German         http://en.wikipedia.org/wiki/Germany   \n",
       "2       B          British  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "3       B  Peter Blackburn                                          NaN   \n",
       "4       I  Peter Blackburn                                          NaN   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0                NaN             NaN            0       0   \n",
       "1            11867.0         Germany            0       0   \n",
       "2            31717.0  United Kingdom            0       0   \n",
       "3                NaN             NaN            1       0   \n",
       "4                NaN             NaN            1       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0                        ['EU', 'German', 'British']                eu   \n",
       "1                        ['EU', 'German', 'British']            german   \n",
       "2                        ['EU', 'German', 'British']           british   \n",
       "3  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "4  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "\n",
       "                     candidate_pool_page_ids  \\\n",
       "0      [9317, 9239, 21347120, 9477, 1882861]   \n",
       "1       [11867, 11884, 152735, 21212, 12674]   \n",
       "2  [31717, 19097669, 13530298, 4721, 158019]   \n",
       "3              [56783206, 9643132, 56873217]   \n",
       "4              [56783206, 9643132, 56873217]   \n",
       "\n",
       "              candidate_pool_item_ids  \\\n",
       "0     [458, 46, 211593, 1396, 363404]   \n",
       "1      [183, 188, 42884, 7318, 43287]   \n",
       "2  [145, 842438, 23666, 8680, 161885]   \n",
       "3        [2073954, 7172840, 26634508]   \n",
       "4        [2073954, 7172840, 26634508]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  ['European_Union', 'Europe', 'Eu,_Seine-Mariti...   \n",
       "1  ['Germany', 'German_language', 'Germans', 'Naz...   \n",
       "2  ['United_Kingdom', 'British_people', 'Great_Br...   \n",
       "3  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "4  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "\n",
       "                               candidate_pool_priors  \n",
       "0  [0.9227799, 0.024651, 0.020196, 0.005346, 0.00...  \n",
       "1  [0.4192066, 0.2893363, 0.1470461, 0.03832, 0.0...  \n",
       "2  [0.6101256, 0.1146913, 0.0681775, 0.0366451, 0...  \n",
       "3                                    [0.5, 0.3, 0.2]  \n",
       "4                                    [0.5, 0.3, 0.2]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base path to input\n",
    "preds_path = '../../predictions/'\n",
    "\n",
    "# Load data\n",
    "full_mentions = pd.read_csv(os.path.join(preds_path, \"anchortext_frequency.csv\"), delimiter=\",\")\n",
    "full_mentions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Saved Candidate Pool\n",
    "\n",
    "Candidate pools when exported to csv are stored as the string of the list. The below function parses the string back into a list with proper formatted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate that list is string\n",
    "type(full_mentions['candidate_pool_page_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse list as string\n",
    "def parse_list_string(list_string, value_type=int):\n",
    "    \n",
    "    parsed_list = []\n",
    "    \n",
    "    # If candidate pool is empty\n",
    "    if list_string == \"[]\" or isinstance(list_string, float):\n",
    "        pass\n",
    "    # Else parse\n",
    "    else:\n",
    "        # Parses lists of titles as strings\n",
    "        if value_type==str:\n",
    "            # Eliminate bracket and parenthesis on either side, split by comma pattern\n",
    "            parsed_list = re.split(\"', '|\\\", \\\"|', \\\"|\\\", \\'\", list_string[2:-2])\n",
    "\n",
    "        # Parses lists of IDs as ints\n",
    "        elif value_type==int:\n",
    "            # Eliminate brackets and convert each number from string to int\n",
    "            parsed_list = list(map(int, list_string[1:-1].split(', ')))\n",
    "        elif value_type==float:\n",
    "            # Eliminate brackets and convert each number from string to int\n",
    "            parsed_list = list(map(float, list_string[1:-1].split(', ')))\n",
    "            \n",
    "        \n",
    "    return parsed_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['European_Union',\n",
       " 'Europe',\n",
       " 'Eu,_Seine-Maritime',\n",
       " 'Europium',\n",
       " 'Citizenship_of_the_European_Union']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "# 0 is the hard one. See how some value is stored with '' and some with \"\". Unsure why.\n",
    "parse_list_string(full_mentions['candidate_pool_titles'][0], value_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9317, 9239, 21347120, 9477, 1882861]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "parse_list_string(full_mentions['candidate_pool_page_ids'][0], value_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "parse_list_string(full_mentions['candidate_pool_page_ids'][13], value_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9227799, 0.024651, 0.020196, 0.005346, 0.002079]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "parse_list_string(full_mentions['candidate_pool_priors'][0], value_type=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "After ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "Before [56783206, 9643132, 56873217]\n",
      "After [56783206, 9643132, 56873217]\n",
      "Before [2073954, 7172840, 26634508]\n",
      "After [2073954, 7172840, 26634508]\n",
      "Before ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(bishop)', 'Peter_Blackburn_(MP)']\n",
      "After ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(bishop)', 'Peter_Blackburn_(MP)']\n",
      "Before [0.5, 0.3, 0.2]\n",
      "After [0.5, 0.3, 0.2]\n",
      "CPU times: user 256 ms, sys: 14.2 ms, total: 271 ms\n",
      "Wall time: 270 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Apply defined function to entire dataframe for all candidate pool columns\n",
    "\n",
    "column = 'congruent_mentions'\n",
    "print(\"Before\", full_mentions[column][3])\n",
    "parsed_candidate_pool = full_mentions[column].apply(parse_list_string, value_type=str)\n",
    "full_mentions[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions[column][3])\n",
    "\n",
    "column = 'candidate_pool_page_ids'\n",
    "print(\"Before\", full_mentions[column][3])\n",
    "parsed_candidate_pool = full_mentions[column].apply(parse_list_string, value_type=int)\n",
    "full_mentions[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_item_ids'\n",
    "print(\"Before\", full_mentions[column][3])\n",
    "parsed_candidate_pool = full_mentions[column].apply(parse_list_string, value_type=int)\n",
    "full_mentions[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_titles'\n",
    "print(\"Before\", full_mentions[column][3])\n",
    "parsed_candidate_pool = full_mentions[column].apply(parse_list_string, value_type=str)\n",
    "full_mentions[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions[column][3])\n",
    "\n",
    "column = 'candidate_pool_priors'\n",
    "print(\"Before\", full_mentions[column][3])\n",
    "parsed_candidate_pool = full_mentions[column].apply(parse_list_string, value_type=float)\n",
    "full_mentions[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions[column][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Entity Vectors from Wikipedia2Vec\n",
    "\n",
    "For provided wikipedia pages, we retrieve a representative entity vector from Wikipedia2vec. This involves passing the normalized title into their get_entity_vector() function. We default to using 100d pre-trained embeddings due to the easier computational requirements, but tested results using higher dimensions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package\n",
    "from wikipedia2vec import Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 94.8 ms, sys: 122 ms, total: 217 ms\n",
      "Wall time: 289 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load unzipped pkl file with word embeddings\n",
    "w2v = Wikipedia2Vec.load(\"../../embeddings/enwiki_20180420_100d.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Wikipedia2Vec Query Success\n",
    "\n",
    "We need to measure what percent of candidates in our candidate pools successfully return a vector from Wikipedia2Vec. This should conceivably be 100% given we're passing known Wikipedia pages into this package trained over Wikipedia pages, but there may be some drop-off due to different creation dates or unique names without pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text normalization function\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    We define normalized in this notebook as:\n",
    "    - strip whitespace\n",
    "    - Spaces, not underlines\n",
    "    \"\"\"\n",
    "    return str(text).strip().replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21810/21810 [00:01<00:00, 16973.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia2vec returned an entity vector for 94.438% of 65,248 searches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over candidate pool titles to see what can be returned\n",
    "\n",
    "found_entity = 0\n",
    "searched_entity = 0\n",
    "\n",
    "for i in tqdm(range(len(full_mentions))):\n",
    "    \n",
    "    # Retrieve candidate pool\n",
    "    candidate_pool = full_mentions['candidate_pool_titles'][i]\n",
    "    \n",
    "    # Query for each candidate\n",
    "    for candidate in candidate_pool:\n",
    "        # Normalize candidate title to form necessary to input into Wikipedia2vec\n",
    "        candidate = normalize_text(candidate)\n",
    "        \n",
    "        # Query Wikipedia2vec get_entity_vector()\n",
    "        try:\n",
    "            entity_vector = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            entity_vector = None\n",
    "        \n",
    "        # Check if result\n",
    "        if entity_vector is not None:\n",
    "            found_entity += 1\n",
    "        \n",
    "        # Increment count\n",
    "        searched_entity += 1\n",
    "\n",
    "print(f\"Wikipedia2vec returned an entity vector for {round(found_entity/searched_entity*100,3)}% of {searched_entity:,} searches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Sentence Properties\n",
    "\n",
    "First, let's get a sense for what the upper bound of congruent calculations might be per sentence as well as the distribution of counts and average priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the maximum number of congruent entities in a single sentence\n",
    "max(full_mentions['congruent_mentions'].apply(len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 is expected given we filtered to only sentences containing less than this in Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What is the distribution of congruent mention counts\n",
    "plt.figure(figsize=(20,5))\n",
    "full_mentions['congruent_mentions'].apply(len).hist(bins=50)\n",
    "plt.title(\"distribution of congruent mention counts\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most sentences occur in a near normal distribution around 3 full mentions, though this distribution then extends far to the right up to 25 full mentions. Next, let's get a sense for the average ranking of the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21810/21810 [00:02<00:00, 7775.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer null 27.0% of the time.\n",
      "Correct answer not present in candidate pool 7.1% of the time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare tracking metrics\n",
    "correct_answer_rank = []\n",
    "correct_answer_prior = {0:[],1:[],2:[],3:[],4:[]}\n",
    "correct_answer_not_present = 0\n",
    "correct_answer_null = 0\n",
    "\n",
    "# Iterate over whole dataframe\n",
    "for i in tqdm(range(len(full_mentions))):\n",
    "    row = full_mentions.iloc[i]\n",
    "    correct_answer = row['wikipedia_page_ID']\n",
    "    if isinstance(row['wikipedia_title'], float):\n",
    "        correct_answer_null += 1\n",
    "    else:\n",
    "        try:\n",
    "            correct_rank = row['candidate_pool_page_ids'].index(correct_answer)\n",
    "            correct_answer_rank.append(correct_rank)\n",
    "            correct_answer_prior[correct_rank].append(row['candidate_pool_priors'][correct_rank])\n",
    "        except ValueError:\n",
    "            correct_answer_not_present += 1\n",
    "\n",
    "print(f\"Correct answer null {round(correct_answer_null/len(full_mentions)*100,1)}% of the time.\")\n",
    "print(f\"Correct answer not present in candidate pool {round(correct_answer_not_present/len(full_mentions)*100,1)}% of the time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 90.263% coverage.\n"
     ]
    }
   ],
   "source": [
    "# Calculate coverage of candidate pools for full mentions with known true values\n",
    "total_len = len(full_mentions[full_mentions['wikipedia_page_ID'].notnull()])\n",
    "print(f\"We have {round((total_len - correct_answer_not_present) / total_len * 100,3)}% coverage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJEAAAH+CAYAAAAs1J+aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAA4BUlEQVR4nO3debhsV1kn/u9rAkkgI0mYkkBQgzIoIJEO4hCISBAkqKABkdCiERq6wbbVxB8IbTfdsaEVaQRFwIBGQmSQtIAahoDSTGFQCCESIZJAIGEKg5CJ9/fH3sWtnNQ5+9xzzr3nes7n8zz1VNXaa+1aVXvvuqe+d+21q7sDAAAAACv5ts3uAAAAAAB7PiESAAAAAJOESAAAAABMEiIBAAAAMEmIBAAAAMAkIRIAAAAAk4RIAMAuUVXnV1VX1TMXLLt0XPa4Na57Xe3ZdarqzHHbnLnZfVnJnt7Pqnrc2L9LFyx75rjs/N3fs9VznAJsPXtvdgcA2P2qaq8kP53koUmOS3LrJLdI8qUk/5Tk75Kc1d0f3qw+7omq6uAkTx2fPre7v7RB6/3eJD+X5P5J7pjkkCRfT/KpJO9N8pokb+ju6zbi9VheVT01ycFJ/rK7P7hJfXhckj9ZsOj6JF9M8pEk5yb5o+7+2m7s2qYbA5U7Lim+PsmXM3x/XZTk/Ules7u3X1U9PMk9k3ywu/9yd7727jbuo0cnOb+7z9/UzgCwWwmRALaZqjouycuS3Hmu+LokX0lyaJL7jbfTquo1SR7V3dfu9o7umQ5O8ozx8ZkZfrSuWVUdkOQPkzwqSY3FneTqJPsluct4e2ySf66qn+vud6/nNfcg/5zkGxne657kqRlCikuTfHAzOzL6XJIbxse3SHJ4kh8Zb0+qqgd0979sVuc20fy+U0kOTHKrJN+e5CFJnl5V70ryy939j8us44okF4/3G+HhSU7J8P36lxuwvqsz9O9TG7Cujfa4DPtgkpy/Qr099TgHYI2czgawjVTVT2T4g//OST6f5PQkd+7um3f3oUlunuT7k5yR4X/2fyrDD1c2WFUdkuSdSR49Fp2d4UfZvt19SHfvm+SIJL+Y5B+TfEeS+25GX3eF7j6hu7+7u1+72X3Zw31/d992vB2Y5DZJnjUu+/Ykf7Z5XdtUr5z7XG7T3fslOSjJA5L8cZJrMoyyfG9VPXTRCrr79HEfPH33dXv1uvu1Y/9O2Oy+rJXjHGDrESIBbBNVdUyGH5z7ZDgd5p7dfUZ3f2xWp7tv6O4Lxh9Vd0ryus3p7bZwVpK7ZTgV52e7+1Hd/fb5UV/d/enufkmGU2T+Q4b/0Wcb6+4ru/tpSV46Fv1gVd15pTbbRXd/ubvf2t2nJrlPkssyBONnj99/AMA6CZEAto//nuGUj28k+cnuvnylyt39he5+eBachlBVt62qZ1fVhVX11ar62vj4f1XVbRatr6qOHidY7fHxd1TVi6rqE1V1zWzy2Ko6flZvfH6vqjqrqi6vquuWTiRbVXuNE9D+TVV9tqquraqrxucnV1XdtDc3an+XqvqDqvpIVX1lfD8XV9XZVfXTVfVtY73zk3xirukn5t7PTk1wW1UPTvLg8elvd/dfrFS/By9M8qIl67lDVT2pql5fVf80boevju/luVV1hxX68K1Jr2vwS1X17qr68vg5vLOqHjPxPvaqqidX1fvH1/7CuN5HrOIzWHHC3arar6qeNr6Xr1fVlVX1hqqaHJVRVd9VVb9WVW+qqn8e23+5qj5QVf+9qg5b0OaZ4z43m2/nT5Zs317mtY6vqldU1Ser6htVdXVVvaeqfr2qbjnV13X467nHd1vQr4PG/f+sqvrQuG2+UVX/UlV/XsNprQvVkkmbq+qEcR+7alzHRVX1jKrady0dr6ofG/exrmFy6w2fXmE8he0RGU4PvWWS31rQjxUn1q6qn6mqN47fK9dV1Zeq6mNVde543O071jt+3D9OGZuesnTfqarj59b7rX2/qvavqt8et9HsMzl6rLfsxNrL9PVt43b+WlW9bzw291qm/rKT3s/Vucnk3bM+ZcepbM9Y8F6PXvRel3mNvarqF6rqLVX1uRr+LfhUVf3F/Ge2Uv9rsNPfXwCsjTmRALaBGoKd2Q/7s7r7n1bbtrtv9OO5qn4kw3wfB49F/5rhh9pdx9svVtXDuvvvV1jtDyT5oyT7j+0XThhdVT+d5BVJbpbh9LrrF7yv1yX5d3PFVyc5LMmPjbdHVdUjF83rVFW/keR/ZMd/qnxj7Mudx9vPZpjk+ktJvpBhfppZADE/V03G5av15Lm+/u5qG3X3N5cUvTw7fszN1ndAdsyl9LiqeujEttgryWuTnJTh8/3XcR3HJTmuqo7p7mcsbVRV+2T47B80Fn0zybVJfjjJj1TV76z2fS1Y962SvCnJvcai6zPsAw9OcmJVPWliFX+THWHQbI6pgzKM6Lpnhs/lhO6+eK7NV5N8NsOcQ9+WYX/7+gp93DvJCzOcbji/jltmOCX0+5P8QlU9aBfNWTQfji4KCn4lO+bvmvUtSe4w3k6uqqd29/NWfJGqX0sy25ZXZxjZ891JnplhOz+wu29Ypvmi9T0mwyiqmyX5X939G6ttu7O6+z1V9YYMcyT9TFWd2t3LbtMl/XxJkl+YK/pqhj5/53j7iSSvzzB31rUZ9p2DkuybxXMALZpX7tAk78vwXXNthmNvp43H2q9n2Ne/NPbh+8bbQ6vqpO6+Zi3rXuDrGd7rrTJ8Hl/Ljn1rZlX7Q1UdlOHfkuPn2n0lye0y/Hv1iKp6Tnf/2gqrWdP3FwBrZyQSwPZw/+z4zl/z3BRVdVR2BEgfSfKD3X3L7t4/Q3hwcYbQ5XVVdcQKq/qjJBdmmO9l1v7HFtQ7M8l5Se7S3QeN85780tiXmyf5vxkCpPdn+KF4y+4+OEM4dUqSK5M8LDt+BM+/lydmmPvp2zJc6epe3b3fOO/MoWN/XpkhHEl3/1SGYGBmfq6a247LJ43hww+PT89b59W1PpzktAzh3S3G975Phs/krzP8qH1lVe23wjqelOFH3OOSHNjdByU5KsNnmyRPq8WnAv3PDAFSJ3lakkO6+5Akt80QrvxGhsBmLV6cIUC6JskTkhwwrvvoDPvf72cIe5bzriT/McOP/X3Htvsm+dEk78kw19Sfzzfo7ud0920znAKVJE9Zsn1vu+Q1npMhQPpshlMND+3uAzJMiH7/JB9I8l1JXlPjaLYNduLc448vWP6ZJL+X4cf0IXN9+/YMn1+S/G5V3WtB25l7ZDhGzkhy6/FzPDjJb4/L758do28mVdV/yRB87p3kqbsyQJrz+vH+5hk+i0lV9YMZAqRvZtiPD+3uA7r7lhlC5AdlmDz72iTp7v837h+vHFfxyqX7Tnf/vwUv9cwMo0N/Ksn+4+d7VIbvrdW6Z4YA6flJbtPdt8rwHfz0DMfmgzIcqxuiu185vtfZ+3nOgvd62UrrmPOSDN891yb5Txm+fw5JcvvsOF3zv1TVE1ZYx1q/vwBYq+52c3Nzc9vityT/LcMPik5y+3Ws54XjOr6Q5LYLlh+Z4X/gO8nzlyw7eq4Pl2b40bToNY6fq/fuJHstU+9JY50PZwgZFtW5d4Yfgtdk+BE8Kz8kw0iTzjDSqVb5/uffw9Fr/Ay/Y24dv7kLt/leSf5hfJ3HLFh+/lw/7r9g+T4ZrgrVSf6/Jctun2HEVmc4HW/R6//53PqfuWD5peOyxy0pv89cu19Y5n393Vydxy16/RU+l/0zBCydIQRdVb+W1Ln7uF99Lcn3LFPngAyBVCd5+E728XHL7WcZwrP54/mDa9w/nj+2f/GCZc9caduNdV49Lj9vwbIzx2Vnjs8rw4i7Ho/Fn13Hfn3p/LpXUf8H5t7LL67Uz7ny2aiev9nJvi1c3wrv4foM4fXUfnDpxDZ6+TLtZ/vJdVnyvZ8dx//C7bvkNc5fsGyy/ZL3+rgl5fPH+anLtH3VuPyqDGHwotff6e8vNzc3N7f13YxEAtgeDp17vDOnXX1LVVWSnxmf/mF3f2ZpnR7mWfrD8enJK6zu+d299BSIRZ7dy58qMzuN6AXd/ZVFFbr7fRlGPN08w6iJmUdk+JF/XZL/3N29ir5slHVvi9UYP7fZvDk/uELVd3T3Wxe0vybDaWFJ8r1LFj8iw2iSr2cYkbPIM1fd2Rub7TeXJfmTBf26IcOP4zUZ97u3jU9X+lxW8vgMwcjru/tDy7zOV7LjMu8PWlRnld5bVZ8Zb1/OMErlaeOyzyT5uTWudzZCZ6XP4Josv31nk+4v3TdupKpulmES+V/JcKrSj3f3K1dqs8Hmj7FbrbLNl8b7w5ebU2iD/HV3f2AD1vPby5Q/O8MxuneSn96A19lIs+P88gwjDxd5+nh/WJIHLlNnLd9fAKyDOZEAtocVJ5depTtlx4+wN61Q77wM/5N/aFXdqbs/saDOO1b5mgvrVdUB2fHD4L9V1U0mzZ0z6/Md58p+YLx/X3dfscq+bJT5bbHu8KqqfihDqHFchpFgiyZzPnKFVbx7hWWfHu+X/vg+dry/oLu/vKhhd/9TVX0qw6ljO2O27vNXCPfenmEUx7J/x9RwWfefz3AK4m2S3GJBtZU+l5XMgpcHV9VNwtQ5+4/3d1yhzpSbTAI+Oi/JI5b7/JOkqr49w6l2988wAu6A3HQqg5U+gwtXCHuX2zfmHZDkDRlOI/xskgdvUGiyM9by3femDPMa3SvJ343zI71lme+y9Vjt9+BKLuvuSxYt6O4vV9X7Muyvxy6qs4lm/Xlr33SutyRJd1809x1ybHacojZvLd9fAKyDEAlge/jc3ONbZccf1zvj1nOPP7VCvfmrvt06N76i2cxq5/xYrt5ts+PH8Gp/IMyHCLP5bXbFhMdT5rfFocvWWoW5CXVnbkjyxeyYxHf/DKHSSlcJWziKazSbyPxmS8pn+8JK+0Ey7As7GyJNrru7v1FVn88QDt3IOP/QnyV51Fzx9bnx53JQhjmS1nr1tNuP9/tnR1C0kkUB1mrdqbsvTZKqunWSE5L8rwwjM/5HdkzSfiNV9ZMZTtXcZ674yxnCkc4wOu+QrH/fWOlvyfl5wn5iEwKkZHiPM59fTYPu/nhV/WKGUZX3HW+pqquSvDXDqZrnbsAIxp2Z+2g5U8fgbPmtV6y1++3sd8hy/V/L9xcA6+B0NoDt4cK5xytNpLtaq/3xtFy9VV29Z4VT2eZPMTmuu2sVt2fuRP92pX/JjqsZrXlbVNUDsyNAekGS70myT3ffqndMBP17s+prfZ0Ju/LzW+u6H58hQLohw2k+x+Smn8urxrpr/Vxm+99pq9z3jl/j69xId1/Z3a/IECB9PcmTFl06vaoOzTA/zz5J3pJhnrFb9DA5/W3Gz+CRG9GnCW/LjonKX1xVK02GvqvcY+7xP6+2UXeflWEE2RMyTJh9WYb5qH4mw2mKb6uqA9fZt1Vf1W4Fm/EdtpHW+28JALuZEAlge3hrxquMJfnJNa5j/n/Nj1qh3vzpMVet8bWmfHbu8fesof3sFLaj19+VndPd12c4HStJHlhVax0NM5tT5G+6+0nd/eEFodvSK4ptlNm+MHU62M6OQlrVuqtqnyw/imv2uby4u5/R3ZcsOF1mvZ/L7BS2tex769bdH80w302SPHtBmPHjGa769cUMI4De1je9tP2u2jfmXZohwPqXDKefvnUcTbU7PWS8vybDVftWrbu/0N1/1N0nd/cdMlzt74wMgcYPZe3zfm2k1R6DS0c9zUbp7LtC24PW1KPVmfVnpX9Lkh3vb1f9WwLAThIiAWwD3f3ZDFdTSpJHV9WdV9t2nFA7GU5Lm01Se8IKTX50vP/8LphDJEnS3V9M8pHx6UoTeC9ndnnqY6vqdjvRbj6MWM/onj8Y7w9K8p9X22jJpeJnP74WniI0brcHrKl30y4Y748d56da9PrHZG1zDs3W/SNz+95SP5zlT6Oa+lz2T/LvVnj92TZeafvO5rJ5yLi+zfC7Ga6EeFiS/7Jk2ewzuLi7/3WZ9j+6TPmG6u6PJ/mRDN8fd0tyflXtjgArVXWfDIFakpzd3d9Yz/q6+5+7+/QMp7MlN53seTX7zkY7qqq+Y9GC8di89/j0giWLvzhrv8K613ucrGTWn/sv+V77lqr67uwIwd67xtcBYIMJkQC2j6dlOI1qvySvqaoVR4lU1SFV9eqM/xs9zv8xu6rSLy/6IVhVt0/yy+PTV2xUx5fxovH+hKpaMUiqqqXzJv1Fhvlh9k7yeyuEFUvNT2J88Crb3ER3vyHJ345Pf6uqHjHVpqpOTfJLc0VXj/f3WFA9GU7D+fa19nHCqzOMZNgvya8uU2elyc5XMtvH7pDklKULxx+cT1taPmfqc3l6hgmflzPbxgevUOePM4xGOTg7RgQtVFU32xVBU3dfnR1h5FPHU9hmZp/BnavqJiNNquqeSR690X1aTnf/S4YRSf+c5C4ZgqTbr9honarqezKctlhJvpaduKLfONJtJbNRXUtH/q1m39kVnr5M+a9mOEavT/KaJcv+Ybx/0KLRkFX1gIxzQS1jve/17PH+iOy40uZSs6vOfS4rX8wBgN1IiASwTXT3P2W4WtW1GUYEfLCqfqOqvnNWp6r2qqp7VdVvJ/l4bjwxbjJM5PulDJNZv6mqfmCu7f0y/KF/cIYRS2fsuneTZJj0dnZlnj+tqv9eVd/6X/WqukVVHV9Vz8+SuVDGH+Cz+YR+Nslrxx/Ws7aHVNVDqup186cKdfeXsmMi2H9fVeu5QMWjk1yUIcg6p6rOqqofGi+JPuvH7arqlPEKS3+U4QfhzF+P9w+uqqfPfghW1cFV9ZtJ/k9WOZHwzuruT2WYhylJnl5Vp89GJFXV4eNn/pjsCDN2Zt3vTnLu+PSFVfVLsx/1VXWHDCHTfZMsN8Jm9rn8UlWdWlU3H9vetqp+L8N2X+lz+fB4/4iqOmRRhe7+YJLnjk+fUFV/UVX3nIWR43F0j6p6eoZ9754rved1eG6Gz+GAJL82V/63GUaK3CrJWbPAuKpuXlU/My5faULiDdfdn8wQJH0syXdlCJLWcrrjsqrqgPGY/6Mk78kwyubaJI/s7lXPh5Tk+VV1TlX99Pzpd1W1f1U9Icljx6I3LGk323d+aBxFsztcneSUqvr9qjps7OcB43fALFz6g/GYnXdOhn3k0CSvqKojx7b7VdUpSV6bHSNPF5m91x9fy3bs7vdkx+jY/1NVT66qW4x9uG1V/XF2zNv19PWOIgNgA3W3m5ubm9s2uiW5X4Yfcj13uybDD+sb5sq+meG0jZstaf8jGYKkWb2vjrfZ8y8m+aEFr3v0XJ2jV+jf8bN6q3gvhyV585L3cvXYh2/OlV23TPvTl7znf83wP+zz6zt4SZunzS37RpJPZpj75ew1bIsDM4Qi8339ZoYfb19f0o+PJLn3XNubZZhbaWm72fv5qwyjLzrJ+Qte+/xx2TNX6N8zV2i/b4bLzM9e//rx9Wfv5YyVXmP8zDrJ4xYsOzTJB+fWfe24TWfv8z8s1z5DiHnRXNvZFetm/frDDJNOd5IzF7z2D8/VvT7DlQwvTXLpknp7ZZi4fH4bfT3DqInrlpTfbyf3i8fNtV32WBnrPjc7jsNbz5WfsaQPXxo/x84QED96tmxntvtqjtOJz/f2ST46Lr8kyVE7+dnMtvvXM8xN9ZkMc6R9bcn77QynHd59hXUt7Odc+ez2lbn9b3b7uyS3XNLukAxz/czqXDXbdzJcAGBy319mP7h0wbJvbaMkv5Md+/rnx/121ofzkuy7zPp/e8E+Mtt3X5uVvz+OyY7vqBvG7TB7r0eu8jg/KDu+I3p87fnvkE7y7GX6Pmu3pu8vNzc3N7e134xEAthmuvsdSb47wxWszsrwQ+4bGUYzfCHJ3yd5VpK7dPeju/u6Je3fNrb/3xl+rH9bhlNGLkrynLHd3+2m9/K5DHO7nJTh1JXLMlyRar8MI4bemOES6Ecv0/5/Zjjt6Y8zfA7J8F4uznA63k/lxqewJcNorKdkmNPjugzz/twxa5iouLu/3N0/m+Eqbc8Z1/m5DNviugyf6cuSPDTJ93T3++baXpfkx5L81yT/NNavDCMwnpjkYdmYqz8t1/dvJHlwhs/igxkCisrw4/pnuvu0daz780l+IMkzMgQO38zww/ivkzywu1+wQtsvjW2fm+EH7A1j2/OTPKq7nzDx2m/PMBnzmzIEkrfJsH3vuKTeDd39K0m+L8OplRePr3VQhsDhHRl+xN5zPOZ2ledk+OxvmeRbn/n4+T82w/7w9Qyh4yUZ9t97ZQjHdrvu/nSGAOqiJN+R4Spnd1yx0WL7Ztg2t8kw4uqaDOHY6zOEH/fs7vt194eXX8Wy/luS/5QhSPlohv1n/wwB0XlJfiHJ8d39tflGPczV9sMZTtX6VIZ9YbbvrDSB9bp0929kmBvuHRm+j6/NcEw+JcmJvcwonu7+rQyjU9+VIYTba2z3hAzffct+f3T3x5LcP8OowasyBL+z97qqEZo9jAg9IcMVFc/PENbtnyGQenWS+3f3ry27AgA2RXX3ZvcBAAAAgD2ckUgAAAAATBIiAQAAADBJiAQAAADApMkQqapeWlVXVtWH58qeXVUfrap/rKrXVtXBc8tOr6pLquriqnrQXPm9q+pD47LnzV0Gd5+qeuVY/u6qOnpj3yIAAAAA67WakUhnJjlxSdl5GS6Z+r0ZrghzepJU1V0zXB3ibmObF1TVXmObFyY5NcMlQY+ZW+fjk3yxu78zw6Vyf2etbwYAAACAXWPyEpzd/falo4O6+2/nnr4rySPGxyclObu7r0nyiaq6JMl9qurSJAd29zuTpKpenuThGS69fFKGS+Amw+WZn19V1ROXjTvssMP66KOPXqkKAAAAADvhfe973+e6+/BFyyZDpFX4hSSvHB8fkSFUmrl8LLtufLy0fNbmsiTp7uur6uokhyb53NIXqqpTM4xmyh3ucIdccMEFG9B9AAAAAJKkqv5luWXrmli7qv6/JNcnOWtWtKBar1C+UpubFna/qLuP7e5jDz98YSgGAAAAwC6w5hCpqk5J8tAkPzd36tnlSY6aq3Zkkk+P5UcuKL9Rm6raO8lBSb6w1n4BAAAAsPHWFCJV1YlJfiPJw7r7X+cWnZvk5PGKa3fKMIH2e7r7iiRfqarjxquyPTbJ6+banDI+fkSSt0zNhwQAAADA7jU5J1JVvSLJ8UkOq6rLkzwjw9XY9kly3pAJ5V3d/YTuvrCqzknykQynuT2pu28YV/XEDFd62y/DhNpvHMtfkuRPx0m4v5Dh6m4AAAAA7EHq3+qgn2OPPbZNrA0AAACwcarqfd197KJl65pYGwAAAIDtQYgEAAAAwCQhEgAAAACThEgAAAAATBIiAQAAADBJiAQAAADAJCESAAAAAJOESAAAAABMEiIBAAAAMEmIBAAAAMAkIRIAAAAAk4RIAAAAAEwSIgEAAAAwSYgEAAAAwKS9N7sDJEef9vrN7sKGufSMh2x2FwAAAIBdwEgkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASZMhUlW9tKqurKoPz5XdqqrOq6qPjfeHzC07vaouqaqLq+pBc+X3rqoPjcueV1U1lu9TVa8cy99dVUdv8HsEAAAAYJ1WMxLpzCQnLik7Lcmbu/uYJG8en6eq7prk5CR3G9u8oKr2Gtu8MMmpSY4Zb7N1Pj7JF7v7O5P8XpLfWeubAQAAAGDXmAyRuvvtSb6wpPikJC8bH78sycPnys/u7mu6+xNJLklyn6q6XZIDu/ud3d1JXr6kzWxdr0pywmyUEgAAAAB7hrXOiXSb7r4iScb7W4/lRyS5bK7e5WPZEePjpeU3atPd1ye5Osmha+wXAAAAALvARk+svWgEUa9QvlKbm6686tSquqCqLrjqqqvW2EUAAAAAdtZaQ6TPjqeoZby/ciy/PMlRc/WOTPLpsfzIBeU3alNVeyc5KDc9fS5J0t0v6u5ju/vYww8/fI1dBwAAAGBnrTVEOjfJKePjU5K8bq785PGKa3fKMIH2e8ZT3r5SVceN8x09dkmb2boekeQt47xJAAAAAOwh9p6qUFWvSHJ8ksOq6vIkz0hyRpJzqurxST6Z5JFJ0t0XVtU5ST6S5PokT+ruG8ZVPTHDld72S/LG8ZYkL0nyp1V1SYYRSCdvyDsDAAAAYMNMhkjd/ahlFp2wTP1nJXnWgvILktx9Qfk3MoZQAAAAAOyZNnpibQAAAAC2ICESAAAAAJOESAAAAABMEiIBAAAAMEmIBAAAAMAkIRIAAAAAk4RIAAAAAEwSIgEAAAAwSYgEAAAAwCQhEgAAAACThEgAAAAATBIiAQAAADBJiAQAAADAJCESAAAAAJOESAAAAABMEiIBAAAAMEmIBAAAAMAkIRIAAAAAk4RIAAAAAEwSIgEAAAAwSYgEAAAAwCQhEgAAAACThEgAAAAATBIiAQAAADBJiAQAAADAJCESAAAAAJOESAAAAABMEiIBAAAAMEmIBAAAAMAkIRIAAAAAk4RIAAAAAEwSIgEAAAAwSYgEAAAAwCQhEgAAAACThEgAAAAATBIiAQAAADBJiAQAAADAJCESAAAAAJOESAAAAABMEiIBAAAAMEmIBAAAAMAkIRIAAAAAk4RIAAAAAEwSIgEAAAAwSYgEAAAAwCQhEgAAAACThEgAAAAATBIiAQAAADBJiAQAAADAJCESAAAAAJOESAAAAABMEiIBAAAAMEmIBAAAAMAkIRIAAAAAk4RIAAAAAEwSIgEAAAAwSYgEAAAAwCQhEgAAAACThEgAAAAATBIiAQAAADBJiAQAAADAJCESAAAAAJOESAAAAABMWleIVFW/UlUXVtWHq+oVVbVvVd2qqs6rqo+N94fM1T+9qi6pqour6kFz5feuqg+Ny55XVbWefgEAAACwsdYcIlXVEUn+U5Jju/vuSfZKcnKS05K8ubuPSfLm8Xmq6q7j8rslOTHJC6pqr3F1L0xyapJjxtuJa+0XAAAAABtvvaez7Z1kv6raO8ktknw6yUlJXjYuf1mSh4+PT0pydndf092fSHJJkvtU1e2SHNjd7+zuTvLyuTYAAAAA7AHWHCJ196eSPCfJJ5NckeTq7v7bJLfp7ivGOlckufXY5Igkl82t4vKx7Ijx8dJyAAAAAPYQ6zmd7ZAMo4vulOT2SW5ZVY9ZqcmCsl6hfNFrnlpVF1TVBVddddXOdhkAAACANVrP6Ww/muQT3X1Vd1+X5DVJfiDJZ8dT1DLeXznWvzzJUXPtj8xw+tvl4+Ol5TfR3S/q7mO7+9jDDz98HV0HAAAAYGesJ0T6ZJLjquoW49XUTkhyUZJzk5wy1jklyevGx+cmObmq9qmqO2WYQPs94ylvX6mq48b1PHauDQAAAAB7gL3X2rC7311Vr0ry/iTXJ/lAkhcl2T/JOVX1+AxB0yPH+hdW1TlJPjLWf1J33zCu7olJzkyyX5I3jjcAAAAA9hBrDpGSpLufkeQZS4qvyTAqaVH9ZyV51oLyC5LcfT19AQAAAGDXWc/pbAAAAABsE0IkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYtK4QqaoOrqpXVdVHq+qiqrpvVd2qqs6rqo+N94fM1T+9qi6pqour6kFz5feuqg+Ny55XVbWefgEAAACwsdY7Eun3k/x1d393knskuSjJaUne3N3HJHnz+DxVddckJye5W5ITk7ygqvYa1/PCJKcmOWa8nbjOfgEAAACwgdYcIlXVgUl+OMlLkqS7r+3uLyU5KcnLxmovS/Lw8fFJSc7u7mu6+xNJLklyn6q6XZIDu/ud3d1JXj7XBgAAAIA9wHpGIn17kquS/ElVfaCqXlxVt0xym+6+IknG+1uP9Y9Ictlc+8vHsiPGx0vLAQAAANhDrCdE2jvJ9yV5YXffK8nXMp66toxF8xz1CuU3XUHVqVV1QVVdcNVVV+1sfwEAAABYo/WESJcnuby73z0+f1WGUOmz4ylqGe+vnKt/1Fz7I5N8eiw/ckH5TXT3i7r72O4+9vDDD19H1wEAAADYGWsOkbr7M0kuq6rvGotOSPKRJOcmOWUsOyXJ68bH5yY5uar2qao7ZZhA+z3jKW9fqarjxquyPXauDQAAAAB7gL3X2f4/Jjmrqm6e5ONJ/n2GYOqcqnp8kk8meWSSdPeFVXVOhqDp+iRP6u4bxvU8McmZSfZL8sbxBgAAAMAeYl0hUnd/MMmxCxadsEz9ZyV51oLyC5LcfT19AQAAAGDXWc+cSAAAAABsE0IkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYNK6Q6Sq2quqPlBVfzU+v1VVnVdVHxvvD5mre3pVXVJVF1fVg+bK711VHxqXPa+qar39AgAAAGDjbMRIpKckuWju+WlJ3tzdxyR58/g8VXXXJCcnuVuSE5O8oKr2Gtu8MMmpSY4ZbyduQL8AAAAA2CDrCpGq6sgkD0ny4rnik5K8bHz8siQPnys/u7uv6e5PJLkkyX2q6nZJDuzud3Z3J3n5XBsAAAAA9gDrHYn03CS/nuSbc2W36e4rkmS8v/VYfkSSy+bqXT6WHTE+Xlp+E1V1alVdUFUXXHXVVevsOgAAAACrteYQqaoemuTK7n7fapssKOsVym9a2P2i7j62u489/PDDV/myAAAAAKzX3utoe78kD6uqH0+yb5IDq+rPkny2qm7X3VeMp6pdOda/PMlRc+2PTPLpsfzIBeUAAAAA7CHWPBKpu0/v7iO7++gME2a/pbsfk+TcJKeM1U5J8rrx8blJTq6qfarqThkm0H7PeMrbV6rquPGqbI+dawMAAADAHmA9I5GWc0aSc6rq8Uk+meSRSdLdF1bVOUk+kuT6JE/q7hvGNk9McmaS/ZK8cbwBAAAAsIfYkBCpu89Pcv74+PNJTlim3rOSPGtB+QVJ7r4RfQEAAABg46336mwAAAAAbANCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJQiQAAAAAJgmRAAAAAJgkRAIAAABgkhAJAAAAgElCJAAAAAAmCZEAAAAAmLTmEKmqjqqqt1bVRVV1YVU9ZSy/VVWdV1UfG+8PmWtzelVdUlUXV9WD5srvXVUfGpc9r6pqfW8LAAAAgI20npFI1yf51e6+S5Ljkjypqu6a5LQkb+7uY5K8eXyecdnJSe6W5MQkL6iqvcZ1vTDJqUmOGW8nrqNfAAAAAGywNYdI3X1Fd79/fPyVJBclOSLJSUleNlZ7WZKHj49PSnJ2d1/T3Z9IckmS+1TV7ZIc2N3v7O5O8vK5NgAAAADsATZkTqSqOjrJvZK8O8ltuvuKZAiaktx6rHZEksvmml0+lh0xPl5aDgAAAMAeYt0hUlXtn+TVSZ7a3V9eqeqCsl6hfNFrnVpVF1TVBVddddXOdxYAAACANdl7PY2r6mYZAqSzuvs1Y/Fnq+p23X3FeKralWP55UmOmmt+ZJJPj+VHLii/ie5+UZIXJcmxxx67MGgC2NMdfdrrN7sLG+bSMx6y2V0AAAB2k/Vcna2SvCTJRd39u3OLzk1yyvj4lCSvmys/uar2qao7ZZhA+z3jKW9fqarjxnU+dq4NAAAAAHuA9YxEul+Sn0/yoar64Fj2m0nOSHJOVT0+ySeTPDJJuvvCqjonyUcyXNntSd19w9juiUnOTLJfkjeONwAAAAD2EGsOkbr777N4PqMkOWGZNs9K8qwF5Rckufta+wIAAADArrUhV2cDAAAAYGsTIgEAAAAwSYgEAAAAwCQhEgAAAACThEgAAAAATBIiAQAAADBJiAQAAADAJCESAAAAAJOESAAAAABMEiIBAAAAMEmIBAAAAMAkIRIAAAAAk4RIAAAAAEwSIgEAAAAwSYgEAAAAwCQhEgAAAACThEgAAAAATBIiAQAAADBJiAQAAADAJCESAAAAAJOESAAAAABMEiIBAAAAMEmIBAAAAMAkIRIAAAAAk4RIAAAAAEwSIgEAAAAwSYgEAAAAwCQhEgAAAACThEgAAAAATBIiAQAAADBJiAQAAADAJCESAAAAAJOESAAAAABMEiIBAAAAMEmIBAAAAMAkIRIAAAAAk4RIAAAAAEwSIgEAAAAwSYgEAAAAwCQhEgAAAACThEgAAAAATBIiAQAAADBJiAQAAADAJCESAAAAAJOESAAAAABMEiIBAAAAMEmIBAAAAMAkIRIAAAAAk/be7A4AAGx1R5/2+s3uwoa59IyHbHYXAIBNYiQSAAAAAJOESAAAAABMEiIBAAAAMEmIBAAAAMAkIRIAAAAAk4RIAAAAAEwSIgEAAAAwae/N7gAAAMBWc/Rpr9/sLmyIS894yGZ3AdiDGIkEAAAAwCQhEgAAAACThEgAAAAATBIiAQAAADBJiAQAAADAJCESAAAAAJP23uwOzFTViUl+P8leSV7c3WdscpcAAACAXejo016/2V3YEJee8ZDN7sJusUeMRKqqvZL8QZIHJ7lrkkdV1V03t1cAAAAAzOwRIVKS+yS5pLs/3t3XJjk7yUmb3CcAAAAARntKiHREksvmnl8+lgEAAACwB6ju3uw+pKoemeRB3f2L4/OfT3Kf7v6PS+qdmuTU8el3Jbl4t3Z01zksyec2uxNsCtt+e7Ldty/bfvuy7bcv2377su23L9t+e9pK2/2O3X34ogV7ysTalyc5au75kUk+vbRSd78oyYt2V6d2l6q6oLuP3ex+sPvZ9tuT7b592fbbl22/fdn225dtv33Z9tvTdtnue8rpbO9NckxV3amqbp7k5CTnbnKfAAAAABjtESORuvv6qnpykr9JsleSl3b3hZvcLQAAAABGe0SIlCTd/YYkb9jsfmySLXeKHqtm229Ptvv2ZdtvX7b99mXbb1+2/fZl229P22K77xETawMAAACwZ9tT5kQCAAAAYA8mRNpNqurEqrq4qi6pqtMWLK+qet64/B+r6vs2o59svFVs++Or6uqq+uB4+63N6Ccbr6peWlVXVtWHl1nuuN+CVrHdHfNbVFUdVVVvraqLqurCqnrKgjqO+y1oldvesb/FVNW+VfWeqvqHcbv/1wV1HPNb0Cq3vWN+C6uqvarqA1X1VwuWbenjfo+ZE2krq6q9kvxBkgcmuTzJe6vq3O7+yFy1Byc5Zrz9uyQvHO/5N2yV2z5J/q67H7rbO8iudmaS5yd5+TLLHfdb05lZebsnjvmt6vokv9rd76+qA5K8r6rO8+/9trCabZ849reaa5I8oLu/WlU3S/L3VfXG7n7XXB3H/Na0mm2fOOa3sqckuSjJgQuWbenj3kik3eM+SS7p7o9397VJzk5y0pI6JyV5eQ/eleTgqrrd7u4oG241254tqrvfnuQLK1Rx3G9Bq9jubFHdfUV3v398/JUMf1wesaSa434LWuW2Z4sZj+Ovjk9vNt6WTjjrmN+CVrnt2aKq6sgkD0ny4mWqbOnjXoi0exyR5LK555fnpn9YrKYO//asdrvedxwO+8aqutvu6Rp7AMf99uWY3+Kq6ugk90ry7iWLHPdb3ArbPnHsbznjKS0fTHJlkvO62zG/Taxi2yeO+a3quUl+Pck3l1m+pY97IdLuUQvKlibVq6nDvz2r2a7vT3LH7r5Hkv+T5C93dafYYzjutyfH/BZXVfsneXWSp3b3l5cuXtDEcb9FTGx7x/4W1N03dPc9kxyZ5D5VdfclVRzzW9Qqtr1jfguqqocmubK737dStQVlW+a4FyLtHpcnOWru+ZFJPr2GOvzbM7ldu/vLs+Gw3f2GJDerqsN2XxfZRI77bcgxv7WNc2O8OslZ3f2aBVUc91vU1LZ37G9t3f2lJOcnOXHJIsf8FrfctnfMb1n3S/Kwqro0w1QlD6iqP1tSZ0sf90Kk3eO9SY6pqjtV1c2TnJzk3CV1zk3y2HEm9+OSXN3dV+zujrLhJrd9Vd22qmp8fJ8Mx+Xnd3tP2QyO+23IMb91jdv1JUku6u7fXaaa434LWs22d+xvPVV1eFUdPD7eL8mPJvnokmqO+S1oNdveMb81dffp3X1kdx+d4bfdW7r7MUuqbenj3tXZdoPuvr6qnpzkb5LsleSl3X1hVT1hXP6HSd6Q5MeTXJLkX5P8+83qLxtnldv+EUmeWFXXJ/l6kpO7e8sMd9zOquoVSY5PclhVXZ7kGRkmXnTcb2Gr2O6O+a3rfkl+PsmHxnkykuQ3k9whcdxvcavZ9o79red2SV42Xo3325Kc091/5W/8bWE1294xv41sp+O+7McAAAAATHE6GwAAAACThEgAAAAATBIiAQAAADBJiAQAAADAJCESAAAAAJOESAAAK6iqZ1ZVz90+U1V/VVXfu4te7/yqetWuWDcAwHoIkQAApl2d5L7j7alJ7pzkvKq61WZ2CgBgd9p7szsAAPBvwPXd/a7x8buq6tIk70xyYpI/37ReAQDsRkYiAQDsvH8Y749Kkqq6b1WdW1WfrqqvVdUHq+rn5htU1ePG0+G+p6rOG+t9tKp+aqUXqqqDquodVfUPVXX4Lno/AACThEgAADvvDuP9J8b7OyZ5R5JfTPITSV6d5E+q6lEL2v55knOT/GSSjyU5u6qOXPQi4+lyb0py8yT37+6rNuwdAADsJKezAQCsQlXN/m66Y5LnJ/lgktclSXefPVevkrw9yZFJfinJK5as6ve6+6Vj3fcl+WyShyb5wyWvd3iGAOmrSR7c3V/e2HcEALBzhEgAANMOTXLd3PPPJ/n+7r4mSarqkCT/NclJSY5IstdY71ML1vW3swfd/fmqujJD4DTvNkneluQzSX6iu7+2EW8CAGA9nM4GADDt6iTfn+S4JL+c4fSyP6+q2d9SZyb52STPTvJjY92XJtl3wbq+tOT5tQvq3TXJXZL8qQAJANhTGIkEADDt+u6+YHz87qr6epKXJ3lkVb0uyUOSPLm7v3VK2lzAtBZvTfKBJC+qqs919/9dx7oAADaEkUgAADvvz5JcmOQ3kuyT4fS1a2YLq+qAJA9bzwt097OS/O8kf1FVD1jPugAANoKRSAAAO6m7u6r+R5Kzkhyb5L1Jfquqvpzkm0lOy3AK3IHrfJ3TxkDqdVX1wO5+1zq7DgCwZkYiAQCszSuTfCzJryd5dJJPZDjF7feTvHp8vBGePK7vjVV1jw1aJwDATqvu3uw+AAAAALCHMxIJAAAAgElCJAAAAAAmCZEAAAAAmCREAgAAAGCSEAkAAACASUIkAAAAACYJkQAAAACYJEQCAAAAYJIQCQAAAIBJ/z8iWU21EE6fngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot rank distribution\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.hist(correct_answer_rank, bins=20)\n",
    "plt.title(\"Correct Candidate Rank Distribution\", size=25)\n",
    "plt.xlabel(\"Rank\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11466\n",
       "1     1970\n",
       "2      518\n",
       "3      300\n",
       "4      114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Provide rank quantities\n",
    "pd.Series(correct_answer_rank).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Candidate Rank: 0 - Average Prior Confidence: 0.88993 - Median Prior Confidence: 0.96091\n",
      "Correct Candidate Rank: 1 - Average Prior Confidence: 0.18542 - Median Prior Confidence: 0.17061\n",
      "Correct Candidate Rank: 2 - Average Prior Confidence: 0.07883 - Median Prior Confidence: 0.07267\n",
      "Correct Candidate Rank: 3 - Average Prior Confidence: 0.04203 - Median Prior Confidence: 0.04132\n",
      "Correct Candidate Rank: 4 - Average Prior Confidence: 0.02247 - Median Prior Confidence: 0.01424\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean prior confidence for each rank\n",
    "for k,v in correct_answer_prior.items():\n",
    "    print(f\"Correct Candidate Rank: {k} - Average Prior Confidence: {round(np.mean(v),5)} - Median Prior Confidence: {round(np.median(v),5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When limited to sentences with 25 congruent mentions or less, 79.8% are in First Place and 13.7% are in Second, totaling 93.5% in our top two positions. Our first place candidates have median prior confidence of 0.96, but mean confidence of 0.889, suggesting that for most mentions we are highly confident in its first option, but a few are much lower and more uncertain, bringing the mean down (like a 50/50 estimate). Conversely, in second place, median is 0.17 but mean is 0.185, further suggesting those 50/50 estimates for some mentions bringing the mean up.\n",
    "\n",
    "Our hope is that congruence will be able to clarify those more evenly split prior values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Congruence Metric between Congruent Entities\n",
    "\n",
    "In order to allow this to become recursive for N many tables, we will need to capture a congruence table for every candidate pool to every other candidate pool in a two-level dictionary so you can retrieve values using `matrix[3][1]`. This would entail duplication except to save that, we sort by value so you always search [small][large]. This saves us computation and storage. To see a sequential example of our logic, scroll to the end of this notebook. Below is our function-based implementation of our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to Create Modular Congruent Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate lists of vectors\n",
    "def get_candidate_pool_vectors(candidate_pool_titles, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return entity vectors from Wikipedia2Vec\n",
    "    Takes as input a list of page titles, representing the candidate pool\n",
    "    Normalizes each page title to match necessary input format\n",
    "    Returns entity vector or empty vector if no match\n",
    "    \"\"\"\n",
    "    # Track failed vector queries\n",
    "    no_vector_count = 0\n",
    "    candidate_pool_vectors = []\n",
    "    for candidate in candidate_pool_titles:\n",
    "        candidate = normalize_text(candidate)\n",
    "        if verbose: print(candidate)\n",
    "        try:\n",
    "            candidate_vectors = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            # Keep empty vector representation to maintain index locations\n",
    "            candidate_vectors = np.zeros(100) # Note: this must change if using higher dimensional embeddings\n",
    "            no_vector_count += 1\n",
    "        candidate_pool_vectors.append(candidate_vectors)\n",
    "    \n",
    "    # Handle case where candidate pool is empty from prior phase\n",
    "    # Add 3 arrays of zeros of same dimensionality as embeddings\n",
    "    if len(candidate_pool_titles) == 0:\n",
    "        candidate_pool_vectors = [np.zeros(100), np.zeros(100), np.zeros(100)]\n",
    "    \n",
    "    if verbose: print(f\"Failed Wikipedia2Vec Entity Vector Queries: {no_vector_count}\")\n",
    "    return candidate_pool_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to retrieve entity vectors\n",
    "def create_entity_vector_dict(sentence_mention_ids, single_sentence_df, verbose=False):\n",
    "    \"\"\"\n",
    "    Function iterates over a provided list of congruent mentions,\n",
    "    finds the associated candidate pool for each mention\n",
    "    and returns the candidate pool in vector representation\n",
    "    \"\"\"\n",
    "    # Save vectors in dictionary\n",
    "    vector_dict = {}\n",
    "    \n",
    "    # For each full mention we are analyzing in the contextual domain\n",
    "    for m in sentence_mention_ids:\n",
    "        \n",
    "        # Retrieve candidate pool titles\n",
    "        candidate_pool_titles = single_sentence_df['candidate_pool_titles'][m]\n",
    "        if verbose: print(candidate_pool_titles)\n",
    "        \n",
    "        # Convert candidate pool titles to candidate pool vectors\n",
    "        candidate_pool_vectors = get_candidate_pool_vectors(candidate_pool_titles, verbose=verbose)\n",
    "        \n",
    "        # Save candidate pool vectors to dictionary\n",
    "        vector_dict[m] = candidate_pool_vectors\n",
    "    \n",
    "    if verbose:\n",
    "        print(vector_dict.keys())\n",
    "        for k in vector_dict.keys():\n",
    "            print(len(vector_dict[k]))\n",
    "\n",
    "    return vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate congruent metric for each candidate in every mention's candidate pool\n",
    "def get_congruence_dict(vector_dict, sentence_mention_ids, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return congruence metric calculations between all candidates for all mentions\n",
    "    Input:\n",
    "    - vector_dict\n",
    "    - Sentence Mentions Numerical Representation: Integers representing congruent mentions in a context domain\n",
    "    Outputs:\n",
    "    - Dictionary with congruence metric calculations for everyone\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Save congruence metrics in a two-level dictionary\n",
    "    # Create first-level dictionary to be returned\n",
    "    congruence_dict = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level congruence metric dictionary\n",
    "        m_dict = {}\n",
    "\n",
    "        # Compare eaech mention against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            if verbose: print(f\"Comparing mentions {m} & {n}\")\n",
    "            # Calculate congruence metric - cosine similarity\n",
    "            congruence_metric = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "            # Save congruence metric to second-level dictionary\n",
    "            m_dict[n] = congruence_metric\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        congruence_dict[m] = m_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "    \n",
    "#     if verbose: print(congruence_dict)\n",
    "    return congruence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to combine mention probabilities\n",
    "def combine_probabilities(mid_1, mid_2, sentence_probabilities):\n",
    "    \"\"\"\n",
    "    Function takes two mentions (numerically represented),\n",
    "    finds their candidate pool probabilities,\n",
    "    and combines them using the chosen logic,\n",
    "    returning a matrix of combined pair-wise probabilities\n",
    "    \"\"\"\n",
    "    # Prepare matrix to return\n",
    "    weights_matrix = []\n",
    "    \n",
    "    # Combine every candidate's probability for one mention with each for the second\n",
    "    for a in sentence_probabilities[mid_1]:\n",
    "        weights_row = []\n",
    "        for b in sentence_probabilities[mid_2]:\n",
    "            \n",
    "            ## Combination logic\n",
    "            weights_row.append(np.mean([a, b])) # Take mean of two prior probabilities\n",
    "        \n",
    "        weights_matrix.append(weights_row)\n",
    "        \n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create a weighted dictionary from priors\n",
    "def create_weighted_dict(sentence_mention_ids, single_sentence_df, verbose=False):\n",
    "    \"\"\"\n",
    "    Function iterates over a provided list of congruent mentions,\n",
    "    finds the associated candidate pool prior probabilities for each mention\n",
    "    and returns a dictionary for the combined probability of every pair of candidates\n",
    "    \"\"\"\n",
    "    # Save weights in dictionary\n",
    "    weights_dict = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level weights dictionary\n",
    "        w_dict = {}\n",
    "        \n",
    "        # Compare each mention probabilities against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            if verbose: print(f\"Comparing mentions {m} & {n}\")\n",
    "            # Return candidate pool probabilities\n",
    "            weights_matrix = combine_probabilities(m, n, single_sentence_df['candidate_pool_priors'])\n",
    "            if not weights_matrix: # Handles error where candidate pool is empty\n",
    "                weights_matrix = [0.0]\n",
    "            # Save weights matrix to second-level dictionary\n",
    "            w_dict[n] = weights_matrix\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        weights_dict[m] = w_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "                \n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to combine congruence and prior weights\n",
    "def combine_congruence_priors(sentence_mention_ids, congruence_dict, weights_dict, verbose=False):\n",
    "    \"\"\"\n",
    "    Function takes two dictionaries representing recursive metric calculations,\n",
    "    Multiplies them element-wise together,\n",
    "    Returns the updated table\n",
    "    \"\"\"\n",
    "   # Combine congruence with prior weights in first-level dictionary\n",
    "    weighted_congruence = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level dictionary\n",
    "        w_c_dict = {}\n",
    "        \n",
    "        # Compare each mention probabilities against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            try:\n",
    "                weighted_table = congruence_dict[m][n] * np.array(weights_dict[m][n])\n",
    "            except ValueError: # Handles error when candidate pool is empty\n",
    "                weighted_table = congruence_dict[m][n]\n",
    "            w_c_dict[n] = weighted_table\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        weighted_congruence[m] = w_c_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "    \n",
    "    return weighted_congruence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standardize on form lookup always row then column\n",
    "def get_most_congruent_pair(congruence_matrix, verbose=False):\n",
    "    \"\"\"\n",
    "    This function takes a congruence matrix and returns the indices\n",
    "    of the two most congruent candidates using your chosen metric.\n",
    "    These indices can be plugged back into the candidate pool lists\n",
    "    to determine which candidates are most similar.\n",
    "    \"\"\"\n",
    "    # Get max values for every row\n",
    "    max_row_values = congruence_matrix.max(axis=1)\n",
    "    max_row_idxs = congruence_matrix.argmax(axis=1)\n",
    "    \n",
    "    # Get overall max value and the row it is in\n",
    "    max_value = max_row_values.max()\n",
    "    max_row_idx = max_row_values.argmax()\n",
    "    \n",
    "    # Get column max value is in\n",
    "    max_column_idx = max_row_idxs[max_row_idx]\n",
    "    \n",
    "    return (max_row_idx, max_column_idx), max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the most congruent pair amongst remaining mentions\n",
    "def find_most_congruent_pair(mention_predictions, mentions_remaining, congruence_dict, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to search the congruence matrices for mentions without predictions\n",
    "    and return the most congruent pair of candidates and associated mentions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with empty most_congruent_pair\n",
    "    # (Candidate Pair, Congruence Metric), (Mention A, Mention B)\n",
    "    most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "    \n",
    "    # Assess whether first pass or recursive pass\n",
    "    if len(mention_predictions) == 0:\n",
    "\n",
    "        # First pass        \n",
    "        for m in mentions_remaining:\n",
    "            for n in mentions_remaining[m+1:]:\n",
    "\n",
    "                # Get most congruent pair in one matrix\n",
    "                congruent_pair = get_most_congruent_pair(congruence_dict[m][n], verbose=verbose)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Comparing {m} & {n}\")\n",
    "                    print(\"Congruent Pair: \", congruent_pair)\n",
    "                    print(\"Current Most Congruent: \", most_congruent_pair)\n",
    "                \n",
    "                if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "                    # Save most congruent candidate pair and mentions\n",
    "                    most_congruent_pair = congruent_pair, (m, n)\n",
    "\n",
    "    elif len(mention_predictions) > 0:\n",
    "        \n",
    "        # Second+, recursive pass\n",
    "        for m in mention_predictions.keys():\n",
    "            for n in mentions_remaining:\n",
    "                \n",
    "                # Becauase we always assume search small mention to large to save computation/storage,\n",
    "                # we must sort incrementing variables to be in increasing order for query\n",
    "                m_tmp, n_tmp = np.sort((m, n))\n",
    "                \n",
    "                # Get most congruent pair in one matrix\n",
    "                congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp], verbose=verbose)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "                    print(\"Congruent Pair: \", congruent_pair)\n",
    "                    print(\"Current Most Congruent: \", most_congruent_pair)\n",
    "                    \n",
    "                if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "                    # Save most congruent candidate pair and mentions\n",
    "                    most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "                \n",
    "    if verbose: print(\"Final Most Congruent Pair: \", most_congruent_pair)\n",
    "    return most_congruent_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congruent Predictions Function\n",
    "\n",
    "This is our main function that takes a sentence ID, calculates congruence for all candidates, updates the prior confidence from Phase 3 with that congruence and selects predictions iteratively based on that final number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate congruent predictions\n",
    "def get_congruent_predictions(sentence_id, dataframe, with_priors=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to calculate congruence metrics over a set of entity full mentions\n",
    "    and return the predicted candidates based on the congruent metric\n",
    "    Input:\n",
    "    - Sentence ID used to filter dataframe\n",
    "    - Dataframe over which to process\n",
    "    Output:\n",
    "    - Prediction for each entity mention\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to dataframe representing single sentence\n",
    "    # Drop duplicates necessary for sentences with the same mention included twice\n",
    "    single_sentence_df = dataframe[dataframe['sentence_id'] == sentence_id]\\\n",
    "                        .drop_duplicates(['full_mention', 'wikipedia_URL', 'wikipedia_page_ID', 'wikipedia_title'])\\\n",
    "                        .reset_index(drop=True)\n",
    "    if verbose: display(single_sentence_df)\n",
    "    \n",
    "    # Define numerical representation of congruent mention list\n",
    "    sentence_congruent_mentions = single_sentence_df['congruent_mentions'][0]\n",
    "    sentence_mention_ids = np.arange(len(sentence_congruent_mentions))\n",
    "    if verbose:\n",
    "        print(\"Congruent Mentions: \", sentence_congruent_mentions)\n",
    "        print(\"Congruent Mentions as numbers: \", sentence_mention_ids)\n",
    "    \n",
    "    # Retrieve dictionary of candidate pool vectors for each mention\n",
    "    vector_dict = create_entity_vector_dict(sentence_mention_ids, single_sentence_df, verbose=verbose)\n",
    "    if verbose: print(\"Mentions with Vectors: \", vector_dict.keys())\n",
    "        \n",
    "    # Calculate congruence metric for each candidate vector for each mention's candidate pool\n",
    "    # This notebook uses cosine similarity as the congruence metric\n",
    "    congruence_dict = get_congruence_dict(vector_dict, sentence_mention_ids, verbose=verbose)\n",
    "    if verbose: print(\"First-Level Congruence Keys: \", congruence_dict.keys())\n",
    "    # This should be one less than congruent mention count, since we are comparing low to high\n",
    "    # and thus don't compare the highest value to anything\n",
    "    \n",
    "    # Add flag for including priors\n",
    "    if with_priors:\n",
    "        \n",
    "        # Calculate weights matric for every pairwise combination of candidates for every mention\n",
    "        weights_dict = create_weighted_dict(sentence_mention_ids, single_sentence_df, verbose=verbose)\n",
    "\n",
    "        # Combine congruence with prior weights\n",
    "        vector_congruence = combine_congruence_priors(sentence_mention_ids, congruence_dict, weights_dict, verbose=verbose)\n",
    "\n",
    "    else:\n",
    "        vector_congruence = congruence_dict\n",
    "\n",
    "    # Create predictions dictionary\n",
    "    mention_predictions = {}\n",
    "    \n",
    "    # Create copy of sentence_mention_ids to iterate through\n",
    "    mentions_remaining = sentence_mention_ids.copy()\n",
    "    \n",
    "    # Iterate through congruent entity mentions to retrieve predictions\n",
    "    # where a prediction is the most congruent candidate between two mentions\n",
    "    while len(mentions_remaining) > 0:\n",
    "        \n",
    "        # Analyze congruence matrices to identify the most congruent pair\n",
    "        most_congruent_pair = find_most_congruent_pair(mention_predictions, mentions_remaining, vector_congruence, verbose=verbose)\n",
    "        \n",
    "        # Save most congrent pair prediction for associated mentions\n",
    "        if len(mention_predictions) == 0:\n",
    "            # First pass\n",
    "            if verbose: print(most_congruent_pair)\n",
    "            # Handles error when nearly all candidate pools are empty\n",
    "            if most_congruent_pair == (((None, None), 0.0), (0, 0)):\n",
    "                mention_predictions[0] = 0\n",
    "            else:\n",
    "                mention_predictions[most_congruent_pair[1][0]] = most_congruent_pair[0][0][0]\n",
    "                mention_predictions[most_congruent_pair[1][1]] = most_congruent_pair[0][0][1]\n",
    "        elif len(mention_predictions) > 0:\n",
    "            # Second_, recursive pass\n",
    "            \n",
    "            # Find new mention you're predicting for\n",
    "            try:\n",
    "                # The number left over in the mention tuple if you remove anything in the prediction dict\n",
    "                new_mention_num = most_congruent_pair[1].index(\\\n",
    "                                                               list(set(most_congruent_pair[1])\\\n",
    "                                                                    - set(mention_predictions.keys())))\n",
    "\n",
    "                # Save new prediction\n",
    "                mention_predictions[most_congruent_pair[1][new_mention_num]] = most_congruent_pair[0][0][new_mention_num]\n",
    "            except ValueError:\n",
    "                for mention in mentions_remaining:\n",
    "                    mention_predictions[mention] = 0 # Zero produces error in final section to produce None prediction\n",
    "            \n",
    "        # Update remaining mentions to mentions without a prediction stored in the dictionary\n",
    "        mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "        if verbose: print(mentions_remaining, mention_predictions.keys())\n",
    "    \n",
    "    if verbose: print(mention_predictions)\n",
    "    \n",
    "    # Use mention predictions to return titles\n",
    "    readable_predictions = {}\n",
    "    for k, v in mention_predictions.items():\n",
    "        if verbose: print(k, v)\n",
    "        readable_key = sentence_congruent_mentions[k]\n",
    "        try:\n",
    "            readable_value = single_sentence_df['candidate_pool_titles'][k][v]\n",
    "            readable_id = single_sentence_df['candidate_pool_page_ids'][k][v]\n",
    "        except IndexError: # Handles case where no candidate pool was provided from Phase 3\n",
    "            readable_value = None \n",
    "            readable_id = None\n",
    "        except TypeError:\n",
    "            # Handles case where no congruence can be calculated\n",
    "            # Either due to one mention in sentence or two mentions but one with no candidate pool\n",
    "            readable_value = single_sentence_df['candidate_pool_titles'][0][0] # Just return top value from Phase 3\n",
    "            readable_id = single_sentence_df['candidate_pool_page_ids'][0][0]\n",
    "        if verbose: print(readable_key, readable_value, readable_id)\n",
    "        readable_predictions[readable_key] = (readable_value, readable_id)\n",
    "    \n",
    "    # Output dictionary with predictions for each entity mention based on congruence\n",
    "    return readable_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>YORK</td>\n",
       "      <td>http://en.wikipedia.org/wiki/York</td>\n",
       "      <td>34361.0</td>\n",
       "      <td>York</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>york</td>\n",
       "      <td>[34361, 134305, 974198, 992970, 2436511]</td>\n",
       "      <td>[42462, 821105, 8055519, 21008612, 994000]</td>\n",
       "      <td>[York, York,_Pennsylvania, York_Racecourse, Yo...</td>\n",
       "      <td>[0.5884024, 0.0486891, 0.0466228, 0.0396487, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>England</td>\n",
       "      <td>http://en.wikipedia.org/wiki/England</td>\n",
       "      <td>9316.0</td>\n",
       "      <td>England</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>england</td>\n",
       "      <td>[9316, 9904, 759125, 691024, 990422]</td>\n",
       "      <td>[21, 47762, 1321565, 378628, 3589698]</td>\n",
       "      <td>[England, England_national_football_team, Engl...</td>\n",
       "      <td>[0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mark_Prescott</td>\n",
       "      <td>25674876.0</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>mark prescott</td>\n",
       "      <td>[25674876]</td>\n",
       "      <td>[6769327]</td>\n",
       "      <td>[Mark_Prescott]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Pivotal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>pivotal</td>\n",
       "      <td>[39421721, 39467663, 34382035, 25015249, 2768719]</td>\n",
       "      <td>[11331695, 16335785, 7180990, 5503397, 15148967]</td>\n",
       "      <td>[Pivotal_(horse), Pivotal_Software, Phases_of_...</td>\n",
       "      <td>[0.6304348, 0.2608696, 0.0217391, 0.0217391, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Nunthorpe_Stakes</td>\n",
       "      <td>727606.0</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>nunthorpe stakes</td>\n",
       "      <td>[727606]</td>\n",
       "      <td>[3346442]</td>\n",
       "      <td>[Nunthorpe_Stakes]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                  wikipedia_URL  \\\n",
       "0       B              YORK              http://en.wikipedia.org/wiki/York   \n",
       "1       B           England           http://en.wikipedia.org/wiki/England   \n",
       "2       B     Mark Prescott     http://en.wikipedia.org/wiki/Mark_Prescott   \n",
       "3       B           Pivotal                                            NaN   \n",
       "4       B  Nunthorpe Stakes  http://en.wikipedia.org/wiki/Nunthorpe_Stakes   \n",
       "\n",
       "   wikipedia_page_ID   wikipedia_title  sentence_id  doc_id  \\\n",
       "0            34361.0              York          210      31   \n",
       "1             9316.0           England          210      31   \n",
       "2         25674876.0     Mark Prescott          210      31   \n",
       "3                NaN               NaN          210      31   \n",
       "4           727606.0  Nunthorpe Stakes          210      31   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [YORK, England, Mark Prescott, Pivotal, Nuntho...              york   \n",
       "1  [YORK, England, Mark Prescott, Pivotal, Nuntho...           england   \n",
       "2  [YORK, England, Mark Prescott, Pivotal, Nuntho...     mark prescott   \n",
       "3  [YORK, England, Mark Prescott, Pivotal, Nuntho...           pivotal   \n",
       "4  [YORK, England, Mark Prescott, Pivotal, Nuntho...  nunthorpe stakes   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0           [34361, 134305, 974198, 992970, 2436511]   \n",
       "1               [9316, 9904, 759125, 691024, 990422]   \n",
       "2                                         [25674876]   \n",
       "3  [39421721, 39467663, 34382035, 25015249, 2768719]   \n",
       "4                                           [727606]   \n",
       "\n",
       "                            candidate_pool_item_ids  \\\n",
       "0        [42462, 821105, 8055519, 21008612, 994000]   \n",
       "1             [21, 47762, 1321565, 378628, 3589698]   \n",
       "2                                         [6769327]   \n",
       "3  [11331695, 16335785, 7180990, 5503397, 15148967]   \n",
       "4                                         [3346442]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [York, York,_Pennsylvania, York_Racecourse, Yo...   \n",
       "1  [England, England_national_football_team, Engl...   \n",
       "2                                    [Mark_Prescott]   \n",
       "3  [Pivotal_(horse), Pivotal_Software, Phases_of_...   \n",
       "4                                 [Nunthorpe_Stakes]   \n",
       "\n",
       "                               candidate_pool_priors  \n",
       "0  [0.5884024, 0.0486891, 0.0466228, 0.0396487, 0...  \n",
       "1  [0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...  \n",
       "2                                              [1.0]  \n",
       "3  [0.6304348, 0.2608696, 0.0217391, 0.0217391, 0...  \n",
       "4                                              [1.0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congruent Mentions:  ['YORK', 'England', 'Mark Prescott', 'Pivotal', 'Nunthorpe Stakes']\n",
      "Congruent Mentions as numbers:  [0 1 2 3 4]\n",
      "['York', 'York,_Pennsylvania', 'York_Racecourse', 'York_Wasps', 'York,_Western_Australia']\n",
      "York\n",
      "York, Pennsylvania\n",
      "York Racecourse\n",
      "York Wasps\n",
      "York, Western Australia\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['England', 'England_national_football_team', 'England_cricket_team', 'England_national_rugby_union_team', 'England_national_rugby_league_team']\n",
      "England\n",
      "England national football team\n",
      "England cricket team\n",
      "England national rugby union team\n",
      "England national rugby league team\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['Mark_Prescott']\n",
      "Mark Prescott\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['Pivotal_(horse)', 'Pivotal_Software', 'Phases_of_clinical_research', 'Fricke_v._Lynch', 'Pivotal']\n",
      "Pivotal (horse)\n",
      "Pivotal Software\n",
      "Phases of clinical research\n",
      "Fricke v. Lynch\n",
      "Pivotal\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "['Nunthorpe_Stakes']\n",
      "Nunthorpe Stakes\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "dict_keys([0, 1, 2, 3, 4])\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "Mentions with Vectors:  dict_keys([0, 1, 2, 3, 4])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 0 & 3\n",
      "Comparing mentions 0 & 4\n",
      "Comparing mentions 1 & 2\n",
      "Comparing mentions 1 & 3\n",
      "Comparing mentions 1 & 4\n",
      "Comparing mentions 2 & 3\n",
      "Comparing mentions 2 & 4\n",
      "Comparing mentions 3 & 4\n",
      "First-Level Congruence Keys:  dict_keys([0, 1, 2, 3])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 0 & 3\n",
      "Comparing mentions 0 & 4\n",
      "Comparing mentions 1 & 2\n",
      "Comparing mentions 1 & 3\n",
      "Comparing mentions 1 & 4\n",
      "Comparing mentions 2 & 3\n",
      "Comparing mentions 2 & 4\n",
      "Comparing mentions 3 & 4\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((0, 0), 0.25858797358032914)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((0, 0), 0.37014722010928397)\n",
      "Current Most Congruent:  (((0, 0), 0.25858797358032914), (0, 1))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((2, 0), 0.23136763545781952)\n",
      "Current Most Congruent:  (((0, 0), 0.37014722010928397), (0, 2))\n",
      "Comparing 0 & 4\n",
      "Congruent Pair:  ((2, 0), 0.40144850716092584)\n",
      "Current Most Congruent:  (((0, 0), 0.37014722010928397), (0, 2))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.261977061977385)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 0), 0.20199003368640245)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Comparing 1 & 4\n",
      "Congruent Pair:  ((0, 0), 0.24040039331027865)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.5795273173607773)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Comparing 2 & 4\n",
      "Congruent Pair:  ((0, 0), 0.6659345030784607)\n",
      "Current Most Congruent:  (((0, 0), 0.5795273173607773), (2, 3))\n",
      "Comparing 3 & 4\n",
      "Congruent Pair:  ((0, 0), 0.6558879848280214)\n",
      "Current Most Congruent:  (((0, 0), 0.6659345030784607), (2, 4))\n",
      "Final Most Congruent Pair:  (((0, 0), 0.6659345030784607), (2, 4))\n",
      "(((0, 0), 0.6659345030784607), (2, 4))\n",
      "[0, 1, 3] dict_keys([2, 4])\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((0, 0), 0.37014722010928397)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.261977061977385)\n",
      "Current Most Congruent:  (((0, 0), 0.37014722010928397), (0, 2))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.5795273173607773)\n",
      "Current Most Congruent:  (((0, 0), 0.37014722010928397), (0, 2))\n",
      "Comparing 0 & 4\n",
      "Congruent Pair:  ((2, 0), 0.40144850716092584)\n",
      "Current Most Congruent:  (((0, 0), 0.5795273173607773), (2, 3))\n",
      "Comparing 1 & 4\n",
      "Congruent Pair:  ((0, 0), 0.24040039331027865)\n",
      "Current Most Congruent:  (((0, 0), 0.5795273173607773), (2, 3))\n",
      "Comparing 3 & 4\n",
      "Congruent Pair:  ((0, 0), 0.6558879848280214)\n",
      "Current Most Congruent:  (((0, 0), 0.5795273173607773), (2, 3))\n",
      "Final Most Congruent Pair:  (((0, 0), 0.6558879848280214), (3, 4))\n",
      "[0, 1] dict_keys([2, 4, 3])\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((0, 0), 0.37014722010928397)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.261977061977385)\n",
      "Current Most Congruent:  (((0, 0), 0.37014722010928397), (0, 2))\n",
      "Comparing 0 & 4\n",
      "Congruent Pair:  ((2, 0), 0.40144850716092584)\n",
      "Current Most Congruent:  (((0, 0), 0.37014722010928397), (0, 2))\n",
      "Comparing 1 & 4\n",
      "Congruent Pair:  ((0, 0), 0.24040039331027865)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((2, 0), 0.23136763545781952)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 0), 0.20199003368640245)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Final Most Congruent Pair:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "[1] dict_keys([2, 4, 3, 0])\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.261977061977385)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 4\n",
      "Congruent Pair:  ((0, 0), 0.24040039331027865)\n",
      "Current Most Congruent:  (((0, 0), 0.261977061977385), (1, 2))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 0), 0.20199003368640245)\n",
      "Current Most Congruent:  (((0, 0), 0.261977061977385), (1, 2))\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((0, 0), 0.25858797358032914)\n",
      "Current Most Congruent:  (((0, 0), 0.261977061977385), (1, 2))\n",
      "Final Most Congruent Pair:  (((0, 0), 0.261977061977385), (1, 2))\n",
      "[] dict_keys([2, 4, 3, 0, 1])\n",
      "{2: 0, 4: 0, 3: 0, 0: 2, 1: 0}\n",
      "2 0\n",
      "Mark Prescott Mark_Prescott 25674876\n",
      "4 0\n",
      "Nunthorpe Stakes Nunthorpe_Stakes 727606\n",
      "3 0\n",
      "Pivotal Pivotal_(horse) 39421721\n",
      "0 2\n",
      "YORK York_Racecourse 974198\n",
      "1 0\n",
      "England England 9316\n",
      "{'Mark Prescott': ('Mark_Prescott', 25674876), 'Nunthorpe Stakes': ('Nunthorpe_Stakes', 727606), 'Pivotal': ('Pivotal_(horse)', 39421721), 'YORK': ('York_Racecourse', 974198), 'England': ('England', 9316)}\n",
      "CPU times: user 31.3 ms, sys: 4.68 ms, total: 36 ms\n",
      "Wall time: 34.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test out function\n",
    "sentence_id = 210\n",
    "congruent_predictions = get_congruent_predictions(sentence_id=sentence_id, dataframe=full_mentions, with_priors=True, verbose=True)\n",
    "print(congruent_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>YORK</td>\n",
       "      <td>http://en.wikipedia.org/wiki/York</td>\n",
       "      <td>34361.0</td>\n",
       "      <td>York</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>york</td>\n",
       "      <td>[34361, 134305, 974198, 992970, 2436511]</td>\n",
       "      <td>[42462, 821105, 8055519, 21008612, 994000]</td>\n",
       "      <td>[York, York,_Pennsylvania, York_Racecourse, Yo...</td>\n",
       "      <td>[0.5884024, 0.0486891, 0.0466228, 0.0396487, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>England</td>\n",
       "      <td>http://en.wikipedia.org/wiki/England</td>\n",
       "      <td>9316.0</td>\n",
       "      <td>England</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>england</td>\n",
       "      <td>[9316, 9904, 759125, 691024, 990422]</td>\n",
       "      <td>[21, 47762, 1321565, 378628, 3589698]</td>\n",
       "      <td>[England, England_national_football_team, Engl...</td>\n",
       "      <td>[0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mark_Prescott</td>\n",
       "      <td>25674876.0</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>mark prescott</td>\n",
       "      <td>[25674876]</td>\n",
       "      <td>[6769327]</td>\n",
       "      <td>[Mark_Prescott]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mark_Prescott</td>\n",
       "      <td>25674876.0</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>mark prescott</td>\n",
       "      <td>[25674876]</td>\n",
       "      <td>[6769327]</td>\n",
       "      <td>[Mark_Prescott]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>Pivotal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>pivotal</td>\n",
       "      <td>[39421721, 39467663, 34382035, 25015249, 2768719]</td>\n",
       "      <td>[11331695, 16335785, 7180990, 5503397, 15148967]</td>\n",
       "      <td>[Pivotal_(horse), Pivotal_Software, Phases_of_...</td>\n",
       "      <td>[0.6304348, 0.2608696, 0.0217391, 0.0217391, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Nunthorpe_Stakes</td>\n",
       "      <td>727606.0</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>nunthorpe stakes</td>\n",
       "      <td>[727606]</td>\n",
       "      <td>[3346442]</td>\n",
       "      <td>[Nunthorpe_Stakes]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Nunthorpe_Stakes</td>\n",
       "      <td>727606.0</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>nunthorpe stakes</td>\n",
       "      <td>[727606]</td>\n",
       "      <td>[3346442]</td>\n",
       "      <td>[Nunthorpe_Stakes]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                  wikipedia_URL  \\\n",
       "0       B              YORK              http://en.wikipedia.org/wiki/York   \n",
       "1       B           England           http://en.wikipedia.org/wiki/England   \n",
       "2       B     Mark Prescott     http://en.wikipedia.org/wiki/Mark_Prescott   \n",
       "3       I     Mark Prescott     http://en.wikipedia.org/wiki/Mark_Prescott   \n",
       "4       B           Pivotal                                            NaN   \n",
       "5       B  Nunthorpe Stakes  http://en.wikipedia.org/wiki/Nunthorpe_Stakes   \n",
       "6       I  Nunthorpe Stakes  http://en.wikipedia.org/wiki/Nunthorpe_Stakes   \n",
       "\n",
       "   wikipedia_page_ID   wikipedia_title  sentence_id  doc_id  \\\n",
       "0            34361.0              York          210      31   \n",
       "1             9316.0           England          210      31   \n",
       "2         25674876.0     Mark Prescott          210      31   \n",
       "3         25674876.0     Mark Prescott          210      31   \n",
       "4                NaN               NaN          210      31   \n",
       "5           727606.0  Nunthorpe Stakes          210      31   \n",
       "6           727606.0  Nunthorpe Stakes          210      31   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [YORK, England, Mark Prescott, Pivotal, Nuntho...              york   \n",
       "1  [YORK, England, Mark Prescott, Pivotal, Nuntho...           england   \n",
       "2  [YORK, England, Mark Prescott, Pivotal, Nuntho...     mark prescott   \n",
       "3  [YORK, England, Mark Prescott, Pivotal, Nuntho...     mark prescott   \n",
       "4  [YORK, England, Mark Prescott, Pivotal, Nuntho...           pivotal   \n",
       "5  [YORK, England, Mark Prescott, Pivotal, Nuntho...  nunthorpe stakes   \n",
       "6  [YORK, England, Mark Prescott, Pivotal, Nuntho...  nunthorpe stakes   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0           [34361, 134305, 974198, 992970, 2436511]   \n",
       "1               [9316, 9904, 759125, 691024, 990422]   \n",
       "2                                         [25674876]   \n",
       "3                                         [25674876]   \n",
       "4  [39421721, 39467663, 34382035, 25015249, 2768719]   \n",
       "5                                           [727606]   \n",
       "6                                           [727606]   \n",
       "\n",
       "                            candidate_pool_item_ids  \\\n",
       "0        [42462, 821105, 8055519, 21008612, 994000]   \n",
       "1             [21, 47762, 1321565, 378628, 3589698]   \n",
       "2                                         [6769327]   \n",
       "3                                         [6769327]   \n",
       "4  [11331695, 16335785, 7180990, 5503397, 15148967]   \n",
       "5                                         [3346442]   \n",
       "6                                         [3346442]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [York, York,_Pennsylvania, York_Racecourse, Yo...   \n",
       "1  [England, England_national_football_team, Engl...   \n",
       "2                                    [Mark_Prescott]   \n",
       "3                                    [Mark_Prescott]   \n",
       "4  [Pivotal_(horse), Pivotal_Software, Phases_of_...   \n",
       "5                                 [Nunthorpe_Stakes]   \n",
       "6                                 [Nunthorpe_Stakes]   \n",
       "\n",
       "                               candidate_pool_priors  \n",
       "0  [0.5884024, 0.0486891, 0.0466228, 0.0396487, 0...  \n",
       "1  [0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...  \n",
       "2                                              [1.0]  \n",
       "3                                              [1.0]  \n",
       "4  [0.6304348, 0.2608696, 0.0217391, 0.0217391, 0...  \n",
       "5                                              [1.0]  \n",
       "6                                              [1.0]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define testing sentence_id\n",
    "single_sentence_df = full_mentions[full_mentions['sentence_id'] == sentence_id].reset_index(drop=True)\n",
    "single_sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YORK', 'England', 'Mark Prescott', 'Pivotal', 'Nunthorpe Stakes']\n"
     ]
    }
   ],
   "source": [
    "print(single_sentence_df['congruent_mentions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Prescott -> 25674876 =? 25674876\n",
      "Nunthorpe Stakes -> 727606 =? 727606\n",
      "Pivotal -> None =? 39421721\n",
      "YORK -> 34361 =? 974198\n",
      "England -> 9316 =? 9316\n",
      "*************************************************\n",
      "This congruent experiment is 75.0% accurate comparing page IDs.\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "no_true = 0\n",
    "for mention, pred in congruent_predictions.items():\n",
    "    pred_page_id = pred[1]\n",
    "    try:\n",
    "        true_page_id = int(single_sentence_df[single_sentence_df['full_mention'] == mention]['wikipedia_page_ID'].values[0])\n",
    "    except ValueError:\n",
    "        true_page_id = None\n",
    "        no_true += 1\n",
    "    print(mention, \"->\", true_page_id, \"=?\", pred_page_id)\n",
    "    if single_sentence_df[single_sentence_df['full_mention'] == mention]['wikipedia_page_ID'].values[0] == pred_page_id:\n",
    "        accuracy += 1\n",
    "print(\"*************************************************\")\n",
    "print(f\"This congruent experiment is {round(accuracy/(len(congruent_predictions)-no_true)*100,3)}% accurate comparing page IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Congruence Predictions and Assess Accuracy over Entire Dataframe\n",
    "\n",
    "We now apply the per-sentence structure over the whole ACY dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 4,438 sentences to predict.\n"
     ]
    }
   ],
   "source": [
    "# Max sentence_id in dataframe\n",
    "max_sentence_id = len(full_mentions['sentence_id'].unique())\n",
    "print(\"We have {:,} sentences to predict.\".format(max_sentence_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate accuracy\n",
    "def calculate_accuracy(predictions, dataframe=full_mentions, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to calculate accuracy over generated predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize tracker metrics\n",
    "    accurate_predictions = 0\n",
    "    accurate_present = 0\n",
    "    \n",
    "    # Iterate through each full mention\n",
    "    for row in tqdm(range(len(dataframe))):\n",
    "        mention_df = dataframe.iloc[row]\n",
    "        \n",
    "        # Save key values\n",
    "        sid = mention_df['sentence_id']\n",
    "        fm = mention_df['full_mention']\n",
    "        title = mention_df['wikipedia_title']\n",
    "        page_id = mention_df['wikipedia_page_ID']\n",
    "        candidate_pool_page_ids = mention_df['candidate_pool_page_ids']\n",
    "        \n",
    "        # Retrieve prediction\n",
    "        pred = predictions[sid][fm]\n",
    "        norm_pred_title = normalize_text(pred[0])\n",
    "        pred_page_id = pred[1]\n",
    "        \n",
    "        # Print comparison (useful for subset review)\n",
    "        if verbose:\n",
    "            print(fm, sid, \"||| True:\", title, page_id, \"==? Pred:\", norm_pred_title, pred_page_id, \"|||\",\\\n",
    "            norm_pred_title==title, pred_page_id==page_id, \" ||| Present? \", (page_id in candidate_pool_page_ids))\n",
    "        \n",
    "        # Compare true vs prediction\n",
    "        if page_id == pred_page_id:\n",
    "            accurate_predictions += 1\n",
    "        if page_id in candidate_pool_page_ids:\n",
    "            accurate_present += 1\n",
    "        \n",
    "    # Print results\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"Predictive Accuracy: {}%\".format(round(accurate_predictions/len(dataframe)*100, 3)))\n",
    "    print(\"Answer Present: {}%\".format(round(accurate_present/len(dataframe)*100, 3)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congruence Accuracy without considering Prior Confidence\n",
    "\n",
    "We first experiment with calculating congruence accuracy without directly incorporating prior confidence. We still indirectly incorporate it since we took the Top N most confidence (highest ranked) values in Phase 3, but after creating the Top N list, we don't explicitly incorporate it in our calculation for final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4438/4438 [00:21<00:00, 207.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "congruent_predictions_freq_nopr = {}\n",
    "for sid in tqdm(full_mentions['sentence_id'].unique()):\n",
    "    congruent_predictions_freq_nopr[sid] = get_congruent_predictions(sid, dataframe=full_mentions,\n",
    "                                                                     with_priors=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21810/21810 [00:02<00:00, 7400.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predictive Accuracy: 42.985%\n",
      "Answer Present: 65.878%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_freq_nopr, dataframe=full_mentions, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy only with Full Mentions with Known True\n",
    "\n",
    "This is a better reflection of our success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mentions remaining:  15918\n"
     ]
    }
   ],
   "source": [
    "# Filter dataframe to only full mentions with known true values\n",
    "known_true_mentions = full_mentions[full_mentions['wikipedia_page_ID'].notnull()].reset_index()\n",
    "print(\"Full Mentions remaining: \", len(known_true_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15918/15918 [00:02<00:00, 7342.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predictive Accuracy: 58.896%\n",
      "Answer Present: 90.263%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_freq_nopr, dataframe=known_true_mentions, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1898.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 930, Number of Mentions: 3\n",
      "------------------------------------------\n",
      "Angels 930 ||| True: Los Angeles Angels of Anaheim 1360083.0 ==? Pred: Los Angeles Angels 1360083 ||| False True  ||| Present?  True\n",
      "Kenny Rogers 930 ||| True: Kenny Rogers (baseball) 599018.0 ==? Pred: Kenny Rogers' perfect game 35639937 ||| False False  ||| Present?  True\n",
      "Kenny Rogers 930 ||| True: Kenny Rogers (baseball) 599018.0 ==? Pred: Kenny Rogers' perfect game 35639937 ||| False False  ||| Present?  True\n",
      "------------------------------------------\n",
      "Predictive Accuracy: 33.333%\n",
      "Answer Present: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to random sentence for manual review\n",
    "rand_sid = np.random.choice(known_true_mentions['sentence_id'])\n",
    "rand_sentence_df = known_true_mentions[known_true_mentions['sentence_id'] == rand_sid]\n",
    "print(f\"Sentence ID: {rand_sid}, Number of Mentions: {len(rand_sentence_df)}\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_freq_nopr, dataframe=rand_sentence_df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congruence Accuracy considering Prior Confidence\n",
    "\n",
    "Now, we directly incorporate prior confidence by combining it during the prediction process with our calculated congruent metric. This involves \"discounting\" congruence by the prior confidence we had in each combination's component candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4438/4438 [00:26<00:00, 167.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "congruent_predictions_freq_pr = {}\n",
    "for sid in tqdm(full_mentions['sentence_id'].unique()):\n",
    "    congruent_predictions_freq_pr[sid] = get_congruent_predictions(sid, dataframe=full_mentions,\n",
    "                                                                   with_priors=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21810/21810 [00:03<00:00, 7115.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predictive Accuracy: 55.296%\n",
      "Answer Present: 65.878%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_freq_pr, dataframe=full_mentions, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy only with Full Mentions with Known True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15918/15918 [00:02<00:00, 7066.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predictive Accuracy: 75.763%\n",
      "Answer Present: 90.263%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_freq_pr, dataframe=known_true_mentions, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1643.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 5436, Number of Mentions: 10\n",
      "------------------------------------------\n",
      "Ajax 5436 ||| True: AFC Ajax 2273.0 ==? Pred: AFC Ajax 2273 ||| True True  ||| Present?  True\n",
      "AZ Alkmaar 5436 ||| True: AZ (football club) 2289.0 ==? Pred: AZ Alkmaar 2289 ||| False True  ||| Present?  True\n",
      "AZ Alkmaar 5436 ||| True: AZ (football club) 2289.0 ==? Pred: AZ Alkmaar 2289 ||| False True  ||| Present?  True\n",
      "Feyenoord 5436 ||| True: Feyenoord 47707.0 ==? Pred: Feyenoord 47707 ||| True True  ||| Present?  True\n",
      "UEFA Cup 5436 ||| True: 1996–97 UEFA Cup 4918282.0 ==? Pred: UEFA Europa League 232175 ||| False False  ||| Present?  False\n",
      "UEFA Cup 5436 ||| True: 1996–97 UEFA Cup 4918282.0 ==? Pred: UEFA Europa League 232175 ||| False False  ||| Present?  False\n",
      "Tenerife 5436 ||| True: CD Tenerife 1769413.0 ==? Pred: Tenerife 269223 ||| False False  ||| Present?  True\n",
      "De Graafschap Doetinchem 5436 ||| True: De Graafschap 1218784.0 ==? Pred: None None ||| False False  ||| Present?  False\n",
      "De Graafschap Doetinchem 5436 ||| True: De Graafschap 1218784.0 ==? Pred: None None ||| False False  ||| Present?  False\n",
      "De Graafschap Doetinchem 5436 ||| True: De Graafschap 1218784.0 ==? Pred: None None ||| False False  ||| Present?  False\n",
      "------------------------------------------\n",
      "Predictive Accuracy: 40.0%\n",
      "Answer Present: 50.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to random sentence for manual review\n",
    "rand_sid = np.random.choice(known_true_mentions['sentence_id'])\n",
    "rand_sentence_df = known_true_mentions[known_true_mentions['sentence_id'] == rand_sid]\n",
    "print(f\"Sentence ID: {rand_sid}, Number of Mentions: {len(rand_sentence_df)}\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_freq_pr, dataframe=rand_sentence_df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We achieved known-true predictive accucracy of 75.763, which is a 3.731% performance improvement using congruence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate with *Popularity* Predictions - Known True Only\n",
    "\n",
    "We also want to see how much of an impact congruence can have on candidate pools produced via page popularity anchor link statistics. These are considered less effective because of large skews that can happen if a word is linked to a very popular page just a single time. Our hypothesis is that congruence may have a larger impact on popularity mentions and to assess this, we compare accuracy with and without prior incorporation for known trues only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 9891, 9472, 10890716]</td>\n",
       "      <td>[458, 46, 45003, 4916, 185441]</td>\n",
       "      <td>['European_Union', 'Europe', 'Entropy', 'Euro'...</td>\n",
       "      <td>[0.2245141, 0.2015168, 0.0965482, 0.0768826, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 27318, 21148, 21212, 26964606]</td>\n",
       "      <td>[183, 334, 55, 7318, 40]</td>\n",
       "      <td>['Germany', 'Singapore', 'Netherlands', 'Nazi_...</td>\n",
       "      <td>[0.0558084, 0.0548498, 0.044104, 0.0268639, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>british</td>\n",
       "      <td>[3434750, 31717, 19344654, 26061, 8569916]</td>\n",
       "      <td>[30, 145, 9531, 172771, 1860]</td>\n",
       "      <td>['United_States', 'United_Kingdom', 'BBC', 'Ro...</td>\n",
       "      <td>[0.1243038, 0.0660245, 0.0317426, 0.0291264, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 56873217, 9643132]</td>\n",
       "      <td>[2073954, 26634508, 7172840]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.6296296, 0.3703704, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 56873217, 9643132]</td>\n",
       "      <td>[2073954, 26634508, 7172840]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.6296296, 0.3703704, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention     full_mention                                wikipedia_URL  \\\n",
       "0       B               EU                                          NaN   \n",
       "1       B           German         http://en.wikipedia.org/wiki/Germany   \n",
       "2       B          British  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "3       B  Peter Blackburn                                          NaN   \n",
       "4       I  Peter Blackburn                                          NaN   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0                NaN             NaN            0       0   \n",
       "1            11867.0         Germany            0       0   \n",
       "2            31717.0  United Kingdom            0       0   \n",
       "3                NaN             NaN            1       0   \n",
       "4                NaN             NaN            1       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0                        ['EU', 'German', 'British']                eu   \n",
       "1                        ['EU', 'German', 'British']            german   \n",
       "2                        ['EU', 'German', 'British']           british   \n",
       "3  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "4  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "\n",
       "                      candidate_pool_page_ids         candidate_pool_item_ids  \\\n",
       "0          [9317, 9239, 9891, 9472, 10890716]  [458, 46, 45003, 4916, 185441]   \n",
       "1      [11867, 27318, 21148, 21212, 26964606]        [183, 334, 55, 7318, 40]   \n",
       "2  [3434750, 31717, 19344654, 26061, 8569916]   [30, 145, 9531, 172771, 1860]   \n",
       "3               [56783206, 56873217, 9643132]    [2073954, 26634508, 7172840]   \n",
       "4               [56783206, 56873217, 9643132]    [2073954, 26634508, 7172840]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  ['European_Union', 'Europe', 'Entropy', 'Euro'...   \n",
       "1  ['Germany', 'Singapore', 'Netherlands', 'Nazi_...   \n",
       "2  ['United_States', 'United_Kingdom', 'BBC', 'Ro...   \n",
       "3  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "4  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "\n",
       "                               candidate_pool_priors  \n",
       "0  [0.2245141, 0.2015168, 0.0965482, 0.0768826, 0...  \n",
       "1  [0.0558084, 0.0548498, 0.044104, 0.0268639, 0....  \n",
       "2  [0.1243038, 0.0660245, 0.0317426, 0.0291264, 0...  \n",
       "3                        [0.6296296, 0.3703704, 0.0]  \n",
       "4                        [0.6296296, 0.3703704, 0.0]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "full_mentions_pop = pd.read_csv(os.path.join(preds_path, \"anchortext_popularity.csv\"), delimiter=\",\")\n",
    "full_mentions_pop.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "After ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "Before [56783206, 56873217, 9643132]\n",
      "After [56783206, 56873217, 9643132]\n",
      "Before [2073954, 26634508, 7172840]\n",
      "After [2073954, 26634508, 7172840]\n",
      "Before ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(MP)', 'Peter_Blackburn_(bishop)']\n",
      "After ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(MP)', 'Peter_Blackburn_(bishop)']\n",
      "Before [0.6296296, 0.3703704, 0.0]\n",
      "After [0.6296296, 0.3703704, 0.0]\n",
      "CPU times: user 291 ms, sys: 15.4 ms, total: 307 ms\n",
      "Wall time: 307 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Apply defined function to entire dataframe for all candidate pool columns\n",
    "\n",
    "column = 'congruent_mentions'\n",
    "print(\"Before\", full_mentions_pop[column][3])\n",
    "parsed_candidate_pool = full_mentions_pop[column].apply(parse_list_string, value_type=str)\n",
    "full_mentions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions_pop[column][3])\n",
    "\n",
    "column = 'candidate_pool_page_ids'\n",
    "print(\"Before\", full_mentions_pop[column][3])\n",
    "parsed_candidate_pool = full_mentions_pop[column].apply(parse_list_string, value_type=int)\n",
    "full_mentions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions_pop[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_item_ids'\n",
    "print(\"Before\", full_mentions_pop[column][3])\n",
    "parsed_candidate_pool = full_mentions_pop[column].apply(parse_list_string, value_type=int)\n",
    "full_mentions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions_pop[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_titles'\n",
    "print(\"Before\", full_mentions_pop[column][3])\n",
    "parsed_candidate_pool = full_mentions_pop[column].apply(parse_list_string, value_type=str)\n",
    "full_mentions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions_pop[column][3])\n",
    "\n",
    "column = 'candidate_pool_priors'\n",
    "print(\"Before\", full_mentions_pop[column][3])\n",
    "parsed_candidate_pool = full_mentions_pop[column].apply(parse_list_string, value_type=float)\n",
    "full_mentions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions_pop[column][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congruence Accuracy without considering Prior Confidence\n",
    "\n",
    "We first experiment with calculating congruence accuracy without directly incorporating prior confidence. We still indirectly incorporate it since we took the Top N most confidence (highest ranked) values in Phase 3, but after creating the Top N list, we don't explicitly incorporate it in our calculation for final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4438/4438 [00:21<00:00, 203.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "congruent_predictions_pop_nopr = {}\n",
    "for sid in tqdm(full_mentions_pop['sentence_id'].unique()):\n",
    "    congruent_predictions_pop_nopr[sid] = get_congruent_predictions(sid, dataframe=full_mentions_pop,\n",
    "                                                                     with_priors=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mentions remaining:  15918\n"
     ]
    }
   ],
   "source": [
    "# Filter dataframe to only full mentions with known true values\n",
    "known_true_mentions_pop = full_mentions_pop[full_mentions_pop['wikipedia_page_ID'].notnull()].reset_index()\n",
    "print(\"Full Mentions remaining: \", len(known_true_mentions_pop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15918/15918 [00:02<00:00, 7208.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predictive Accuracy: 58.443%\n",
      "Answer Present: 85.997%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_pop_nopr, dataframe=known_true_mentions_pop, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 1496.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 4310, Number of Mentions: 7\n",
      "------------------------------------------\n",
      "S. Waugh 4310 ||| True: Steve Waugh 156123.0 ==? Pred: None None ||| False False  ||| Present?  False\n",
      "S. Waugh 4310 ||| True: Steve Waugh 156123.0 ==? Pred: None None ||| False False  ||| Present?  False\n",
      "Law 4310 ||| True: Stuart Law 2047175.0 ==? Pred: Ten Commandments 2539671 ||| False False  ||| Present?  False\n",
      "McGrath 4310 ||| True: Glenn McGrath 423488.0 ==? Pred: Glenn McGrath 423488 ||| True True  ||| Present?  True\n",
      "Gillespie 4310 ||| True: Jason Gillespie 409561.0 ==? Pred: Jason Gillespie 409561 ||| True True  ||| Present?  True\n",
      "Lehmann 4310 ||| True: Darren Lehmann 994713.0 ==? Pred: Darren Lehmann 994713 ||| True True  ||| Present?  True\n",
      "Bevan 4310 ||| True: Michael Bevan 284900.0 ==? Pred: Bevan Docherty 947381 ||| False False  ||| Present?  False\n",
      "------------------------------------------\n",
      "Predictive Accuracy: 42.857%\n",
      "Answer Present: 42.857%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to random sentence for manual review\n",
    "rand_sid = np.random.choice(known_true_mentions_pop['sentence_id'])\n",
    "rand_sentence_df = known_true_mentions_pop[known_true_mentions_pop['sentence_id'] == rand_sid]\n",
    "print(f\"Sentence ID: {rand_sid}, Number of Mentions: {len(rand_sentence_df)}\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_pop_nopr, dataframe=rand_sentence_df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congruence Accuracy considering Prior Confidence\n",
    "\n",
    "Now, we directly incorporate prior confidence by combining it during the prediction process with our calculated congruent metric. This involves \"discounting\" congruence by the prior confidence we had in each combination's component candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4438/4438 [00:33<00:00, 131.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "congruent_predictions_pop_pr = {}\n",
    "for sid in tqdm(full_mentions_pop['sentence_id'].unique()):\n",
    "    congruent_predictions_pop_pr[sid] = get_congruent_predictions(sid, dataframe=full_mentions_pop,\n",
    "                                                                   with_priors=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15918/15918 [00:02<00:00, 7046.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predictive Accuracy: 64.851%\n",
      "Answer Present: 85.997%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_pop_pr, dataframe=known_true_mentions_pop, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:00<00:00, 1627.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 1545, Number of Mentions: 49\n",
      "------------------------------------------\n",
      "AMSTERDAM 1545 ||| True: Amsterdam 844.0 ==? Pred: Amsterdam 844 ||| True True  ||| Present?  True\n",
      "Dutch 1545 ||| True: Netherlands 21148.0 ==? Pred: Netherlands national football team 9647657 ||| False False  ||| Present?  True\n",
      "RKC Waalwijk 1545 ||| True: RKC Waalwijk 1107824.0 ==? Pred: RKC Waalwijk 1107824 ||| True True  ||| Present?  True\n",
      "RKC Waalwijk 1545 ||| True: RKC Waalwijk 1107824.0 ==? Pred: RKC Waalwijk 1107824 ||| True True  ||| Present?  True\n",
      "Willem II Tilburg 1545 ||| True: Willem II (football club) 834275.0 ==? Pred: Willem II (football club) 834275 ||| True True  ||| Present?  True\n",
      "Willem II Tilburg 1545 ||| True: Willem II (football club) 834275.0 ==? Pred: Willem II (football club) 834275 ||| True True  ||| Present?  True\n",
      "Willem II Tilburg 1545 ||| True: Willem II (football club) 834275.0 ==? Pred: Willem II (football club) 834275 ||| True True  ||| Present?  True\n",
      "Fortuna Sittard 1545 ||| True: Fortuna Sittard 1336325.0 ==? Pred: Fortuna Sittard 1336325 ||| True True  ||| Present?  True\n",
      "Fortuna Sittard 1545 ||| True: Fortuna Sittard 1336325.0 ==? Pred: Fortuna Sittard 1336325 ||| True True  ||| Present?  True\n",
      "NAC Breda 1545 ||| True: NAC Breda 505173.0 ==? Pred: NAC Breda 505173 ||| True True  ||| Present?  True\n",
      "NAC Breda 1545 ||| True: NAC Breda 505173.0 ==? Pred: NAC Breda 505173 ||| True True  ||| Present?  True\n",
      "Sparta Rotterdam 1545 ||| True: Sparta Rotterdam 79818.0 ==? Pred: Sparta Rotterdam 79818 ||| True True  ||| Present?  True\n",
      "Sparta Rotterdam 1545 ||| True: Sparta Rotterdam 79818.0 ==? Pred: Sparta Rotterdam 79818 ||| True True  ||| Present?  True\n",
      "Heerenveen 1545 ||| True: SC Heerenveen 745757.0 ==? Pred: SC Heerenveen 745757 ||| True True  ||| Present?  True\n",
      "Ajax Amsterdam 1545 ||| True: AFC Ajax 2273.0 ==? Pred: AFC Ajax 2273 ||| True True  ||| Present?  True\n",
      "Ajax Amsterdam 1545 ||| True: AFC Ajax 2273.0 ==? Pred: AFC Ajax 2273 ||| True True  ||| Present?  True\n",
      "Fortuna Sittard 1545 ||| True: Fortuna Sittard 1336325.0 ==? Pred: Fortuna Sittard 1336325 ||| True True  ||| Present?  True\n",
      "Fortuna Sittard 1545 ||| True: Fortuna Sittard 1336325.0 ==? Pred: Fortuna Sittard 1336325 ||| True True  ||| Present?  True\n",
      "PSV Eindhoven 1545 ||| True: PSV Eindhoven 79820.0 ==? Pred: PSV Eindhoven 79820 ||| True True  ||| Present?  True\n",
      "PSV Eindhoven 1545 ||| True: PSV Eindhoven 79820.0 ==? Pred: PSV Eindhoven 79820 ||| True True  ||| Present?  True\n",
      "Twente Enschede 1545 ||| True: FC Twente 800033.0 ==? Pred: FC Twente 800033 ||| True True  ||| Present?  True\n",
      "Twente Enschede 1545 ||| True: FC Twente 800033.0 ==? Pred: FC Twente 800033 ||| True True  ||| Present?  True\n",
      "Vitesse Arnhem 1545 ||| True: Vitesse 834256.0 ==? Pred: SBV Vitesse 834256 ||| False True  ||| Present?  True\n",
      "Vitesse Arnhem 1545 ||| True: Vitesse 834256.0 ==? Pred: SBV Vitesse 834256 ||| False True  ||| Present?  True\n",
      "Heerenveen 1545 ||| True: SC Heerenveen 745757.0 ==? Pred: SC Heerenveen 745757 ||| True True  ||| Present?  True\n",
      "NAC Breda 1545 ||| True: NAC Breda 505173.0 ==? Pred: NAC Breda 505173 ||| True True  ||| Present?  True\n",
      "NAC Breda 1545 ||| True: NAC Breda 505173.0 ==? Pred: NAC Breda 505173 ||| True True  ||| Present?  True\n",
      "Ajax Amsterdam 1545 ||| True: AFC Ajax 2273.0 ==? Pred: AFC Ajax 2273 ||| True True  ||| Present?  True\n",
      "Ajax Amsterdam 1545 ||| True: AFC Ajax 2273.0 ==? Pred: AFC Ajax 2273 ||| True True  ||| Present?  True\n",
      "Utrecht 1545 ||| True: FC Utrecht 834226.0 ==? Pred: FC Utrecht 834226 ||| True True  ||| Present?  True\n",
      "Feyenoord Rotterdam 1545 ||| True: Feyenoord 47707.0 ==? Pred: Feyenoord 47707 ||| True True  ||| Present?  True\n",
      "Feyenoord Rotterdam 1545 ||| True: Feyenoord 47707.0 ==? Pred: Feyenoord 47707 ||| True True  ||| Present?  True\n",
      "Roda JC Kerkrade 1545 ||| True: Roda JC 834196.0 ==? Pred: Roda JC Kerkrade 834196 ||| False True  ||| Present?  True\n",
      "Roda JC Kerkrade 1545 ||| True: Roda JC 834196.0 ==? Pred: Roda JC Kerkrade 834196 ||| False True  ||| Present?  True\n",
      "Roda JC Kerkrade 1545 ||| True: Roda JC 834196.0 ==? Pred: Roda JC Kerkrade 834196 ||| False True  ||| Present?  True\n",
      "Volendam 1545 ||| True: FC Volendam 2202202.0 ==? Pred: FC Volendam 2202202 ||| True True  ||| Present?  True\n",
      "Groningen 1545 ||| True: FC Groningen 1103112.0 ==? Pred: FC Groningen 1103112 ||| True True  ||| Present?  True\n",
      "RKC Waalwijk 1545 ||| True: RKC Waalwijk 1107824.0 ==? Pred: RKC Waalwijk 1107824 ||| True True  ||| Present?  True\n",
      "RKC Waalwijk 1545 ||| True: RKC Waalwijk 1107824.0 ==? Pred: RKC Waalwijk 1107824 ||| True True  ||| Present?  True\n",
      "Sparta Rotterdam 1545 ||| True: Sparta Rotterdam 79818.0 ==? Pred: Sparta Rotterdam 79818 ||| True True  ||| Present?  True\n",
      "Sparta Rotterdam 1545 ||| True: Sparta Rotterdam 79818.0 ==? Pred: Sparta Rotterdam 79818 ||| True True  ||| Present?  True\n",
      "Willem II Tilburg 1545 ||| True: Willem II (football club) 834275.0 ==? Pred: Willem II (football club) 834275 ||| True True  ||| Present?  True\n",
      "Willem II Tilburg 1545 ||| True: Willem II (football club) 834275.0 ==? Pred: Willem II (football club) 834275 ||| True True  ||| Present?  True\n",
      "Willem II Tilburg 1545 ||| True: Willem II (football club) 834275.0 ==? Pred: Willem II (football club) 834275 ||| True True  ||| Present?  True\n",
      "AZ Alkmaar 1545 ||| True: AZ (football club) 2289.0 ==? Pred: AZ Alkmaar 2289 ||| False True  ||| Present?  True\n",
      "AZ Alkmaar 1545 ||| True: AZ (football club) 2289.0 ==? Pred: AZ Alkmaar 2289 ||| False True  ||| Present?  True\n",
      "NEC Nijmegen 1545 ||| True: N.E.C. (football club) 834154.0 ==? Pred: NEC Nijmegen 834154 ||| False True  ||| Present?  True\n",
      "NEC Nijmegen 1545 ||| True: N.E.C. (football club) 834154.0 ==? Pred: NEC Nijmegen 834154 ||| False True  ||| Present?  True\n",
      "GERMAN 1545 ||| True: Germany 11867.0 ==? Pred: Netherlands 21148 ||| False False  ||| Present?  True\n",
      "------------------------------------------\n",
      "Predictive Accuracy: 95.918%\n",
      "Answer Present: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to random sentence for manual review\n",
    "rand_sid = np.random.choice(known_true_mentions_pop['sentence_id'])\n",
    "rand_sentence_df = known_true_mentions_pop[known_true_mentions_pop['sentence_id'] == rand_sid]\n",
    "print(f\"Sentence ID: {rand_sid}, Number of Mentions: {len(rand_sentence_df)}\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_pop_pr, dataframe=rand_sentence_df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we still see a nearly 3% increase in predictive accuracy, it is not greater than the performance increase seen when using the frequency dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical Flow Demonstration\n",
    "\n",
    "The cells below have been included as a more easily understood logical flow to understand how we designed the recursive congruence algorithm for an arbitrary length of full mentions in a sentence. We manually select a sentence and work through that. This is identical to the above but with more printed out breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>YORK</td>\n",
       "      <td>http://en.wikipedia.org/wiki/York</td>\n",
       "      <td>34361.0</td>\n",
       "      <td>York</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>york</td>\n",
       "      <td>[34361, 134305, 974198, 992970, 2436511]</td>\n",
       "      <td>[42462, 821105, 8055519, 21008612, 994000]</td>\n",
       "      <td>[York, York,_Pennsylvania, York_Racecourse, Yo...</td>\n",
       "      <td>[0.5884024, 0.0486891, 0.0466228, 0.0396487, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>England</td>\n",
       "      <td>http://en.wikipedia.org/wiki/England</td>\n",
       "      <td>9316.0</td>\n",
       "      <td>England</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>england</td>\n",
       "      <td>[9316, 9904, 759125, 691024, 990422]</td>\n",
       "      <td>[21, 47762, 1321565, 378628, 3589698]</td>\n",
       "      <td>[England, England_national_football_team, Engl...</td>\n",
       "      <td>[0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mark_Prescott</td>\n",
       "      <td>25674876.0</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>mark prescott</td>\n",
       "      <td>[25674876]</td>\n",
       "      <td>[6769327]</td>\n",
       "      <td>[Mark_Prescott]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Pivotal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>pivotal</td>\n",
       "      <td>[39421721, 39467663, 34382035, 25015249, 2768719]</td>\n",
       "      <td>[11331695, 16335785, 7180990, 5503397, 15148967]</td>\n",
       "      <td>[Pivotal_(horse), Pivotal_Software, Phases_of_...</td>\n",
       "      <td>[0.6304348, 0.2608696, 0.0217391, 0.0217391, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Nunthorpe_Stakes</td>\n",
       "      <td>727606.0</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>nunthorpe stakes</td>\n",
       "      <td>[727606]</td>\n",
       "      <td>[3346442]</td>\n",
       "      <td>[Nunthorpe_Stakes]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                  wikipedia_URL  \\\n",
       "0       B              YORK              http://en.wikipedia.org/wiki/York   \n",
       "1       B           England           http://en.wikipedia.org/wiki/England   \n",
       "2       B     Mark Prescott     http://en.wikipedia.org/wiki/Mark_Prescott   \n",
       "3       B           Pivotal                                            NaN   \n",
       "4       B  Nunthorpe Stakes  http://en.wikipedia.org/wiki/Nunthorpe_Stakes   \n",
       "\n",
       "   wikipedia_page_ID   wikipedia_title  sentence_id  doc_id  \\\n",
       "0            34361.0              York          210      31   \n",
       "1             9316.0           England          210      31   \n",
       "2         25674876.0     Mark Prescott          210      31   \n",
       "3                NaN               NaN          210      31   \n",
       "4           727606.0  Nunthorpe Stakes          210      31   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [YORK, England, Mark Prescott, Pivotal, Nuntho...              york   \n",
       "1  [YORK, England, Mark Prescott, Pivotal, Nuntho...           england   \n",
       "2  [YORK, England, Mark Prescott, Pivotal, Nuntho...     mark prescott   \n",
       "3  [YORK, England, Mark Prescott, Pivotal, Nuntho...           pivotal   \n",
       "4  [YORK, England, Mark Prescott, Pivotal, Nuntho...  nunthorpe stakes   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0           [34361, 134305, 974198, 992970, 2436511]   \n",
       "1               [9316, 9904, 759125, 691024, 990422]   \n",
       "2                                         [25674876]   \n",
       "3  [39421721, 39467663, 34382035, 25015249, 2768719]   \n",
       "4                                           [727606]   \n",
       "\n",
       "                            candidate_pool_item_ids  \\\n",
       "0        [42462, 821105, 8055519, 21008612, 994000]   \n",
       "1             [21, 47762, 1321565, 378628, 3589698]   \n",
       "2                                         [6769327]   \n",
       "3  [11331695, 16335785, 7180990, 5503397, 15148967]   \n",
       "4                                         [3346442]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [York, York,_Pennsylvania, York_Racecourse, Yo...   \n",
       "1  [England, England_national_football_team, Engl...   \n",
       "2                                    [Mark_Prescott]   \n",
       "3  [Pivotal_(horse), Pivotal_Software, Phases_of_...   \n",
       "4                                 [Nunthorpe_Stakes]   \n",
       "\n",
       "                               candidate_pool_priors  \n",
       "0  [0.5884024, 0.0486891, 0.0466228, 0.0396487, 0...  \n",
       "1  [0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...  \n",
       "2                                              [1.0]  \n",
       "3  [0.6304348, 0.2608696, 0.0217391, 0.0217391, 0...  \n",
       "4                                              [1.0]  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on manually selected sentence\n",
    "single_sentence_df = full_mentions[full_mentions['sentence_id'] == sentence_id].drop_duplicates(['full_mention', 'wikipedia_page_ID', 'sentence_id']).reset_index(drop=True)\n",
    "single_sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YORK', 'England', 'Mark Prescott', 'Pivotal', 'Nunthorpe Stakes']\n"
     ]
    }
   ],
   "source": [
    "# Congruent Mention\n",
    "print(single_sentence_df['congruent_mentions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numerical for easier recursive logic later\n",
    "sentence_mention_nums = np.arange(len(single_sentence_df['congruent_mentions'][0]))\n",
    "sentence_mention_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate lists of vectors\n",
    "def get_candidate_pool_vectors(candidate_pool_titles, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return entity vectors from Wikipedia2Vec\n",
    "    Takes as input a list of page titles, representing the candidate pool\n",
    "    Normalizes each page title to match necessary input format\n",
    "    Returns entity vector or empty vector if no match\n",
    "    \"\"\"\n",
    "    # Track failed vector queries\n",
    "    no_vector_count = 0\n",
    "    candidate_pool_vectors = []\n",
    "    for candidate in candidate_pool_titles:\n",
    "        candidate = normalize_text(candidate)\n",
    "        try:\n",
    "            candidate_vectors = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            # Keep empty vector representation to maintain index locations\n",
    "            candidate_vectors = np.zeros(100)\n",
    "            no_vector_count += 1\n",
    "        candidate_pool_vectors.append(candidate_vectors)\n",
    "    \n",
    "    if len(candidate_pool_titles) == 0:\n",
    "        candidate_pool_vectors = [np.zeros(100), np.zeros(100), np.zeros(100)]\n",
    "    \n",
    "    if verbose: print(f\"Failed Wikipedia2Vec Entity Vector Queries: {no_vector_count}\")\n",
    "    return candidate_pool_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n"
     ]
    }
   ],
   "source": [
    "# Save vectors in dictionary\n",
    "vector_dict = {}\n",
    "\n",
    "# For each full mention we are analyzing in the contextual domain (i.e. sentence)\n",
    "for m in sentence_mention_nums:\n",
    "    \n",
    "    # Retrieve candidate pool titles\n",
    "    candidate_pool_titles = single_sentence_df['candidate_pool_titles'][m]\n",
    "    \n",
    "    # Convert candidate pool titles to candidate pool vectors\n",
    "    candidate_pool_vectors = get_candidate_pool_vectors(candidate_pool_titles, verbose=True)\n",
    "    \n",
    "    # Save candidate pool vectors to dictionary\n",
    "    vector_dict[m] = candidate_pool_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[memmap([ 0.2683868 , -0.05262262,  0.54006284,  0.05049284,  0.3950385 ,\n",
       "          0.18029454,  0.9676563 ,  0.5245846 , -0.8257101 , -0.06190873,\n",
       "          0.0048545 , -0.3587773 ,  1.0333955 , -0.8392176 , -0.5148771 ,\n",
       "          0.7619262 ,  1.1545883 , -0.28774223,  0.37944788,  0.36179924,\n",
       "         -0.18364815,  0.25456274, -0.42908797,  0.4699114 , -1.1465906 ,\n",
       "         -1.344148  ,  0.46428162,  0.01225942,  1.0560988 ,  0.391352  ,\n",
       "          0.01914932, -0.5230255 ,  0.01505912,  0.2288472 ,  0.95887125,\n",
       "         -0.3009153 , -0.74820864,  0.2446501 , -0.71423006, -0.058732  ,\n",
       "         -0.5778156 , -1.1819346 , -0.51720476,  0.56831   ,  0.39721656,\n",
       "         -0.5982357 ,  0.21792424,  0.3120324 , -1.3766816 , -0.25159237,\n",
       "          0.14914982,  0.09284619, -0.42491975, -0.40097916,  1.121219  ,\n",
       "         -0.21171518,  0.17501397,  0.23550397, -1.2049819 ,  0.09586409,\n",
       "         -0.5030386 , -0.82044554, -0.01860892, -0.44457683,  1.9537257 ,\n",
       "         -0.1218124 , -0.16592291, -0.61572707, -0.83974636, -0.78331035,\n",
       "         -0.45257503,  1.068549  ,  1.520831  ,  1.0346282 , -1.4640044 ,\n",
       "          0.7263678 ,  0.8289772 ,  0.35587314,  0.36035538, -0.09692112,\n",
       "          1.5906011 , -0.07746844,  0.06886535, -0.4741191 , -0.48017558,\n",
       "         -0.69907963, -0.12020469, -0.34679833,  0.09163934, -0.07474833,\n",
       "         -0.00315162,  0.49266675, -0.32719526, -0.33517686, -0.7439421 ,\n",
       "         -1.1595058 ,  0.63044286,  0.04471726, -0.07303976,  0.17590128],\n",
       "        dtype=float32)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display vector_dict output\n",
    "print(vector_dict.keys())\n",
    "# Preview one candidate vector from a candidate pool vectors\n",
    "vector_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "2 3\n",
      "2 4\n",
      "3 4\n"
     ]
    }
   ],
   "source": [
    "# Calculate congruence metric for each candidate vector for each mention's candidate pool\n",
    "# This notebook uses cosine similarity as the congruence metric\n",
    "\n",
    "## Save congruence measurements in a two-level dictionary\n",
    "# Create first-level dictionary\n",
    "congruence_dict = {}\n",
    "\n",
    "# Always work low numbers to high\n",
    "m = 0\n",
    "while m < len(sentence_mention_nums)-1:\n",
    "    \n",
    "    # Save second-level congruence measurement dictionary\n",
    "    m_dict = {}\n",
    "    # Compare each mention against mentions after it\n",
    "    for n in sentence_mention_nums[m+1:]:\n",
    "        print(m, n)\n",
    "        # Calculate congruence measurement - cosine similarity\n",
    "        congruence_measurement = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "        # Save congruence measurement to second-level dictionary\n",
    "        m_dict[n] = congruence_measurement\n",
    "    \n",
    "    # Save second-level dictionary to first-level\n",
    "    congruence_dict[m] = m_dict\n",
    "    \n",
    "    # Increment mention\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Display congruence_dict output\n",
    "# This should be one less than congruent mention count, since we are comparing low to high\n",
    "# and thus don't compare the highest value to anything\n",
    "print(congruence_dict.keys())\n",
    "# Preview congruence matrix derived from comparing Mention 1 to Mention 2\n",
    "for k in congruence_dict.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>YORK</td>\n",
       "      <td>http://en.wikipedia.org/wiki/York</td>\n",
       "      <td>34361.0</td>\n",
       "      <td>York</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>york</td>\n",
       "      <td>[34361, 134305, 974198, 992970, 2436511]</td>\n",
       "      <td>[42462, 821105, 8055519, 21008612, 994000]</td>\n",
       "      <td>[York, York,_Pennsylvania, York_Racecourse, Yo...</td>\n",
       "      <td>[0.5884024, 0.0486891, 0.0466228, 0.0396487, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>England</td>\n",
       "      <td>http://en.wikipedia.org/wiki/England</td>\n",
       "      <td>9316.0</td>\n",
       "      <td>England</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>england</td>\n",
       "      <td>[9316, 9904, 759125, 691024, 990422]</td>\n",
       "      <td>[21, 47762, 1321565, 378628, 3589698]</td>\n",
       "      <td>[England, England_national_football_team, Engl...</td>\n",
       "      <td>[0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mark_Prescott</td>\n",
       "      <td>25674876.0</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>mark prescott</td>\n",
       "      <td>[25674876]</td>\n",
       "      <td>[6769327]</td>\n",
       "      <td>[Mark_Prescott]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Pivotal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>pivotal</td>\n",
       "      <td>[39421721, 39467663, 34382035, 25015249, 2768719]</td>\n",
       "      <td>[11331695, 16335785, 7180990, 5503397, 15148967]</td>\n",
       "      <td>[Pivotal_(horse), Pivotal_Software, Phases_of_...</td>\n",
       "      <td>[0.6304348, 0.2608696, 0.0217391, 0.0217391, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Nunthorpe_Stakes</td>\n",
       "      <td>727606.0</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>nunthorpe stakes</td>\n",
       "      <td>[727606]</td>\n",
       "      <td>[3346442]</td>\n",
       "      <td>[Nunthorpe_Stakes]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                  wikipedia_URL  \\\n",
       "0       B              YORK              http://en.wikipedia.org/wiki/York   \n",
       "1       B           England           http://en.wikipedia.org/wiki/England   \n",
       "2       B     Mark Prescott     http://en.wikipedia.org/wiki/Mark_Prescott   \n",
       "3       B           Pivotal                                            NaN   \n",
       "4       B  Nunthorpe Stakes  http://en.wikipedia.org/wiki/Nunthorpe_Stakes   \n",
       "\n",
       "   wikipedia_page_ID   wikipedia_title  sentence_id  doc_id  \\\n",
       "0            34361.0              York          210      31   \n",
       "1             9316.0           England          210      31   \n",
       "2         25674876.0     Mark Prescott          210      31   \n",
       "3                NaN               NaN          210      31   \n",
       "4           727606.0  Nunthorpe Stakes          210      31   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [YORK, England, Mark Prescott, Pivotal, Nuntho...              york   \n",
       "1  [YORK, England, Mark Prescott, Pivotal, Nuntho...           england   \n",
       "2  [YORK, England, Mark Prescott, Pivotal, Nuntho...     mark prescott   \n",
       "3  [YORK, England, Mark Prescott, Pivotal, Nuntho...           pivotal   \n",
       "4  [YORK, England, Mark Prescott, Pivotal, Nuntho...  nunthorpe stakes   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0           [34361, 134305, 974198, 992970, 2436511]   \n",
       "1               [9316, 9904, 759125, 691024, 990422]   \n",
       "2                                         [25674876]   \n",
       "3  [39421721, 39467663, 34382035, 25015249, 2768719]   \n",
       "4                                           [727606]   \n",
       "\n",
       "                            candidate_pool_item_ids  \\\n",
       "0        [42462, 821105, 8055519, 21008612, 994000]   \n",
       "1             [21, 47762, 1321565, 378628, 3589698]   \n",
       "2                                         [6769327]   \n",
       "3  [11331695, 16335785, 7180990, 5503397, 15148967]   \n",
       "4                                         [3346442]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [York, York,_Pennsylvania, York_Racecourse, Yo...   \n",
       "1  [England, England_national_football_team, Engl...   \n",
       "2                                    [Mark_Prescott]   \n",
       "3  [Pivotal_(horse), Pivotal_Software, Phases_of_...   \n",
       "4                                 [Nunthorpe_Stakes]   \n",
       "\n",
       "                               candidate_pool_priors  \n",
       "0  [0.5884024, 0.0486891, 0.0466228, 0.0396487, 0...  \n",
       "1  [0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...  \n",
       "2                                              [1.0]  \n",
       "3  [0.6304348, 0.2608696, 0.0217391, 0.0217391, 0...  \n",
       "4                                              [1.0]  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3\n"
     ]
    }
   ],
   "source": [
    "# Pick mentions to compare\n",
    "men1 = np.random.choice(sentence_mention_nums[:-1])\n",
    "men2 = np.random.choice(sentence_mention_nums)\n",
    "while men1 == men2:\n",
    "    men2 = np.random.choice(sentence_mention_nums)\n",
    "men1, men2 = np.sort([men1, men2])\n",
    "print(men1, men2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_4faa0398_3e76_11eb_865c_acde48001122row4_col3{\n",
       "            background:  skyblue;\n",
       "        }</style><table id=\"T_4faa0398_3e76_11eb_865c_acde48001122\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_4faa0398_3e76_11eb_865c_acde48001122level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row0_col0\" class=\"data row0 col0\" >0.293464</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row0_col1\" class=\"data row0 col1\" >0.090070</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row0_col2\" class=\"data row0 col2\" >0.273404</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row0_col3\" class=\"data row0 col3\" >0.245676</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4faa0398_3e76_11eb_865c_acde48001122level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row1_col0\" class=\"data row1 col0\" >0.233738</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row1_col1\" class=\"data row1 col1\" >0.194647</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row1_col2\" class=\"data row1 col2\" >0.268510</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row1_col3\" class=\"data row1 col3\" >0.200175</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4faa0398_3e76_11eb_865c_acde48001122level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row2_col0\" class=\"data row2 col0\" >0.350722</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row2_col1\" class=\"data row2 col1\" >0.254812</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row2_col2\" class=\"data row2 col2\" >0.195908</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row2_col3\" class=\"data row2 col3\" >0.202840</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4faa0398_3e76_11eb_865c_acde48001122level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row3_col0\" class=\"data row3 col0\" >0.338334</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row3_col1\" class=\"data row3 col1\" >0.228758</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row3_col2\" class=\"data row3 col2\" >0.186814</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row3_col3\" class=\"data row3 col3\" >0.304612</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row3_col4\" class=\"data row3 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4faa0398_3e76_11eb_865c_acde48001122level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row4_col0\" class=\"data row4 col0\" >0.340859</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row4_col1\" class=\"data row4 col1\" >0.176496</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row4_col2\" class=\"data row4 col2\" >0.182557</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row4_col3\" class=\"data row4 col3\" >0.361004</td>\n",
       "                        <td id=\"T_4faa0398_3e76_11eb_865c_acde48001122row4_col4\" class=\"data row4 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe1f9233390>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot congruence calculation between candidates\n",
    "congruence_test_df = pd.DataFrame(congruence_dict[men1][men2])\n",
    "max_num = max(np.max(congruence_test_df))\n",
    "congruence_test_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_4fad4d50_3e76_11eb_865c_acde48001122row0_col0{\n",
       "            background:  skyblue;\n",
       "        }</style><table id=\"T_4fad4d50_3e76_11eb_865c_acde48001122\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_4fad4d50_3e76_11eb_865c_acde48001122level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row0_col0\" class=\"data row0 col0\" >0.688296</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row0_col1\" class=\"data row0 col1\" >0.503514</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row0_col2\" class=\"data row0 col2\" >0.383949</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row0_col3\" class=\"data row0 col3\" >0.383949</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row0_col4\" class=\"data row0 col4\" >0.383949</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4fad4d50_3e76_11eb_865c_acde48001122level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row1_col0\" class=\"data row1 col0\" >0.352058</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row1_col1\" class=\"data row1 col1\" >0.167275</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row1_col2\" class=\"data row1 col2\" >0.047710</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row1_col3\" class=\"data row1 col3\" >0.047710</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row1_col4\" class=\"data row1 col4\" >0.047710</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4fad4d50_3e76_11eb_865c_acde48001122level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row2_col0\" class=\"data row2 col0\" >0.336003</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row2_col1\" class=\"data row2 col1\" >0.151220</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row2_col2\" class=\"data row2 col2\" >0.031655</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row2_col3\" class=\"data row2 col3\" >0.031655</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row2_col4\" class=\"data row2 col4\" >0.031655</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4fad4d50_3e76_11eb_865c_acde48001122level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row3_col0\" class=\"data row3 col0\" >0.331648</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row3_col1\" class=\"data row3 col1\" >0.146865</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row3_col2\" class=\"data row3 col2\" >0.027300</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row3_col3\" class=\"data row3 col3\" >0.027300</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row3_col4\" class=\"data row3 col4\" >0.027300</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4fad4d50_3e76_11eb_865c_acde48001122level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row4_col0\" class=\"data row4 col0\" >0.326475</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row4_col1\" class=\"data row4 col1\" >0.141693</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row4_col2\" class=\"data row4 col2\" >0.022127</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row4_col3\" class=\"data row4 col3\" >0.022127</td>\n",
       "                        <td id=\"T_4fad4d50_3e76_11eb_865c_acde48001122row4_col4\" class=\"data row4 col4\" >0.022127</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe1f8d53710>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean prior confidence for same candidates\n",
    "matrix_test = []\n",
    "for a in single_sentence_df['candidate_pool_priors'][men1]:\n",
    "    row_test = []\n",
    "    for b in single_sentence_df['candidate_pool_priors'][men2]:\n",
    "        row_test.append(np.mean([a, b]))\n",
    "    matrix_test.append(row_test)\n",
    "matrix_df = pd.DataFrame(matrix_test)\n",
    "max_num = max(np.max(matrix_df))\n",
    "matrix_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_4faf9a92_3e76_11eb_865c_acde48001122row0_col0{\n",
       "            background:  skyblue;\n",
       "        }</style><table id=\"T_4faf9a92_3e76_11eb_865c_acde48001122\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_4faf9a92_3e76_11eb_865c_acde48001122level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row0_col0\" class=\"data row0 col0\" >0.201990</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row0_col1\" class=\"data row0 col1\" >0.045352</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row0_col2\" class=\"data row0 col2\" >0.104973</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row0_col3\" class=\"data row0 col3\" >0.094327</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4faf9a92_3e76_11eb_865c_acde48001122level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row1_col0\" class=\"data row1 col0\" >0.082289</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row1_col1\" class=\"data row1 col1\" >0.032560</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row1_col2\" class=\"data row1 col2\" >0.012811</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row1_col3\" class=\"data row1 col3\" >0.009550</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4faf9a92_3e76_11eb_865c_acde48001122level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row2_col0\" class=\"data row2 col0\" >0.117844</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row2_col1\" class=\"data row2 col1\" >0.038533</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row2_col2\" class=\"data row2 col2\" >0.006201</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row2_col3\" class=\"data row2 col3\" >0.006421</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4faf9a92_3e76_11eb_865c_acde48001122level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row3_col0\" class=\"data row3 col0\" >0.112208</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row3_col1\" class=\"data row3 col1\" >0.033597</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row3_col2\" class=\"data row3 col2\" >0.005100</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row3_col3\" class=\"data row3 col3\" >0.008316</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row3_col4\" class=\"data row3 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4faf9a92_3e76_11eb_865c_acde48001122level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row4_col0\" class=\"data row4 col0\" >0.111282</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row4_col1\" class=\"data row4 col1\" >0.025008</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row4_col2\" class=\"data row4 col2\" >0.004040</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row4_col3\" class=\"data row4 col3\" >0.007988</td>\n",
       "                        <td id=\"T_4faf9a92_3e76_11eb_865c_acde48001122row4_col4\" class=\"data row4 col4\" >0.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe1f93d87d0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discount congruence metric with prior candidate confidence\n",
    "weighted_df = np.array(matrix_df) * np.array(congruence_test_df)\n",
    "weighted_df = pd.DataFrame(weighted_df)\n",
    "max_num = max(np.max(weighted_df))\n",
    "weighted_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  5\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "Length:  5\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "Length:  5\n",
      "2 3\n",
      "2 4\n",
      "Length:  5\n",
      "3 4\n",
      "Length:  5\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of logic to ensure unique mention congruence only calculated once\n",
    "m = 0\n",
    "while m < len(sentence_mention_nums):\n",
    "    print(\"Length: \", len(sentence_mention_nums))\n",
    "    for i in sentence_mention_nums[m+1:]:\n",
    "        print(m, i)\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 4), 0.7768851)\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((2, 0), 0.6718753)\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((2, 0), 0.6834503754416745)\n",
      "Comparing 0 & 4\n",
      "Congruent Pair:  ((2, 0), 0.7671312)\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((4, 0), 0.42040652)\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((4, 3), 0.36100444986173746)\n",
      "Comparing 1 & 4\n",
      "Congruent Pair:  ((4, 0), 0.4167)\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.7108868350464272)\n",
      "Comparing 2 & 4\n",
      "Congruent Pair:  ((0, 0), 0.6659345)\n",
      "Comparing 3 & 4\n",
      "Congruent Pair:  ((0, 0), 0.8045559194737764)\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((4, 0), 0.42040652), 1, 2)\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "most_congruent_pair = (None, 0.0, 0, 0) # Most Congruent Candidates, Congruence Metric, Mention A, Mention B\n",
    "\n",
    "for m in sentence_mention_nums:\n",
    "    for n in sentence_mention_nums[m+1:]:\n",
    "        print(f\"Comparing {m} & {n}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m][n])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, m, n\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 4, 2: 0}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save max congruent pair estimates for each mention\n",
    "mention_predictions = {}\n",
    "mention_predictions[most_congruent_pair[1]] = most_congruent_pair[0][0][0]\n",
    "mention_predictions[most_congruent_pair[2]] = most_congruent_pair[0][0][1]\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have predictions for  dict_keys([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# With two mentions set in their predictions, we must filter the other congruent matrices to find the next most\n",
    "print(\"We have predictions for \", mention_predictions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 4]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(sentence_mention_nums) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 4), 0.7768851)\n",
      "MAX: (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((4, 3), 0.36100444986173746)\n",
      "MAX: (((3, 4), 0.7768851), (0, 1))\n",
      "Comparing 1 & 4\n",
      "Congruent Pair:  ((4, 0), 0.4167)\n",
      "MAX: (((3, 4), 0.7768851), (0, 1))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((2, 0), 0.6718753)\n",
      "MAX: (((3, 4), 0.7768851), (0, 1))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.7108868350464272)\n",
      "MAX: (((3, 4), 0.7768851), (0, 1))\n",
      "Comparing 2 & 4\n",
      "Congruent Pair:  ((0, 0), 0.6659345)\n",
      "MAX: (((3, 4), 0.7768851), (0, 1))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((3, 4), 0.7768851), (0, 1))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 4, 2: 0, 0: 3}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((4, 3), 0.36100444986173746)\n",
      "MAX: (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 4\n",
      "Congruent Pair:  ((4, 0), 0.4167)\n",
      "MAX: (((4, 3), 0.36100444986173746), (1, 3))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.7108868350464272)\n",
      "MAX: (((4, 0), 0.4167), (1, 4))\n",
      "Comparing 2 & 4\n",
      "Congruent Pair:  ((0, 0), 0.6659345)\n",
      "MAX: (((0, 0), 0.7108868350464272), (2, 3))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((2, 0), 0.6834503754416745)\n",
      "MAX: (((0, 0), 0.7108868350464272), (2, 3))\n",
      "Comparing 0 & 4\n",
      "Congruent Pair:  ((2, 0), 0.7671312)\n",
      "MAX: (((0, 0), 0.7108868350464272), (2, 3))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((2, 0), 0.7671312), (0, 4))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 4, 2: 0, 0: 3, 4: 0}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((4, 3), 0.36100444986173746)\n",
      "MAX: (((None, None), 0.0), (0, 0))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.7108868350464272)\n",
      "MAX: (((4, 3), 0.36100444986173746), (1, 3))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((2, 0), 0.6834503754416745)\n",
      "MAX: (((0, 0), 0.7108868350464272), (2, 3))\n",
      "Comparing 3 & 4\n",
      "Congruent Pair:  ((0, 0), 0.8045559194737764)\n",
      "MAX: (((0, 0), 0.7108868350464272), (2, 3))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((0, 0), 0.8045559194737764), (3, 4))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 4, 2: 0, 0: 3, 4: 0, 3: 0}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've predicted everything!\n"
     ]
    }
   ],
   "source": [
    "if len(mentions_remaining) == 0:\n",
    "    print(\"You've predicted everything!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
