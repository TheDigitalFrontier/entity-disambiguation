{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congruence via Entity Vector Similarity\n",
    "\n",
    "In this notebook, we calculate the cosine similarity measure between vectors of each entity candidate. The vectors will be retrieved from Wikipedia2vec's pre-trained API, which creates vectors for the entire Wikipedia page. Comparing two vectors in this way thus lets us make a statement about similar pages and update our likelihood scores based on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>mention_candidate_pools_page_ids</th>\n",
       "      <th>mention_candidate_pools_item_ids</th>\n",
       "      <th>candidate_pools_titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 21347120, 9477, 1882861, 3261189,...</td>\n",
       "      <td>[458, 46, 211593, 1396, 363404, 3327447, 40537...</td>\n",
       "      <td>['European_Union', 'Europe', 'Eu,_Seine-Mariti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 11884, 152735, 21212, 12674, 290327, 1...</td>\n",
       "      <td>[183, 188, 42884, 7318, 43287, 141817, 181287,...</td>\n",
       "      <td>['Germany', 'German_language', 'Germans', 'Naz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>british</td>\n",
       "      <td>[31717, 19097669, 13530298, 4721, 158019, 1522...</td>\n",
       "      <td>[145, 842438, 23666, 8680, 161885, 174193, 354...</td>\n",
       "      <td>['United_Kingdom', 'British_people', 'Great_Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention     full_mention                                wikipedia_URL  \\\n",
       "0       B               EU                                          NaN   \n",
       "1       B           German         http://en.wikipedia.org/wiki/Germany   \n",
       "2       B          British  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "3       B  Peter Blackburn                                          NaN   \n",
       "4       I  Peter Blackburn                                          NaN   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0                NaN             NaN            0       0   \n",
       "1            11867.0         Germany            0       0   \n",
       "2            31717.0  United Kingdom            0       0   \n",
       "3                NaN             NaN            1       0   \n",
       "4                NaN             NaN            1       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0                        ['EU', 'German', 'British']                eu   \n",
       "1                        ['EU', 'German', 'British']            german   \n",
       "2                        ['EU', 'German', 'British']           british   \n",
       "3  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "4  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "\n",
       "                    mention_candidate_pools_page_ids  \\\n",
       "0  [9317, 9239, 21347120, 9477, 1882861, 3261189,...   \n",
       "1  [11867, 11884, 152735, 21212, 12674, 290327, 1...   \n",
       "2  [31717, 19097669, 13530298, 4721, 158019, 1522...   \n",
       "3                      [56783206, 9643132, 56873217]   \n",
       "4                      [56783206, 9643132, 56873217]   \n",
       "\n",
       "                    mention_candidate_pools_item_ids  \\\n",
       "0  [458, 46, 211593, 1396, 363404, 3327447, 40537...   \n",
       "1  [183, 188, 42884, 7318, 43287, 141817, 181287,...   \n",
       "2  [145, 842438, 23666, 8680, 161885, 174193, 354...   \n",
       "3                       [2073954, 7172840, 26634508]   \n",
       "4                       [2073954, 7172840, 26634508]   \n",
       "\n",
       "                              candidate_pools_titles  \n",
       "0  ['European_Union', 'Europe', 'Eu,_Seine-Mariti...  \n",
       "1  ['Germany', 'German_language', 'Germans', 'Naz...  \n",
       "2  ['United_Kingdom', 'British_people', 'Great_Br...  \n",
       "3  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...  \n",
       "4  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base path to input\n",
    "preds_path = '../../predictions/'\n",
    "\n",
    "# Load data\n",
    "predictions = pd.read_csv(os.path.join(preds_path, \"anchortext_frequency.csv\"), delimiter=\",\")\n",
    "predictions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Saved Candidate Pool\n",
    "\n",
    "Candidate pools when exported are typically stored as the string of a list. The below function parses the string back into a list with proper formatted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate that list is string\n",
    "type(predictions['mention_candidate_pools_page_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse list as string\n",
    "def parse_list_string(list_string, value_type=int):\n",
    "    \n",
    "    parsed_list = []\n",
    "    \n",
    "    # If candidate pool is empty\n",
    "    if list_string == \"[]\" or isinstance(list_string, float):\n",
    "        pass\n",
    "    # Else parse\n",
    "    else:\n",
    "        # Parses lists of titles as strings\n",
    "        if value_type==str:\n",
    "            # Eliminate bracket and parenthesis on either side, split by comma pattern\n",
    "            parsed_list = re.split(\"', '|\\\", \\\"|', \\\"|\\\", \\'\", list_string[2:-2])\n",
    "\n",
    "        # Parses lists of IDs as ints\n",
    "        elif value_type==int:\n",
    "            # Eliminate brackets and convert each number from string to int\n",
    "            parsed_list = list(map(int, list_string[1:-1].split(', ')))\n",
    "            \n",
    "        \n",
    "    return parsed_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['European_Union',\n",
       " 'Europe',\n",
       " 'Eu,_Seine-Maritime',\n",
       " 'Europium',\n",
       " 'Citizenship_of_the_European_Union',\n",
       " 'United_Left_(Galicia)',\n",
       " 'EU_(group)',\n",
       " 'European_Union_law',\n",
       " 'Eu_station',\n",
       " 'Entropy']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "# 0 is the hard one. See how some value is stored with '' and some with \"\". Unsure why.\n",
    "parse_list_string(predictions['candidate_pools_titles'][0], value_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9317,\n",
       " 9239,\n",
       " 21347120,\n",
       " 9477,\n",
       " 1882861,\n",
       " 3261189,\n",
       " 14024977,\n",
       " 276436,\n",
       " 27532324,\n",
       " 9891]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "# 0 is the hard one. See how some value is stored with '' and some with \"\". Unsure why.\n",
    "parse_list_string(predictions['mention_candidate_pools_page_ids'][0], value_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "parse_list_string(predictions['mention_candidate_pools_page_ids'][13], value_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "After ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "Before [56783206, 9643132, 56873217]\n",
      "After [56783206, 9643132, 56873217]\n",
      "Before [2073954, 7172840, 26634508]\n",
      "After [2073954, 7172840, 26634508]\n",
      "Before ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(bishop)', 'Peter_Blackburn_(MP)']\n",
      "After ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(bishop)', 'Peter_Blackburn_(MP)']\n",
      "CPU times: user 590 ms, sys: 200 ms, total: 790 ms\n",
      "Wall time: 812 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Apply defined function to entire dataframe for all candidate pool columns\n",
    "\n",
    "column = 'congruent_mentions'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=str)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "column = 'mention_candidate_pools_page_ids'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=int)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "\n",
    "column = 'mention_candidate_pools_item_ids'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=int)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pools_titles'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=str)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Entity Vectors from Wikipedia2Vec\n",
    "\n",
    "For provided wikipedia pages, we retrieve a representative entity vector from Wikipedia2vec. This involves passing the normalized title into their get_entity_vector() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package\n",
    "from wikipedia2vec import Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96.9 ms, sys: 143 ms, total: 240 ms\n",
      "Wall time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load unzipped pkl file with word embeddings\n",
    "w2v = Wikipedia2Vec.load(\"../../embeddings/enwiki_20180420_100d.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Coverage of Candidate Pools in Wikipedia2vec\n",
    "\n",
    "We need to measure what percent of candidates in our candidate pools successfully return a vector from Wikipedia2vec. This should conceivably be 100% given we're passing known Wikipedia pages into this package trained over Wikipedia pages, but there may be some drop-off due to different creation dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text normalization function\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    We define normalized as:\n",
    "    - strip whitespace\n",
    "    - Spaces, not underlines\n",
    "    \"\"\"\n",
    "    return str(text).strip().replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29312/29312 [00:02<00:00, 9909.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia2vec returned an entity vector for 93.614% of 155,553 searches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over candidate pool titles to see what can be returned\n",
    "\n",
    "found_entity = 0\n",
    "searched_entity = 0\n",
    "\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    \n",
    "    # Retrieve candidate pool\n",
    "    candidate_pool = predictions['candidate_pools_titles'][i]\n",
    "    \n",
    "    # Query for each candidate\n",
    "    for candidate in candidate_pool:\n",
    "        # Normalize candidate title to form necessary to input into Wikipedia2vec\n",
    "        candidate = normalize_text(candidate)\n",
    "        \n",
    "        # Query Wikipedia2vec get_entity_vector()\n",
    "        try:\n",
    "            entity_vector = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            entity_vector = None\n",
    "        \n",
    "        # Check if result\n",
    "        if entity_vector is not None:\n",
    "            found_entity += 1\n",
    "        \n",
    "        # Increment count\n",
    "        searched_entity += 1\n",
    "\n",
    "print(f\"Wikipedia2vec returned an entity vector for {round(found_entity/searched_entity*100,3)}% of {searched_entity:,} searches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Congruence Metric between Congruent Entities\n",
    "\n",
    "First, let's get a sense for what the upper bound of congruent calculations might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the maximum number of congruent entities in a single sentence\n",
    "max(predictions['congruent_mentions'].apply(parse_list_string, value_type=str).apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What is the distribution of congruent mention counts\n",
    "plt.figure(figsize=(20,5))\n",
    "predictions['congruent_mentions'].apply(parse_list_string, value_type=str).apply(len).hist(bins=50)\n",
    "plt.title(\"distribution of congruent mention counts\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Current Design Thinking\n",
    "\n",
    "In order to allow this to become recursive for N many tables, you will need to capture a congruence table for every candidate pool to every other candidate pool in a two-level dictionary so you can retrieve values using `matrix[3][1]`. This would entail duplication except to save that, we sort by value so you always search [small][large]. Saves us computation and storage. To see a sequential example of our logic, scroll to the end of this notebook. Below is our function-based implementation of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>mention_candidate_pools_page_ids</th>\n",
       "      <th>mention_candidate_pools_item_ids</th>\n",
       "      <th>candidate_pools_titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Iran</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iran</td>\n",
       "      <td>14653.0</td>\n",
       "      <td>Iran</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iran</td>\n",
       "      <td>[14653, 272865, 338883, 8294810, 46823116, 126...</td>\n",
       "      <td>[794, 184602, 207991, 1465546, 932162, 1042614...</td>\n",
       "      <td>[Iran, Iran_national_football_team, Pahlavi_dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>turkey</td>\n",
       "      <td>[11125639, 743577, 72821, 24513964, 297071, 22...</td>\n",
       "      <td>[43, 483856, 43794, 4200953, 26844, 12560, 848...</td>\n",
       "      <td>[Turkey, Turkey_national_football_team, Turkey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iraq</td>\n",
       "      <td>7515928.0</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iraq</td>\n",
       "      <td>[7515928, 1039652, 5043324, 26215470, 2900620,...</td>\n",
       "      <td>[796, 186243, 545449, 3108185, 149805, 107802,...</td>\n",
       "      <td>[Iraq, Iraq_national_football_team, Iraq_War, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Kurdish</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kurdish_people</td>\n",
       "      <td>17068.0</td>\n",
       "      <td>Kurdish people</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>[17068, 40316, 80777, 3821855, 4314285, 354232...</td>\n",
       "      <td>[12223, 36368, 41470, 1792998, 1117020, 121801...</td>\n",
       "      <td>[Kurds, Kurdish_languages, Kurdistan, Kurds_in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention full_mention                                wikipedia_URL  \\\n",
       "0       B         Iran            http://en.wikipedia.org/wiki/Iran   \n",
       "1       B       Turkey                                          NaN   \n",
       "2       B         Iraq            http://en.wikipedia.org/wiki/Iraq   \n",
       "3       B      Kurdish  http://en.wikipedia.org/wiki/Kurdish_people   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            14653.0            Iran           90      11   \n",
       "1                NaN             NaN           90      11   \n",
       "2          7515928.0            Iraq           90      11   \n",
       "3            17068.0  Kurdish people           90      11   \n",
       "\n",
       "              congruent_mentions norm_full_mention  \\\n",
       "0  [Iran, Turkey, Iraq, Kurdish]              iran   \n",
       "1  [Iran, Turkey, Iraq, Kurdish]            turkey   \n",
       "2  [Iran, Turkey, Iraq, Kurdish]              iraq   \n",
       "3  [Iran, Turkey, Iraq, Kurdish]           kurdish   \n",
       "\n",
       "                    mention_candidate_pools_page_ids  \\\n",
       "0  [14653, 272865, 338883, 8294810, 46823116, 126...   \n",
       "1  [11125639, 743577, 72821, 24513964, 297071, 22...   \n",
       "2  [7515928, 1039652, 5043324, 26215470, 2900620,...   \n",
       "3  [17068, 40316, 80777, 3821855, 4314285, 354232...   \n",
       "\n",
       "                    mention_candidate_pools_item_ids  \\\n",
       "0  [794, 184602, 207991, 1465546, 932162, 1042614...   \n",
       "1  [43, 483856, 43794, 4200953, 26844, 12560, 848...   \n",
       "2  [796, 186243, 545449, 3108185, 149805, 107802,...   \n",
       "3  [12223, 36368, 41470, 1792998, 1117020, 121801...   \n",
       "\n",
       "                              candidate_pools_titles  \n",
       "0  [Iran, Iran_national_football_team, Pahlavi_dy...  \n",
       "1  [Turkey, Turkey_national_football_team, Turkey...  \n",
       "2  [Iraq, Iraq_national_football_team, Iraq_War, ...  \n",
       "3  [Kurds, Kurdish_languages, Kurdistan, Kurds_in...  "
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define testing sentence_id\n",
    "sentence_id = 90\n",
    "sentence_predictions = predictions[predictions['sentence_id'] == sentence_id].reset_index(drop=True)\n",
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iran', 'Turkey', 'Iraq', 'Kurdish']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_predictions['congruent_mentions'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to Create Modular Congruent Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate lists of vectors\n",
    "def get_candidate_pool_vectors(candidate_pool_titles, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return entity vectors from Wikipedia2Vec\n",
    "    Takes as input a list of page titles, representing the candidate pool\n",
    "    Normalizes each page title to match necessary input format\n",
    "    Returns entity vector or empty vector if no match\n",
    "    \"\"\"\n",
    "    # Track failed vector queries\n",
    "    no_vector_count = 0\n",
    "    candidate_pool_vectors = []\n",
    "    for candidate in candidate_pool_titles:\n",
    "        candidate = normalize_text(candidate)\n",
    "        try:\n",
    "            candidate_vectors = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            # Keep empty vector representation to maintain index locations\n",
    "            candidate_vectors = np.zeros(100)\n",
    "            no_vector_count += 1\n",
    "        candidate_pool_vectors.append(candidate_vectors)\n",
    "    \n",
    "    if verbose: print(f\"Failed Wikipedia2Vec Entity Vector Queries: {no_vector_count}\")\n",
    "    return candidate_pool_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to retrieve entity vectors\n",
    "def create_entity_vector_dict(sentence_mentions_nums, sentence_predictions, verbose=False):\n",
    "    \"\"\"\n",
    "    Function iterates over a provided list of congruent mentions,\n",
    "    finds the associated candidate pool for each mention\n",
    "    and returns the candidate pool vector\n",
    "    \"\"\"\n",
    "    # Save vectors in dictionary\n",
    "    vector_dict = {}\n",
    "    \n",
    "    # For each full mention we are analyzing in the contextual domain\n",
    "    for m in sentence_mentions_nums:\n",
    "        \n",
    "        # Retrieve candidate pool titles\n",
    "        candidate_pool_titles = sentence_predictions['candidate_pools_titles'][m]\n",
    "        \n",
    "        # Convert candidate pool titles to candidate pool vectors\n",
    "        candidate_pool_vectors = get_candidate_pool_vectors(candidate_pool_titles, verbose=verbose)\n",
    "        \n",
    "        # Save candidate pool vectors to dictionary\n",
    "        vector_dict[m] = candidate_pool_vectors\n",
    "    \n",
    "    return vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate congruent metric for each candidate in every mention's candidate pool\n",
    "def get_congruence_dict(vector_dict, sentence_mentions_nums, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return congruence metric calculations between all candidates for all mentions\n",
    "    Input:\n",
    "    - vector_dict: todo this could be generalized to allow comparison between text/ints/vectors\n",
    "    - Sentence Mentions Numerical Representation: Integers representing congruent mentions in a context domain\n",
    "    Outputs:\n",
    "    - Dictionary with congruence metric calculations for everyone\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Save congruence metrics in a two-level dictionary\n",
    "    # Create first-level dictionary to be returned\n",
    "    congruence_dict = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mentions_nums)-1:\n",
    "        \n",
    "        # Create second-level congruence metric dictionary\n",
    "        m_dict = {}\n",
    "\n",
    "        # Compare eaech mention against mentions after it\n",
    "        for n in sentence_mentions_nums[m+1:]:\n",
    "            if verbose: print(f\"Comparing mentions {m} & {n}\")\n",
    "            # Calculate congruence metric - cosine similarity todo generalize\n",
    "            congruence_metric = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "            # Save congruence metric to second-level dictionary\n",
    "            m_dict[n] = congruence_metric\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        congruence_dict[m] = m_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "    \n",
    "    return congruence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standardize on form lookup always row then column\n",
    "def get_most_congruent_pair(congruence_matrix, verbose=False):\n",
    "    \"\"\"\n",
    "    This function takes a congruence matrix and returns the indices\n",
    "    of the two most congruent candidates using your chosen metric.\n",
    "    These indices can be plugged back into the candidate pool lists\n",
    "    to determine which candidates are most similar.\n",
    "    \"\"\"\n",
    "    # Get max values for every row\n",
    "    max_row_values = congruence_matrix.max(axis=1)\n",
    "    max_row_idxs = congruence_matrix.argmax(axis=1)\n",
    "    \n",
    "    # Get overall max value and the row it is in\n",
    "    max_value = max_row_values.max()\n",
    "    max_row_idx = max_row_values.argmax()\n",
    "    \n",
    "    # Get column max value is in\n",
    "    max_column_idx = max_row_idxs[max_row_idx]\n",
    "    \n",
    "    return (max_row_idx, max_column_idx), max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the most congruent pair amongst remaining mentions\n",
    "# todo should I pass congruence_dict in here?\n",
    "def find_most_congruent_pair(mention_predictions, mentions_remaining, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to search the congruence matrices for mentions without predictions\n",
    "    and return the most congruent pair of candidates and associated mentions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with empty most_congruent_pair\n",
    "    # (Candidate Pair, Congruence Metric), (Mention A, Mention B)\n",
    "    most_congruent_pair = ((None, 0.0), (0, 0))\n",
    "    \n",
    "    # Assess whether first pass or recursive pass\n",
    "    if len(mention_predictions) == 0:\n",
    "\n",
    "        # First pass        \n",
    "        for m in mentions_remaining:\n",
    "            for n in mentions_remaining[m+1:]:\n",
    "\n",
    "                # Get most congruent pair in one matrix\n",
    "                congruent_pair = get_most_congruent_pair(congruence_dict[m][n], verbose=verbose)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Comparing {m} & {n}\")\n",
    "                    print(\"Congruent Pair: \", congruent_pair)\n",
    "                    print(\"Current Most Congruent: \", most_congruent_pair)\n",
    "                \n",
    "                if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "                    # Save most congruent candidate pair and mentions\n",
    "                    most_congruent_pair = congruent_pair, (m, n)\n",
    "\n",
    "    elif len(mention_predictions) > 0:\n",
    "        \n",
    "        # Second+, recursive pass\n",
    "        for m in mention_predictions.keys():\n",
    "            for n in mentions_remaining:\n",
    "                \n",
    "                # Becauase we always assume search small mention to large to save computation/storage,\n",
    "                # we must sort incrementing variables to be in increasing order for query\n",
    "                m_tmp, n_tmp = np.sort((m, n))\n",
    "                \n",
    "                # Get most congruent pair in one matrix\n",
    "                congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp], verbose=verbose)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "                    print(\"Congruent Pair: \", congruent_pair)\n",
    "                    print(\"Current Most Congruent: \", most_congruent_pair)\n",
    "                    \n",
    "                if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "                    # Save most congruent candidate pair and mentions\n",
    "                    most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "                \n",
    "    if verbose: print(\"Final Most Congruent Pair: \", most_congruent_pair)\n",
    "    return most_congruent_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congruent Predictions Function\n",
    "\n",
    "This is our main function that takes a sentence ID, calculates congruence for all candidates and selects predictions iteratively based on that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate congruent predictions\n",
    "def get_congruent_predictions(sentence_id, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to calculate congruence metrics over a set of entity full mentions\n",
    "    and return the predicted candidates based on the congruent metric\n",
    "    Input:\n",
    "    - todo Should I pass the dataframe as well?\n",
    "    - Sentence ID used to filter dataframe\n",
    "    Output:\n",
    "    - Prediction for each entity mention\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to dataframe representing single sentence\n",
    "    sentence_predictions = predictions[predictions['sentence_id'] == sentence_id].reset_index(drop=True)\n",
    "    \n",
    "    # Define numerical representation of congruent mention list\n",
    "    sentence_congruent_mentions = sentence_predictions['congruent_mentions'][0]\n",
    "    sentence_mentions_nums = np.arange(len(sentence_congruent_mentions)) # todo rename\n",
    "    if verbose:\n",
    "        print(\"Congruent Mentions: \", sentence_congruent_mentions)\n",
    "        print(\"Congruent Mentions as numbers: \", sentence_mentions_nums)\n",
    "    \n",
    "    # Retrieve dictionary of candidate pool vectors for each mention\n",
    "    mention_vectors_dict = create_entity_vector_dict(sentence_mentions_nums, sentence_predictions, verbose=verbose)\n",
    "    if verbose: print(\"Mentions with Vectors: \", mention_vectors_dict.keys())\n",
    "    \n",
    "    # Calculate congruence metric for each candidate vector for each mention's candidate pool\n",
    "    # This notebook uses cosine similarity as the congruence metric\n",
    "    congruence_dict = get_congruence_dict(vector_dict, sentence_mentions_nums, verbose=verbose)\n",
    "    if verbose: print(\"First-Level Congruence Keys: \", congruence_dict.keys())\n",
    "    # This should be one less than congruent mention count, since we are comparing low to high\n",
    "    # and thus don't compare the highest value to anything\n",
    "    \n",
    "    # Create predictions dictionary\n",
    "    mention_predictions = {}\n",
    "    \n",
    "    # Create copy of sentence_mentions_nums to iterate through\n",
    "    mentions_remaining = sentence_mentions_nums.copy()\n",
    "    \n",
    "    # Iterate through congruent entity mentions to retrieve predictions\n",
    "    # where a prediction is the most congruent candidate between two mentions\n",
    "    while len(mentions_remaining) > 0:\n",
    "        \n",
    "        # Analyze congruence matrices to identify the most congruent pair\n",
    "        most_congruent_pair = find_most_congruent_pair(mention_predictions, mentions_remaining, verbose=verbose)\n",
    "        \n",
    "        # Save most congrent pair prediction for associated mentions\n",
    "        if len(mention_predictions) == 0:\n",
    "            # First pass\n",
    "            mention_predictions[most_congruent_pair[1][0]] = most_congruent_pair[0][0][0]\n",
    "            mention_predictions[most_congruent_pair[1][1]] = most_congruent_pair[0][0][1]\n",
    "        elif len(mention_predictions) > 0:\n",
    "            # Second_, recursive pass\n",
    "            \n",
    "            # Find new mention you're predicting for\n",
    "            # The number left over in the mention tuple if you remove anything in in the prediction dict\n",
    "            new_mention_num = most_congruent_pair[1].index(\\\n",
    "                                                           list(set(most_congruent_pair[1])\\\n",
    "                                                                - set(mention_predictions.keys())))\n",
    "            \n",
    "            # Save new prediction\n",
    "            mention_predictions[most_congruent_pair[1][new_mention_num]] = most_congruent_pair[0][0][new_mention_num]\n",
    "            \n",
    "        # Update remaining mentions to mentions without a prediction stored in the dictionary\n",
    "        mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "        \n",
    "    # Use mention predictions to return titles\n",
    "    readable_predictions = {}\n",
    "    for k, v in mention_predictions.items():\n",
    "        if verbose: print(k, v)\n",
    "        readable_key = sentence_predictions['full_mention'][k]\n",
    "        readable_value = sentence_predictions['candidate_pools_titles'][k][v]\n",
    "        if verbose: print(readable_key, readable_value)\n",
    "        readable_predictions[readable_key] = readable_value\n",
    "    \n",
    "    # Output dictionary with predictions for each entity mention based on congruence\n",
    "    return readable_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congruent Mentions:  ['Iran', 'Turkey', 'Iraq', 'Kurdish']\n",
      "Congruent Mentions as numbers:  [0 1 2 3]\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Mentions with Vectors:  dict_keys([0, 1, 2, 3])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 0 & 3\n",
      "Comparing mentions 1 & 2\n",
      "Comparing mentions 1 & 3\n",
      "Comparing mentions 2 & 3\n",
      "First-Level Congruence Keys:  dict_keys([0, 1, 2])\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "Current Most Congruent:  ((None, 0.0), (0, 0))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 1), 0.8189154129945448)\n",
      "Current Most Congruent:  (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 2), 0.6412995855581739)\n",
      "Current Most Congruent:  (((1, 1), 0.8189154129945448), (0, 2))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((1, 1), 0.6947784687002414)\n",
      "Current Most Congruent:  (((1, 1), 0.8189154129945448), (0, 2))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "Current Most Congruent:  (((1, 1), 0.8189154129945448), (0, 2))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "Current Most Congruent:  (((1, 1), 0.8189154129945448), (0, 2))\n",
      "Final Most Congruent Pair:  (((1, 1), 0.8189154129945448), (0, 2))\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "Current Most Congruent:  ((None, 0.0), (0, 0))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 2), 0.6412995855581739)\n",
      "Current Most Congruent:  (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((1, 1), 0.6947784687002414)\n",
      "Current Most Congruent:  (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "Current Most Congruent:  (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Final Most Congruent Pair:  (((0, 6), 0.78401095), (2, 3))\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "Current Most Congruent:  ((None, 0.0), (0, 0))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((1, 1), 0.6947784687002414)\n",
      "Current Most Congruent:  (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "Current Most Congruent:  (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Final Most Congruent Pair:  (((3, 7), 0.7314480614025645), (0, 1))\n",
      "0 1\n",
      "Iran Iran_national_football_team\n",
      "2 1\n",
      "Iraq Iraq_national_football_team\n",
      "3 6\n",
      "Kurdish Iraqi_Kurdistan\n",
      "1 7\n",
      "Turkey Turkey_men's_national_basketball_team\n",
      "CPU times: user 8.92 ms, sys: 3.73 ms, total: 12.7 ms\n",
      "Wall time: 11.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Iran': 'Iran_national_football_team',\n",
       " 'Iraq': 'Iraq_national_football_team',\n",
       " 'Kurdish': 'Iraqi_Kurdistan',\n",
       " 'Turkey': \"Turkey_men's_national_basketball_team\"}"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Test out function\n",
    "get_congruent_predictions(sentence_id=sentence_id, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical Flow Demonstration\n",
    "\n",
    "The cells below have been included as a more easily understood logical flow to understand how we designed the recursive congruence algorithm for an arbitrary length of full mentions in a sentence. We manually select a sentence and work through that. This is identical to the above but with more printed out breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>mention_candidate_pools_page_ids</th>\n",
       "      <th>mention_candidate_pools_item_ids</th>\n",
       "      <th>candidate_pools_titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Iran</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iran</td>\n",
       "      <td>14653.0</td>\n",
       "      <td>Iran</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iran</td>\n",
       "      <td>[14653, 272865, 338883, 8294810, 46823116, 126...</td>\n",
       "      <td>[794, 184602, 207991, 1465546, 932162, 1042614...</td>\n",
       "      <td>[Iran, Iran_national_football_team, Pahlavi_dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>turkey</td>\n",
       "      <td>[11125639, 743577, 72821, 24513964, 297071, 22...</td>\n",
       "      <td>[43, 483856, 43794, 4200953, 26844, 12560, 848...</td>\n",
       "      <td>[Turkey, Turkey_national_football_team, Turkey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iraq</td>\n",
       "      <td>7515928.0</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iraq</td>\n",
       "      <td>[7515928, 1039652, 5043324, 26215470, 2900620,...</td>\n",
       "      <td>[796, 186243, 545449, 3108185, 149805, 107802,...</td>\n",
       "      <td>[Iraq, Iraq_national_football_team, Iraq_War, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Kurdish</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kurdish_people</td>\n",
       "      <td>17068.0</td>\n",
       "      <td>Kurdish people</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>[17068, 40316, 80777, 3821855, 4314285, 354232...</td>\n",
       "      <td>[12223, 36368, 41470, 1792998, 1117020, 121801...</td>\n",
       "      <td>[Kurds, Kurdish_languages, Kurdistan, Kurds_in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention full_mention                                wikipedia_URL  \\\n",
       "0       B         Iran            http://en.wikipedia.org/wiki/Iran   \n",
       "1       B       Turkey                                          NaN   \n",
       "2       B         Iraq            http://en.wikipedia.org/wiki/Iraq   \n",
       "3       B      Kurdish  http://en.wikipedia.org/wiki/Kurdish_people   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            14653.0            Iran           90      11   \n",
       "1                NaN             NaN           90      11   \n",
       "2          7515928.0            Iraq           90      11   \n",
       "3            17068.0  Kurdish people           90      11   \n",
       "\n",
       "              congruent_mentions norm_full_mention  \\\n",
       "0  [Iran, Turkey, Iraq, Kurdish]              iran   \n",
       "1  [Iran, Turkey, Iraq, Kurdish]            turkey   \n",
       "2  [Iran, Turkey, Iraq, Kurdish]              iraq   \n",
       "3  [Iran, Turkey, Iraq, Kurdish]           kurdish   \n",
       "\n",
       "                    mention_candidate_pools_page_ids  \\\n",
       "0  [14653, 272865, 338883, 8294810, 46823116, 126...   \n",
       "1  [11125639, 743577, 72821, 24513964, 297071, 22...   \n",
       "2  [7515928, 1039652, 5043324, 26215470, 2900620,...   \n",
       "3  [17068, 40316, 80777, 3821855, 4314285, 354232...   \n",
       "\n",
       "                    mention_candidate_pools_item_ids  \\\n",
       "0  [794, 184602, 207991, 1465546, 932162, 1042614...   \n",
       "1  [43, 483856, 43794, 4200953, 26844, 12560, 848...   \n",
       "2  [796, 186243, 545449, 3108185, 149805, 107802,...   \n",
       "3  [12223, 36368, 41470, 1792998, 1117020, 121801...   \n",
       "\n",
       "                              candidate_pools_titles  \n",
       "0  [Iran, Iran_national_football_team, Pahlavi_dy...  \n",
       "1  [Turkey, Turkey_national_football_team, Turkey...  \n",
       "2  [Iraq, Iraq_national_football_team, Iraq_War, ...  \n",
       "3  [Kurds, Kurdish_languages, Kurdistan, Kurds_in...  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on manually selected sentence\n",
    "sentence_predictions = predictions[predictions['sentence_id'] == sentence_id].reset_index(drop=True)\n",
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iran', 'Turkey', 'Iraq', 'Kurdish']\n"
     ]
    }
   ],
   "source": [
    "# Congruent Mention\n",
    "print(sentence_predictions['congruent_mentions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numerical for easier recursive logic later\n",
    "sentence_mention_nums = np.arange(len(sentence_predictions['congruent_mentions'][0]))\n",
    "sentence_mention_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate lists of vectors\n",
    "def get_candidate_pool_vectors(candidate_pool_titles, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return entity vectors from Wikipedia2Vec\n",
    "    Takes as input a list of page titles, representing the candidate pool\n",
    "    Normalizes each page title to match necessary input format\n",
    "    Returns entity vector or empty vector if no match\n",
    "    \"\"\"\n",
    "    # Track failed vector queries\n",
    "    no_vector_count = 0\n",
    "    candidate_pool_vectors = []\n",
    "    for candidate in candidate_pool_titles:\n",
    "        candidate = normalize_text(candidate)\n",
    "        try:\n",
    "            candidate_vectors = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            # Keep empty vector representation to maintain index locations\n",
    "            candidate_vectors = np.zeros(100)\n",
    "            no_vector_count += 1\n",
    "        candidate_pool_vectors.append(candidate_vectors)\n",
    "    \n",
    "    if verbose: print(f\"Failed Wikipedia2Vec Entity Vector Queries: {no_vector_count}\")\n",
    "    return candidate_pool_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectors in dictionary\n",
    "vector_dict = {}\n",
    "\n",
    "# For each full mention we are analyzing in the contextual domain (i.e. sentence)\n",
    "for m in sentence_mention_nums:\n",
    "    \n",
    "    # Retrieve candidate pool titles\n",
    "    candidate_pool_titles = sentence_predictions['candidate_pools_titles'][m]\n",
    "    \n",
    "    # Convert candidate pool titles to candidate pool vectors\n",
    "    candidate_pool_vectors = get_candidate_pool_vectors(candidate_pool_titles, verbose=True)\n",
    "    \n",
    "    # Save candidate pool vectors to dictionary\n",
    "    vector_dict[m] = candidate_pool_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "memmap([-0.12054954, -0.50742626,  0.06441808, -0.5646962 , -0.6091529 ,\n",
       "        -0.62058866,  0.34451225, -0.59047294,  0.14729242, -0.2636625 ,\n",
       "         1.238541  , -0.7548667 ,  1.780854  ,  0.2174651 , -1.2011365 ,\n",
       "        -0.23852998, -0.02962461, -1.052696  ,  0.7661167 ,  0.23614497,\n",
       "         0.3289804 ,  0.43364018,  0.44217652,  0.70779175, -0.42379835,\n",
       "         0.46034932,  0.02453618, -0.94843006,  0.69518584,  0.50452614,\n",
       "        -0.7682347 , -0.6042409 , -0.2935586 ,  0.7102783 ,  0.9417312 ,\n",
       "        -1.0872686 ,  0.11868034, -0.01909127, -0.12601753,  0.4462274 ,\n",
       "         1.0541848 , -0.07844509, -0.01494464,  0.7695808 , -0.8390789 ,\n",
       "        -0.8257057 , -0.12845561,  0.48051977, -0.58919144, -0.06965847,\n",
       "        -0.7897618 ,  0.68234414,  0.40284485, -0.49510485,  0.5415569 ,\n",
       "         1.0128313 ,  0.18627214,  0.00809101, -0.894761  ,  0.12653595,\n",
       "         1.2837574 , -0.15226471, -0.38862622,  0.41277474,  0.8946806 ,\n",
       "         0.0666991 , -0.55063474, -0.31081298, -0.49015087, -0.39218   ,\n",
       "         0.34222102,  0.32000673,  0.13176554,  0.50195974,  0.08881193,\n",
       "        -0.05729331, -0.75744575,  0.4202557 ,  0.5982818 , -1.4515332 ,\n",
       "         0.39127624,  0.03607881,  0.6681063 ,  0.09667338, -0.5147987 ,\n",
       "         0.31095037,  0.31614837, -0.236758  ,  0.6665708 , -0.0218142 ,\n",
       "         0.7358051 ,  0.19489993,  0.10365985, -0.26721543, -0.11328024,\n",
       "         0.1712962 , -0.21920836,  0.8218476 ,  0.2516369 ,  0.2441283 ],\n",
       "       dtype=float32)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display vector_dict output\n",
    "print(vector_dict.keys())\n",
    "# Preview one candidate vector from first candidate pool vectors\n",
    "vector_dict[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n"
     ]
    }
   ],
   "source": [
    "# Calculate congruence metric for each candidate vector for each mention's candidate pool\n",
    "# This notebook uses cosine similarity as the congruence metric\n",
    "\n",
    "## Save congruence measurements in a two-level dictionary\n",
    "# Create first-level dictionary\n",
    "congruence_dict = {}\n",
    "\n",
    "# Always work low numbers to high\n",
    "m = 0\n",
    "while m < len(sentence_mention_nums)-1:\n",
    "    \n",
    "    # Save second-level congruence measurement dictionary\n",
    "    m_dict = {}\n",
    "    # Compare each mention against mentions after it\n",
    "    for n in sentence_mention_nums[m+1:]:\n",
    "        # Calculate congruence measurement - cosine similarity\n",
    "        congruence_measurement = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "        # Save congruence measurement to second-level dictionary\n",
    "        m_dict[n] = congruence_measurement\n",
    "    \n",
    "    # Save second-level dictionary to first-level\n",
    "    congruence_dict[m] = m_dict\n",
    "    \n",
    "    # Increment mention\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.56212992, 0.36148783, 0.31132665, 0.38497798, 0.47352787,\n",
       "        0.25787476, 0.46396884, 0.43122558, 0.33630148, 0.33087376],\n",
       "       [0.41690328, 0.69477847, 0.31527382, 0.37472297, 0.40325552,\n",
       "        0.28097344, 0.27095665, 0.43815354, 0.59332593, 0.30506308],\n",
       "       [0.29317915, 0.24111332, 0.29271907, 0.22163597, 0.20807747,\n",
       "        0.34074995, 0.21319858, 0.32036459, 0.1304153 , 0.12607761],\n",
       "       [0.23274391, 0.25557891, 0.17535679, 0.20398974, 0.23075791,\n",
       "        0.28223372, 0.27250616, 0.23178755, 0.16910588, 0.17002748],\n",
       "       [0.28206786, 0.1866784 , 0.2785948 , 0.16865316, 0.12640467,\n",
       "        0.26873877, 0.16823895, 0.22268224, 0.1330765 , 0.10017692],\n",
       "       [0.47455753, 0.39756386, 0.31078716, 0.41740504, 0.57560418,\n",
       "        0.33900629, 0.48486113, 0.56223861, 0.36131878, 0.46812174],\n",
       "       [0.230455  , 0.22550274, 0.25124374, 0.22469996, 0.19709154,\n",
       "        0.28370466, 0.26979778, 0.23395714, 0.18510168, 0.12074457],\n",
       "       [0.35948028, 0.53769633, 0.29062405, 0.2741779 , 0.38657547,\n",
       "        0.26814114, 0.25552802, 0.35422548, 0.48355553, 0.36482511],\n",
       "       [0.34277563, 0.55144576, 0.27487674, 0.3158352 , 0.3605707 ,\n",
       "        0.23293723, 0.26203764, 0.37020876, 0.57480229, 0.30380816],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display congruence_dict output\n",
    "# This should be one less than congruent mention count, since we are comparing low to high\n",
    "# and thus don't compare the highest value to anything\n",
    "print(congruence_dict.keys())\n",
    "# Preview congruence matrix derived from comparing Mention 1 to Mention 2\n",
    "congruence_dict[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow1_col1{\n",
       "            background:  skyblue;\n",
       "        }</style><table id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745ab\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>        <th class=\"col_heading level0 col8\" >8</th>        <th class=\"col_heading level0 col9\" >9</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745ablevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow0_col0\" class=\"data row0 col0\" >0.562130</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow0_col1\" class=\"data row0 col1\" >0.361488</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow0_col2\" class=\"data row0 col2\" >0.311327</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow0_col3\" class=\"data row0 col3\" >0.384978</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow0_col4\" class=\"data row0 col4\" >0.473528</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow0_col5\" class=\"data row0 col5\" >0.257875</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow0_col6\" class=\"data row0 col6\" >0.463969</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow0_col7\" class=\"data row0 col7\" >0.431226</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow0_col8\" class=\"data row0 col8\" >0.336301</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow0_col9\" class=\"data row0 col9\" >0.330874</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745ablevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow1_col0\" class=\"data row1 col0\" >0.416903</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow1_col1\" class=\"data row1 col1\" >0.694778</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow1_col2\" class=\"data row1 col2\" >0.315274</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow1_col3\" class=\"data row1 col3\" >0.374723</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow1_col4\" class=\"data row1 col4\" >0.403256</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow1_col5\" class=\"data row1 col5\" >0.280973</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow1_col6\" class=\"data row1 col6\" >0.270957</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow1_col7\" class=\"data row1 col7\" >0.438154</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow1_col8\" class=\"data row1 col8\" >0.593326</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow1_col9\" class=\"data row1 col9\" >0.305063</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745ablevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow2_col0\" class=\"data row2 col0\" >0.293179</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow2_col1\" class=\"data row2 col1\" >0.241113</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow2_col2\" class=\"data row2 col2\" >0.292719</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow2_col3\" class=\"data row2 col3\" >0.221636</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow2_col4\" class=\"data row2 col4\" >0.208077</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow2_col5\" class=\"data row2 col5\" >0.340750</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow2_col6\" class=\"data row2 col6\" >0.213199</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow2_col7\" class=\"data row2 col7\" >0.320365</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow2_col8\" class=\"data row2 col8\" >0.130415</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow2_col9\" class=\"data row2 col9\" >0.126078</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745ablevel0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow3_col0\" class=\"data row3 col0\" >0.232744</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow3_col1\" class=\"data row3 col1\" >0.255579</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow3_col2\" class=\"data row3 col2\" >0.175357</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow3_col3\" class=\"data row3 col3\" >0.203990</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow3_col4\" class=\"data row3 col4\" >0.230758</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow3_col5\" class=\"data row3 col5\" >0.282234</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow3_col6\" class=\"data row3 col6\" >0.272506</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow3_col7\" class=\"data row3 col7\" >0.231788</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow3_col8\" class=\"data row3 col8\" >0.169106</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow3_col9\" class=\"data row3 col9\" >0.170027</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745ablevel0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow4_col0\" class=\"data row4 col0\" >0.282068</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow4_col1\" class=\"data row4 col1\" >0.186678</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow4_col2\" class=\"data row4 col2\" >0.278595</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow4_col3\" class=\"data row4 col3\" >0.168653</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow4_col4\" class=\"data row4 col4\" >0.126405</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow4_col5\" class=\"data row4 col5\" >0.268739</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow4_col6\" class=\"data row4 col6\" >0.168239</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow4_col7\" class=\"data row4 col7\" >0.222682</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow4_col8\" class=\"data row4 col8\" >0.133077</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow4_col9\" class=\"data row4 col9\" >0.100177</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745ablevel0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow5_col0\" class=\"data row5 col0\" >0.474558</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow5_col1\" class=\"data row5 col1\" >0.397564</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow5_col2\" class=\"data row5 col2\" >0.310787</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow5_col3\" class=\"data row5 col3\" >0.417405</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow5_col4\" class=\"data row5 col4\" >0.575604</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow5_col5\" class=\"data row5 col5\" >0.339006</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow5_col6\" class=\"data row5 col6\" >0.484861</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow5_col7\" class=\"data row5 col7\" >0.562239</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow5_col8\" class=\"data row5 col8\" >0.361319</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow5_col9\" class=\"data row5 col9\" >0.468122</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745ablevel0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow6_col0\" class=\"data row6 col0\" >0.230455</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow6_col1\" class=\"data row6 col1\" >0.225503</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow6_col2\" class=\"data row6 col2\" >0.251244</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow6_col3\" class=\"data row6 col3\" >0.224700</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow6_col4\" class=\"data row6 col4\" >0.197092</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow6_col5\" class=\"data row6 col5\" >0.283705</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow6_col6\" class=\"data row6 col6\" >0.269798</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow6_col7\" class=\"data row6 col7\" >0.233957</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow6_col8\" class=\"data row6 col8\" >0.185102</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow6_col9\" class=\"data row6 col9\" >0.120745</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745ablevel0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow7_col0\" class=\"data row7 col0\" >0.359480</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow7_col1\" class=\"data row7 col1\" >0.537696</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow7_col2\" class=\"data row7 col2\" >0.290624</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow7_col3\" class=\"data row7 col3\" >0.274178</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow7_col4\" class=\"data row7 col4\" >0.386575</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow7_col5\" class=\"data row7 col5\" >0.268141</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow7_col6\" class=\"data row7 col6\" >0.255528</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow7_col7\" class=\"data row7 col7\" >0.354225</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow7_col8\" class=\"data row7 col8\" >0.483556</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow7_col9\" class=\"data row7 col9\" >0.364825</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745ablevel0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow8_col0\" class=\"data row8 col0\" >0.342776</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow8_col1\" class=\"data row8 col1\" >0.551446</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow8_col2\" class=\"data row8 col2\" >0.274877</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow8_col3\" class=\"data row8 col3\" >0.315835</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow8_col4\" class=\"data row8 col4\" >0.360571</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow8_col5\" class=\"data row8 col5\" >0.232937</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow8_col6\" class=\"data row8 col6\" >0.262038</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow8_col7\" class=\"data row8 col7\" >0.370209</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow8_col8\" class=\"data row8 col8\" >0.574802</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow8_col9\" class=\"data row8 col9\" >0.303808</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745ablevel0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow9_col0\" class=\"data row9 col0\" >0.000000</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow9_col1\" class=\"data row9 col1\" >0.000000</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow9_col2\" class=\"data row9 col2\" >0.000000</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow9_col3\" class=\"data row9 col3\" >0.000000</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow9_col4\" class=\"data row9 col4\" >0.000000</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow9_col5\" class=\"data row9 col5\" >0.000000</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow9_col6\" class=\"data row9 col6\" >0.000000</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow9_col7\" class=\"data row9 col7\" >0.000000</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow9_col8\" class=\"data row9 col8\" >0.000000</td>\n",
       "                        <td id=\"T_2d3cd96c_2c37_11eb_8a72_a683e72745abrow9_col9\" class=\"data row9 col9\" >0.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9a4bc155d0>"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congruence_test_df = pd.DataFrame(congruence_dict[1][2])\n",
    "max_num = max(np.max(congruence_test_df))\n",
    "congruence_test_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  4\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "Length:  4\n",
      "1 2\n",
      "1 3\n",
      "Length:  4\n",
      "2 3\n",
      "Length:  4\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of logic to ensure unique mention congruence only calculated once\n",
    "m = 0\n",
    "while m < len(sentence_mention_nums):\n",
    "    print(\"Length: \", len(sentence_mention_nums))\n",
    "    for i in sentence_mention_nums[m+1:]:\n",
    "        print(m, i)\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 1), 0.8189154129945448)\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 2), 0.6412995855581739)\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((1, 1), 0.6947784687002414)\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((1, 1), 0.6947784687002414), 1, 2)\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "most_congruent_pair = (None, 0.0, 0, 0) # Most Congruent Candidates, Congruence Metric, Mention A, Mention B\n",
    "\n",
    "for m in sentence_mention_nums:\n",
    "    for n in sentence_mention_nums[m+1:]:\n",
    "        print(f\"Comparing {m} & {n}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m][n])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, m, n\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1}"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save max congruent pair estimates for each mention\n",
    "mention_predictions = {}\n",
    "mention_predictions[most_congruent_pair[1]] = most_congruent_pair[0][0][0]\n",
    "mention_predictions[most_congruent_pair[2]] = most_congruent_pair[0][0][1]\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have predictions for  dict_keys([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# With two mentions set in their predictions, we must filter the other congruent matrices to find the next most\n",
    "print(\"We have predictions for \", mention_predictions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(sentence_mention_nums) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "MAX: ((None, 0.0), (0, 0))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "MAX: (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 1), 0.8189154129945448)\n",
      "MAX: (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "MAX: (((1, 1), 0.8189154129945448), (0, 2))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((1, 1), 0.8189154129945448), (0, 2))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = ((None, 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1, 0: 1}"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "MAX: ((None, 0.0), (0, 0))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "MAX: (((0, 4), 0.6645409154870272), (1, 3))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 2), 0.6412995855581739)\n",
      "MAX: (((0, 6), 0.78401095), (2, 3))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((0, 6), 0.78401095), (2, 3))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = ((None, 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1, 0: 1, 3: 6}"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've predicted everything!\n"
     ]
    }
   ],
   "source": [
    "if len(mentions_remaining) == 0:\n",
    "    print(\"You've predicted everything!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
