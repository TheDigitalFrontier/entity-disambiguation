{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congruence via Entity Vector Similarity\n",
    "\n",
    "In this notebook, we calculate the cosine similarity measure between vectors of each entity candidate. The vectors will be retrieved from Wikipedia2vec's pre-trained API, which creates vectors for the entire Wikipedia page. Comparing two vectors in this way thus lets us make a statement about similar pages and update our likelihood scores based on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>mention_candidate_pools_page_ids</th>\n",
       "      <th>mention_candidate_pools_item_ids</th>\n",
       "      <th>candidate_pools_titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 21347120, 9477, 1882861, 3261189,...</td>\n",
       "      <td>[458, 46, 211593, 1396, 363404, 3327447, 40537...</td>\n",
       "      <td>['European_Union', 'Europe', 'Eu,_Seine-Mariti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 11884, 152735, 21212, 12674, 290327, 1...</td>\n",
       "      <td>[183, 188, 42884, 7318, 43287, 141817, 181287,...</td>\n",
       "      <td>['Germany', 'German_language', 'Germans', 'Naz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>british</td>\n",
       "      <td>[31717, 19097669, 13530298, 4721, 158019, 1522...</td>\n",
       "      <td>[145, 842438, 23666, 8680, 161885, 174193, 354...</td>\n",
       "      <td>['United_Kingdom', 'British_people', 'Great_Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention     full_mention                                wikipedia_URL  \\\n",
       "0       B               EU                                          NaN   \n",
       "1       B           German         http://en.wikipedia.org/wiki/Germany   \n",
       "2       B          British  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "3       B  Peter Blackburn                                          NaN   \n",
       "4       I  Peter Blackburn                                          NaN   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0                NaN             NaN            0       0   \n",
       "1            11867.0         Germany            0       0   \n",
       "2            31717.0  United Kingdom            0       0   \n",
       "3                NaN             NaN            1       0   \n",
       "4                NaN             NaN            1       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0                        ['EU', 'German', 'British']                eu   \n",
       "1                        ['EU', 'German', 'British']            german   \n",
       "2                        ['EU', 'German', 'British']           british   \n",
       "3  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "4  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "\n",
       "                    mention_candidate_pools_page_ids  \\\n",
       "0  [9317, 9239, 21347120, 9477, 1882861, 3261189,...   \n",
       "1  [11867, 11884, 152735, 21212, 12674, 290327, 1...   \n",
       "2  [31717, 19097669, 13530298, 4721, 158019, 1522...   \n",
       "3                      [56783206, 9643132, 56873217]   \n",
       "4                      [56783206, 9643132, 56873217]   \n",
       "\n",
       "                    mention_candidate_pools_item_ids  \\\n",
       "0  [458, 46, 211593, 1396, 363404, 3327447, 40537...   \n",
       "1  [183, 188, 42884, 7318, 43287, 141817, 181287,...   \n",
       "2  [145, 842438, 23666, 8680, 161885, 174193, 354...   \n",
       "3                       [2073954, 7172840, 26634508]   \n",
       "4                       [2073954, 7172840, 26634508]   \n",
       "\n",
       "                              candidate_pools_titles  \n",
       "0  ['European_Union', 'Europe', 'Eu,_Seine-Mariti...  \n",
       "1  ['Germany', 'German_language', 'Germans', 'Naz...  \n",
       "2  ['United_Kingdom', 'British_people', 'Great_Br...  \n",
       "3  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...  \n",
       "4  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base path to input\n",
    "preds_path = '../../predictions/'\n",
    "\n",
    "# Load data\n",
    "predictions = pd.read_csv(os.path.join(preds_path, \"anchortext_frequency.csv\"), delimiter=\",\")\n",
    "predictions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Saved Candidate Pool\n",
    "\n",
    "Candidate pools when exported are typically stored as the string of a list. The below function parses the string back into a list with proper formatted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate that list is string\n",
    "type(predictions['mention_candidate_pools_page_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse list as string\n",
    "def parse_list_string(list_string, value_type=int):\n",
    "    \n",
    "    parsed_list = []\n",
    "    \n",
    "    # If candidate pool is empty\n",
    "    if list_string == \"[]\" or isinstance(list_string, float):\n",
    "        pass\n",
    "    # Else parse\n",
    "    else:\n",
    "        # Parses lists of titles as strings\n",
    "        if value_type==str:\n",
    "            # Eliminate bracket and parenthesis on either side, split by comma pattern\n",
    "            parsed_list = re.split(\"', '|\\\", \\\"|', \\\"|\\\", \\'\", list_string[2:-2])\n",
    "\n",
    "        # Parses lists of IDs as ints\n",
    "        elif value_type==int:\n",
    "            # Eliminate brackets and convert each number from string to int\n",
    "            parsed_list = list(map(int, list_string[1:-1].split(', ')))\n",
    "            \n",
    "        \n",
    "    return parsed_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['European_Union',\n",
       " 'Europe',\n",
       " 'Eu,_Seine-Maritime',\n",
       " 'Europium',\n",
       " 'Citizenship_of_the_European_Union',\n",
       " 'United_Left_(Galicia)',\n",
       " 'EU_(group)',\n",
       " 'European_Union_law',\n",
       " 'Eu_station',\n",
       " 'Entropy']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "# 0 is the hard one. See how some value is stored with '' and some with \"\". Unsure why.\n",
    "parse_list_string(predictions['candidate_pools_titles'][0], value_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9317,\n",
       " 9239,\n",
       " 21347120,\n",
       " 9477,\n",
       " 1882861,\n",
       " 3261189,\n",
       " 14024977,\n",
       " 276436,\n",
       " 27532324,\n",
       " 9891]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "# 0 is the hard one. See how some value is stored with '' and some with \"\". Unsure why.\n",
    "parse_list_string(predictions['mention_candidate_pools_page_ids'][0], value_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "parse_list_string(predictions['mention_candidate_pools_page_ids'][13], value_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "After ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "Before [56783206, 9643132, 56873217]\n",
      "After [56783206, 9643132, 56873217]\n",
      "Before [2073954, 7172840, 26634508]\n",
      "After [2073954, 7172840, 26634508]\n",
      "Before ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(bishop)', 'Peter_Blackburn_(MP)']\n",
      "After ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(bishop)', 'Peter_Blackburn_(MP)']\n",
      "CPU times: user 590 ms, sys: 200 ms, total: 790 ms\n",
      "Wall time: 812 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Apply defined function to entire dataframe for all candidate pool columns\n",
    "\n",
    "column = 'congruent_mentions'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=str)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "column = 'mention_candidate_pools_page_ids'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=int)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "\n",
    "column = 'mention_candidate_pools_item_ids'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=int)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pools_titles'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=str)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Entity Vectors from Wikipedia2Vec\n",
    "\n",
    "For provided wikipedia pages, we retrieve a representative entity vector from Wikipedia2vec. This involves passing the normalized title into their get_entity_vector() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package\n",
    "from wikipedia2vec import Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96.9 ms, sys: 143 ms, total: 240 ms\n",
      "Wall time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load unzipped pkl file with word embeddings\n",
    "w2v = Wikipedia2Vec.load(\"../../embeddings/enwiki_20180420_100d.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Coverage of Candidate Pools in Wikipedia2vec\n",
    "\n",
    "We need to measure what percent of candidates in our candidate pools successfully return a vector from Wikipedia2vec. This should conceivably be 100% given we're passing known Wikipedia pages into this package trained over Wikipedia pages, but there may be some drop-off due to different creation dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text normalization function\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    We define normalized as:\n",
    "    - strip whitespace\n",
    "    - Spaces, not underlines\n",
    "    \"\"\"\n",
    "    return str(text).strip().replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29312/29312 [00:02<00:00, 9909.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia2vec returned an entity vector for 93.614% of 155,553 searches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over candidate pool titles to see what can be returned\n",
    "\n",
    "found_entity = 0\n",
    "searched_entity = 0\n",
    "\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    \n",
    "    # Retrieve candidate pool\n",
    "    candidate_pool = predictions['candidate_pools_titles'][i]\n",
    "    \n",
    "    # Query for each candidate\n",
    "    for candidate in candidate_pool:\n",
    "        # Normalize candidate title to form necessary to input into Wikipedia2vec\n",
    "        candidate = normalize_text(candidate)\n",
    "        \n",
    "        # Query Wikipedia2vec get_entity_vector()\n",
    "        try:\n",
    "            entity_vector = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            entity_vector = None\n",
    "        \n",
    "        # Check if result\n",
    "        if entity_vector is not None:\n",
    "            found_entity += 1\n",
    "        \n",
    "        # Increment count\n",
    "        searched_entity += 1\n",
    "\n",
    "print(f\"Wikipedia2vec returned an entity vector for {round(found_entity/searched_entity*100,3)}% of {searched_entity:,} searches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Congruence Metric between Congruent Entities\n",
    "\n",
    "First, let's get a sense for what the upper bound of congruent calculations might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the maximum number of congruent entities in a single sentence\n",
    "max(predictions['congruent_mentions'].apply(parse_list_string, value_type=str).apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What is the distribution of congruent mention counts\n",
    "plt.figure(figsize=(20,5))\n",
    "predictions['congruent_mentions'].apply(parse_list_string, value_type=str).apply(len).hist(bins=50)\n",
    "plt.title(\"distribution of congruent mention counts\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Current Design Thinking\n",
    "\n",
    "In order to allow this to become recursive for N many tables, you will need to capture a congruence table for every candidate pool to every other candidate pool in a two-level dictionary so you can retrieve values using `matrix[3][1]`. This would entail duplication except to save that, we sort by value so you always search [small][large]. Saves us computation and storage. To see a sequential example of our logic, scroll to the end of this notebook. Below is our function-based implementation of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>mention_candidate_pools_page_ids</th>\n",
       "      <th>mention_candidate_pools_item_ids</th>\n",
       "      <th>candidate_pools_titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Iran</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iran</td>\n",
       "      <td>14653.0</td>\n",
       "      <td>Iran</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iran</td>\n",
       "      <td>[14653, 272865, 338883, 8294810, 46823116, 126...</td>\n",
       "      <td>[794, 184602, 207991, 1465546, 932162, 1042614...</td>\n",
       "      <td>[Iran, Iran_national_football_team, Pahlavi_dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>turkey</td>\n",
       "      <td>[11125639, 743577, 72821, 24513964, 297071, 22...</td>\n",
       "      <td>[43, 483856, 43794, 4200953, 26844, 12560, 848...</td>\n",
       "      <td>[Turkey, Turkey_national_football_team, Turkey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iraq</td>\n",
       "      <td>7515928.0</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iraq</td>\n",
       "      <td>[7515928, 1039652, 5043324, 26215470, 2900620,...</td>\n",
       "      <td>[796, 186243, 545449, 3108185, 149805, 107802,...</td>\n",
       "      <td>[Iraq, Iraq_national_football_team, Iraq_War, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Kurdish</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kurdish_people</td>\n",
       "      <td>17068.0</td>\n",
       "      <td>Kurdish people</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>[17068, 40316, 80777, 3821855, 4314285, 354232...</td>\n",
       "      <td>[12223, 36368, 41470, 1792998, 1117020, 121801...</td>\n",
       "      <td>[Kurds, Kurdish_languages, Kurdistan, Kurds_in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention full_mention                                wikipedia_URL  \\\n",
       "0       B         Iran            http://en.wikipedia.org/wiki/Iran   \n",
       "1       B       Turkey                                          NaN   \n",
       "2       B         Iraq            http://en.wikipedia.org/wiki/Iraq   \n",
       "3       B      Kurdish  http://en.wikipedia.org/wiki/Kurdish_people   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            14653.0            Iran           90      11   \n",
       "1                NaN             NaN           90      11   \n",
       "2          7515928.0            Iraq           90      11   \n",
       "3            17068.0  Kurdish people           90      11   \n",
       "\n",
       "              congruent_mentions norm_full_mention  \\\n",
       "0  [Iran, Turkey, Iraq, Kurdish]              iran   \n",
       "1  [Iran, Turkey, Iraq, Kurdish]            turkey   \n",
       "2  [Iran, Turkey, Iraq, Kurdish]              iraq   \n",
       "3  [Iran, Turkey, Iraq, Kurdish]           kurdish   \n",
       "\n",
       "                    mention_candidate_pools_page_ids  \\\n",
       "0  [14653, 272865, 338883, 8294810, 46823116, 126...   \n",
       "1  [11125639, 743577, 72821, 24513964, 297071, 22...   \n",
       "2  [7515928, 1039652, 5043324, 26215470, 2900620,...   \n",
       "3  [17068, 40316, 80777, 3821855, 4314285, 354232...   \n",
       "\n",
       "                    mention_candidate_pools_item_ids  \\\n",
       "0  [794, 184602, 207991, 1465546, 932162, 1042614...   \n",
       "1  [43, 483856, 43794, 4200953, 26844, 12560, 848...   \n",
       "2  [796, 186243, 545449, 3108185, 149805, 107802,...   \n",
       "3  [12223, 36368, 41470, 1792998, 1117020, 121801...   \n",
       "\n",
       "                              candidate_pools_titles  \n",
       "0  [Iran, Iran_national_football_team, Pahlavi_dy...  \n",
       "1  [Turkey, Turkey_national_football_team, Turkey...  \n",
       "2  [Iraq, Iraq_national_football_team, Iraq_War, ...  \n",
       "3  [Kurds, Kurdish_languages, Kurdistan, Kurds_in...  "
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define testing sentence_id\n",
    "sentence_id = 90\n",
    "sentence_predictions = predictions[predictions['sentence_id'] == sentence_id].reset_index(drop=True)\n",
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iran', 'Turkey', 'Iraq', 'Kurdish']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_predictions['congruent_mentions'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to Create Modular Congruent Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate lists of vectors\n",
    "def get_candidate_pool_vectors(candidate_pool_titles, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return entity vectors from Wikipedia2Vec\n",
    "    Takes as input a list of page titles, representing the candidate pool\n",
    "    Normalizes each page title to match necessary input format\n",
    "    Returns entity vector or empty vector if no match\n",
    "    \"\"\"\n",
    "    # Track failed vector queries\n",
    "    no_vector_count = 0\n",
    "    candidate_pool_vectors = []\n",
    "    for candidate in candidate_pool_titles:\n",
    "        candidate = normalize_text(candidate)\n",
    "        if verbose: print(candidate)\n",
    "        try:\n",
    "            candidate_vectors = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            # Keep empty vector representation to maintain index locations\n",
    "            candidate_vectors = np.zeros(100)\n",
    "            no_vector_count += 1\n",
    "        candidate_pool_vectors.append(candidate_vectors)\n",
    "    \n",
    "    # Handle case where candidate pool is empty from Phase 3\n",
    "    # Add arbitrarily chosen 3 arrays of zeros\n",
    "    if len(candidate_pool_titles) == 0:\n",
    "        candidate_pool_vectors = [np.zeros(100), np.zeros(100), np.zeros(100)]\n",
    "    \n",
    "    if verbose: print(f\"Failed Wikipedia2Vec Entity Vector Queries: {no_vector_count}\")\n",
    "    return candidate_pool_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to retrieve entity vectors\n",
    "def create_entity_vector_dict(sentence_mention_ids, sentence_predictions, verbose=False):\n",
    "    \"\"\"\n",
    "    Function iterates over a provided list of congruent mentions,\n",
    "    finds the associated candidate pool for each mention\n",
    "    and returns the candidate pool vector\n",
    "    \"\"\"\n",
    "    # Save vectors in dictionary\n",
    "    vector_dict = {}\n",
    "    \n",
    "    # For each full mention we are analyzing in the contextual domain\n",
    "    for m in sentence_mention_ids:\n",
    "        \n",
    "        # Retrieve candidate pool titles\n",
    "        candidate_pool_titles = sentence_predictions['candidate_pools_titles'][m]\n",
    "        if verbose: print(candidate_pool_titles)\n",
    "        \n",
    "        # Convert candidate pool titles to candidate pool vectors\n",
    "        candidate_pool_vectors = get_candidate_pool_vectors(candidate_pool_titles, verbose=verbose)\n",
    "        \n",
    "        # Save candidate pool vectors to dictionary\n",
    "        vector_dict[m] = candidate_pool_vectors\n",
    "    \n",
    "    if verbose:\n",
    "        print(vector_dict.keys())\n",
    "        for k in vector_dict.keys():\n",
    "            print(len(vector_dict[k]))\n",
    "    return vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate congruent metric for each candidate in every mention's candidate pool\n",
    "def get_congruence_dict(vector_dict, sentence_mention_ids, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return congruence metric calculations between all candidates for all mentions\n",
    "    Input:\n",
    "    - vector_dict: todo this could be generalized to allow comparison between text/ints/vectors\n",
    "    - Sentence Mentions Numerical Representation: Integers representing congruent mentions in a context domain\n",
    "    Outputs:\n",
    "    - Dictionary with congruence metric calculations for everyone\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Save congruence metrics in a two-level dictionary\n",
    "    # Create first-level dictionary to be returned\n",
    "    congruence_dict = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level congruence metric dictionary\n",
    "        m_dict = {}\n",
    "\n",
    "        # Compare eaech mention against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            if verbose: print(f\"Comparing mentions {m} & {n}\")\n",
    "            # Calculate congruence metric - cosine similarity\n",
    "            congruence_metric = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "            # Save congruence metric to second-level dictionary\n",
    "            m_dict[n] = congruence_metric\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        congruence_dict[m] = m_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "    \n",
    "#     if verbose: print(congruence_dict)\n",
    "    return congruence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standardize on form lookup always row then column\n",
    "def get_most_congruent_pair(congruence_matrix, verbose=False):\n",
    "    \"\"\"\n",
    "    This function takes a congruence matrix and returns the indices\n",
    "    of the two most congruent candidates using your chosen metric.\n",
    "    These indices can be plugged back into the candidate pool lists\n",
    "    to determine which candidates are most similar.\n",
    "    \"\"\"\n",
    "    # Get max values for every row\n",
    "    max_row_values = congruence_matrix.max(axis=1)\n",
    "    max_row_idxs = congruence_matrix.argmax(axis=1)\n",
    "    \n",
    "    # Get overall max value and the row it is in\n",
    "    max_value = max_row_values.max()\n",
    "    max_row_idx = max_row_values.argmax()\n",
    "    \n",
    "    # Get column max value is in\n",
    "    max_column_idx = max_row_idxs[max_row_idx]\n",
    "    \n",
    "    return (max_row_idx, max_column_idx), max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the most congruent pair amongst remaining mentions\n",
    "def find_most_congruent_pair(mention_predictions, mentions_remaining, congruence_dict, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to search the congruence matrices for mentions without predictions\n",
    "    and return the most congruent pair of candidates and associated mentions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with empty most_congruent_pair\n",
    "    # (Candidate Pair, Congruence Metric), (Mention A, Mention B)\n",
    "    most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "    \n",
    "    # Assess whether first pass or recursive pass\n",
    "    if len(mention_predictions) == 0:\n",
    "\n",
    "        # First pass        \n",
    "        for m in mentions_remaining:\n",
    "            for n in mentions_remaining[m+1:]:\n",
    "\n",
    "                # Get most congruent pair in one matrix\n",
    "                congruent_pair = get_most_congruent_pair(congruence_dict[m][n], verbose=verbose)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Comparing {m} & {n}\")\n",
    "                    print(\"Congruent Pair: \", congruent_pair)\n",
    "                    print(\"Current Most Congruent: \", most_congruent_pair)\n",
    "                \n",
    "                if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "                    # Save most congruent candidate pair and mentions\n",
    "                    most_congruent_pair = congruent_pair, (m, n)\n",
    "\n",
    "    elif len(mention_predictions) > 0:\n",
    "        \n",
    "        # Second+, recursive pass\n",
    "        for m in mention_predictions.keys():\n",
    "            for n in mentions_remaining:\n",
    "                \n",
    "                # Becauase we always assume search small mention to large to save computation/storage,\n",
    "                # we must sort incrementing variables to be in increasing order for query\n",
    "                m_tmp, n_tmp = np.sort((m, n))\n",
    "                \n",
    "                # Get most congruent pair in one matrix\n",
    "                congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp], verbose=verbose)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "                    print(\"Congruent Pair: \", congruent_pair)\n",
    "                    print(\"Current Most Congruent: \", most_congruent_pair)\n",
    "                    \n",
    "                if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "                    # Save most congruent candidate pair and mentions\n",
    "                    most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "                \n",
    "    if verbose: print(\"Final Most Congruent Pair: \", most_congruent_pair)\n",
    "    return most_congruent_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congruent Predictions Function\n",
    "\n",
    "This is our main function that takes a sentence ID, calculates congruence for all candidates and selects predictions iteratively based on that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate congruent predictions\n",
    "def get_congruent_predictions(sentence_id, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to calculate congruence metrics over a set of entity full mentions\n",
    "    and return the predicted candidates based on the congruent metric\n",
    "    Input:\n",
    "    - todo Should I pass the dataframe as well?\n",
    "    - Sentence ID used to filter dataframe\n",
    "    Output:\n",
    "    - Prediction for each entity mention\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to dataframe representing single sentence\n",
    "    # Drop duplicates necessary for sentences with the same mention included twice\n",
    "    sentence_predictions = predictions[predictions['sentence_id'] == sentence_id]\\\n",
    "                        .drop_duplicates(['full_mention', 'wikipedia_URL', 'wikipedia_page_ID', 'wikipedia_title'])\\\n",
    "                        .reset_index(drop=True)\n",
    "    if verbose: display(sentence_predictions)\n",
    "    \n",
    "    # Define numerical representation of congruent mention list\n",
    "    sentence_congruent_mentions = sentence_predictions['congruent_mentions'][0]\n",
    "    sentence_mention_ids = np.arange(len(sentence_congruent_mentions)) # todo rename\n",
    "    if verbose:\n",
    "        print(\"Congruent Mentions: \", sentence_congruent_mentions)\n",
    "        print(\"Congruent Mentions as numbers: \", sentence_mention_ids)\n",
    "    \n",
    "    # Retrieve dictionary of candidate pool vectors for each mention\n",
    "    vectors_dict = create_entity_vector_dict(sentence_mention_ids, sentence_predictions, verbose=verbose)\n",
    "    if verbose: print(\"Mentions with Vectors: \", vectors_dict.keys())\n",
    "    \n",
    "    # Calculate congruence metric for each candidate vector for each mention's candidate pool\n",
    "    # This notebook uses cosine similarity as the congruence metric\n",
    "    congruence_dict = get_congruence_dict(vectors_dict, sentence_mention_ids, verbose=verbose)\n",
    "    if verbose: print(\"First-Level Congruence Keys: \", congruence_dict.keys())\n",
    "    # This should be one less than congruent mention count, since we are comparing low to high\n",
    "    # and thus don't compare the highest value to anything\n",
    "    \n",
    "    # Create predictions dictionary\n",
    "    mention_predictions = {}\n",
    "    \n",
    "    # Create copy of sentence_mention_ids to iterate through\n",
    "    mentions_remaining = sentence_mention_ids.copy()\n",
    "    \n",
    "    # Iterate through congruent entity mentions to retrieve predictions\n",
    "    # where a prediction is the most congruent candidate between two mentions\n",
    "    while len(mentions_remaining) > 0:\n",
    "        \n",
    "        # Analyze congruence matrices to identify the most congruent pair\n",
    "        most_congruent_pair = find_most_congruent_pair(mention_predictions, mentions_remaining, congruence_dict, verbose=verbose)\n",
    "        \n",
    "        # Save most congrent pair prediction for associated mentions\n",
    "        if len(mention_predictions) == 0:\n",
    "            # First pass\n",
    "            if verbose: print(most_congruent_pair)\n",
    "            # Handles error when nearly all candidate pools are empty\n",
    "            if most_congruent_pair == (((None, None), 0.0), (0, 0)):\n",
    "                mention_predictions[0] = 0\n",
    "            else:\n",
    "                mention_predictions[most_congruent_pair[1][0]] = most_congruent_pair[0][0][0]\n",
    "                mention_predictions[most_congruent_pair[1][1]] = most_congruent_pair[0][0][1]\n",
    "        elif len(mention_predictions) > 0:\n",
    "            # Second_, recursive pass\n",
    "            \n",
    "            # Find new mention you're predicting for\n",
    "            try:\n",
    "                # The number left over in the mention tuple if you remove anything in the prediction dict\n",
    "                new_mention_num = most_congruent_pair[1].index(\\\n",
    "                                                               list(set(most_congruent_pair[1])\\\n",
    "                                                                    - set(mention_predictions.keys())))\n",
    "\n",
    "                # Save new prediction\n",
    "                mention_predictions[most_congruent_pair[1][new_mention_num]] = most_congruent_pair[0][0][new_mention_num]\n",
    "            except ValueError:\n",
    "                for mention in mentions_remaining:\n",
    "                    mention_predictions[mention] = 0 # Zero produces error in final section to produce None prediction\n",
    "            \n",
    "        # Update remaining mentions to mentions without a prediction stored in the dictionary\n",
    "        mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "        if verbose: print(mentions_remaining, mention_predictions.keys())\n",
    "    \n",
    "    if verbose: print(mention_predictions)\n",
    "    \n",
    "    # Use mention predictions to return titles\n",
    "    readable_predictions = {}\n",
    "    for k, v in mention_predictions.items():\n",
    "        if verbose: print(k, v)\n",
    "        readable_key = sentence_congruent_mentions[k]\n",
    "        try:\n",
    "            readable_value = sentence_predictions['candidate_pools_titles'][k][v]\n",
    "        except IndexError:\n",
    "            readable_value = None # Handles case where no candidate pool was provided from Phase 3\n",
    "        except TypeError:\n",
    "            # Handles case where no congruence can be calculated\n",
    "            # Either due to one mention in sentence or two mentions but one with no candidate pool\n",
    "            readable_value = sentence_predictions['candidate_pools_titles'][0][0] # Just return top value from Phase 3\n",
    "        if verbose: print(readable_key, readable_value)\n",
    "        readable_predictions[readable_key] = readable_value\n",
    "    \n",
    "    # Output dictionary with predictions for each entity mention based on congruence\n",
    "    return readable_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>mention_candidate_pools_page_ids</th>\n",
       "      <th>mention_candidate_pools_item_ids</th>\n",
       "      <th>candidate_pools_titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Dimitris Kontogiannis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>[Dimitris Kontogiannis, Athens Newsroom, Bayer...</td>\n",
       "      <td>dimitris kontogiannis</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Athens Newsroom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>[Dimitris Kontogiannis, Athens Newsroom, Bayer...</td>\n",
       "      <td>athens newsroom</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>BayerVB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>[Dimitris Kontogiannis, Athens Newsroom, Bayer...</td>\n",
       "      <td>bayervb</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>C$</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>[Dimitris Kontogiannis, Athens Newsroom, Bayer...</td>\n",
       "      <td>c$</td>\n",
       "      <td>[101846, 482655, 5042916]</td>\n",
       "      <td>[1104069, 207312, 16]</td>\n",
       "      <td>[Canadian_dollar, Nicaraguan_córdoba, Canada]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention           full_mention wikipedia_URL  wikipedia_page_ID  \\\n",
       "0       B  Dimitris Kontogiannis           NaN                NaN   \n",
       "1       B        Athens Newsroom           NaN                NaN   \n",
       "2       B                BayerVB           NaN                NaN   \n",
       "3       B                     C$           NaN                NaN   \n",
       "\n",
       "  wikipedia_title  sentence_id  doc_id  \\\n",
       "0             NaN           49       5   \n",
       "1             NaN           49       5   \n",
       "2             NaN           49       6   \n",
       "3             NaN           49       6   \n",
       "\n",
       "                                  congruent_mentions      norm_full_mention  \\\n",
       "0  [Dimitris Kontogiannis, Athens Newsroom, Bayer...  dimitris kontogiannis   \n",
       "1  [Dimitris Kontogiannis, Athens Newsroom, Bayer...        athens newsroom   \n",
       "2  [Dimitris Kontogiannis, Athens Newsroom, Bayer...                bayervb   \n",
       "3  [Dimitris Kontogiannis, Athens Newsroom, Bayer...                     c$   \n",
       "\n",
       "  mention_candidate_pools_page_ids mention_candidate_pools_item_ids  \\\n",
       "0                               []                               []   \n",
       "1                               []                               []   \n",
       "2                               []                               []   \n",
       "3        [101846, 482655, 5042916]            [1104069, 207312, 16]   \n",
       "\n",
       "                          candidate_pools_titles  \n",
       "0                                             []  \n",
       "1                                             []  \n",
       "2                                             []  \n",
       "3  [Canadian_dollar, Nicaraguan_córdoba, Canada]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congruent Mentions:  ['Dimitris Kontogiannis', 'Athens Newsroom', 'BayerVB', 'C$']\n",
      "Congruent Mentions as numbers:  [0 1 2 3]\n",
      "[]\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "[]\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "[]\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['Canadian_dollar', 'Nicaraguan_córdoba', 'Canada']\n",
      "Canadian dollar\n",
      "Nicaraguan córdoba\n",
      "Canada\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "dict_keys([0, 1, 2, 3])\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "Mentions with Vectors:  dict_keys([0, 1, 2, 3])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 0 & 3\n",
      "Comparing mentions 1 & 2\n",
      "Comparing mentions 1 & 3\n",
      "Comparing mentions 2 & 3\n",
      "First-Level Congruence Keys:  dict_keys([0, 1, 2])\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((0, 0), 0.0)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((0, 0), 0.0)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 0), 0.0)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.0)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 0), 0.0)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.0)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Final Most Congruent Pair:  (((None, None), 0.0), (0, 0))\n",
      "(((None, None), 0.0), (0, 0))\n",
      "[1, 2, 3] dict_keys([0])\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((0, 0), 0.0)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((0, 0), 0.0)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 0), 0.0)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Final Most Congruent Pair:  (((None, None), 0.0), (0, 0))\n",
      "[] dict_keys([0, 1, 2, 3])\n",
      "{0: 0, 1: 0, 2: 0, 3: 0}\n",
      "0 0\n",
      "Dimitris Kontogiannis None\n",
      "1 0\n",
      "Athens Newsroom None\n",
      "2 0\n",
      "BayerVB None\n",
      "3 0\n",
      "C$ Canadian_dollar\n",
      "{'Dimitris Kontogiannis': None, 'Athens Newsroom': None, 'BayerVB': None, 'C$': 'Canadian_dollar'}\n",
      "CPU times: user 19.9 ms, sys: 3.02 ms, total: 22.9 ms\n",
      "Wall time: 20.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test out function\n",
    "congruent_predictions = get_congruent_predictions(sentence_id=49, verbose=True)\n",
    "print(congruent_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>mention_candidate_pools_page_ids</th>\n",
       "      <th>mention_candidate_pools_item_ids</th>\n",
       "      <th>candidate_pools_titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Germany</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>germany</td>\n",
       "      <td>[11867, 250204, 21212, 12674, 33685, 662281, 1...</td>\n",
       "      <td>[183, 43310, 7318, 43287, 41304, 154408, 12031...</td>\n",
       "      <td>[Germany, Germany_national_football_team, Nazi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>European Union</td>\n",
       "      <td>http://en.wikipedia.org/wiki/European_Union</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>European Union</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>european union</td>\n",
       "      <td>[9317, 1933156, 9317, 10890716, 276436, 265743...</td>\n",
       "      <td>[458, 1376407, 458, 185441, 208202, 319328, 36...</td>\n",
       "      <td>[European_Union, European_Boxing_Union, Europe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Werner Zwingmann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>werner zwingmann</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Britain</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>britain</td>\n",
       "      <td>[31717, 13530298, 152256, 158019, 13525, 4721,...</td>\n",
       "      <td>[145, 23666, 174193, 161885, 185103, 8680, 977...</td>\n",
       "      <td>[United_Kingdom, Great_Britain, United_Kingdom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                wikipedia_URL  \\\n",
       "0       B           Germany         http://en.wikipedia.org/wiki/Germany   \n",
       "1       B    European Union  http://en.wikipedia.org/wiki/European_Union   \n",
       "2       B  Werner Zwingmann                                          NaN   \n",
       "3       B           Britain  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            11867.0         Germany            2       0   \n",
       "1             9317.0  European Union            2       0   \n",
       "2                NaN             NaN            2       0   \n",
       "3            31717.0  United Kingdom            2       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [Germany, European Union, Werner Zwingmann, Br...           germany   \n",
       "1  [Germany, European Union, Werner Zwingmann, Br...    european union   \n",
       "2  [Germany, European Union, Werner Zwingmann, Br...  werner zwingmann   \n",
       "3  [Germany, European Union, Werner Zwingmann, Br...           britain   \n",
       "\n",
       "                    mention_candidate_pools_page_ids  \\\n",
       "0  [11867, 250204, 21212, 12674, 33685, 662281, 1...   \n",
       "1  [9317, 1933156, 9317, 10890716, 276436, 265743...   \n",
       "2                                                 []   \n",
       "3  [31717, 13530298, 152256, 158019, 13525, 4721,...   \n",
       "\n",
       "                    mention_candidate_pools_item_ids  \\\n",
       "0  [183, 43310, 7318, 43287, 41304, 154408, 12031...   \n",
       "1  [458, 1376407, 458, 185441, 208202, 319328, 36...   \n",
       "2                                                 []   \n",
       "3  [145, 23666, 174193, 161885, 185103, 8680, 977...   \n",
       "\n",
       "                              candidate_pools_titles  \n",
       "0  [Germany, Germany_national_football_team, Nazi...  \n",
       "1  [European_Union, European_Boxing_Union, Europe...  \n",
       "2                                                 []  \n",
       "3  [United_Kingdom, Great_Britain, United_Kingdom...  "
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview dataframe again\n",
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for mention, pred in congruent_predictions.items():\n",
    "    print(mention, normalize_text(pred))\n",
    "    if sentence_predictions[sentence_predictions['full_mention'] == mention]['wikipedia_title'].values[0] == normalize_text(pred):\n",
    "        accuracy += 1\n",
    "print(\"*************************************************\")\n",
    "print(f\"This congruent experiment is {round(accuracy/len(congruent_predictions)*100,3)}% accurate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Congruence Predictions and Assess Accuracy over Entire Dataframe\n",
    "\n",
    "We now apply the per-sentence structure over the whole ACY dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5,447 sentences to predict.\n"
     ]
    }
   ],
   "source": [
    "# Max sentence_id in dataframe\n",
    "max_sentence_id = max(predictions['sentence_id'])\n",
    "print(\"We have {:,} sentences to predict.\".format(max_sentence_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4516/4516 [01:42<00:00, 44.00it/s] \n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "dataframe_predictions = {}\n",
    "for sid in tqdm(predictions['sentence_id'].unique()):\n",
    "    dataframe_predictions[sid] = get_congruent_predictions(sid, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29312/29312 [00:03<00:00, 8065.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After congruence, we have achieved 36.548% accuracy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each mention\n",
    "accurate_predictions = 0\n",
    "for row in tqdm(range(len(predictions))):\n",
    "    mention_df = predictions.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    pred = dataframe_predictions[sid][fm]\n",
    "    norm_pred = normalize_text(pred)\n",
    "#     print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred, \"|||\", norm_pred==title)\n",
    "    if title == norm_pred:\n",
    "        accurate_predictions += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy.\".format(round(accurate_predictions/len(predictions)*100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rangers 5362 ||| True: Rangers F.C. ==? Pred: Rangers F.C. ||| True\n",
      "Hibernian 5362 ||| True: Hibernian F.C. ==? Pred: Hibernian F.C. ||| True\n",
      "Dundee 5362 ||| True: Dundee United F.C. ==? Pred: Dundee F.C. ||| False\n",
      "Falkirk 5362 ||| True: Falkirk F.C. ==? Pred: Falkirk F.C. ||| True\n",
      "Greenock Morton 5362 ||| True: Greenock Morton F.C. ==? Pred: Greenock Morton F.C. ||| True\n",
      "Greenock Morton 5362 ||| True: Greenock Morton F.C. ==? Pred: Greenock Morton F.C. ||| True\n",
      "St Johnstone 5362 ||| True: St. Johnstone F.C. ==? Pred: St Johnstone W.F.C. ||| False\n",
      "St Johnstone 5362 ||| True: St. Johnstone F.C. ==? Pred: St Johnstone W.F.C. ||| False\n",
      "Airdrieonians 5362 ||| True: Airdrieonians F.C. ==? Pred: Airdrieonians F.C. ||| True\n",
      "Clydebank 5362 ||| True: Clydebank F.C. ==? Pred: Clydebank F.C. ||| True\n",
      "After congruence, we have achieved 70.0% accuracy.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each mention\n",
    "accurate_predictions = 0\n",
    "rand_idx = np.random.randint(len(predictions)) # Observe random subsection of predictions\n",
    "window_size = 10\n",
    "for row in range(rand_idx, rand_idx+window_size):\n",
    "    mention_df = predictions.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    pred = dataframe_predictions[sid][fm]\n",
    "    norm_pred = normalize_text(pred)\n",
    "    print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred, \"|||\", norm_pred==title)\n",
    "    if title == norm_pred:\n",
    "        accurate_predictions += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy.\".format(round(accurate_predictions/window_size*100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical Flow Demonstration\n",
    "\n",
    "The cells below have been included as a more easily understood logical flow to understand how we designed the recursive congruence algorithm for an arbitrary length of full mentions in a sentence. We manually select a sentence and work through that. This is identical to the above but with more printed out breaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Known Oddities\n",
    "1. Test with sentence_id == 1. We only return unique mentions in a single sentence. Is that ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>mention_candidate_pools_page_ids</th>\n",
       "      <th>mention_candidate_pools_item_ids</th>\n",
       "      <th>candidate_pools_titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Iran</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iran</td>\n",
       "      <td>14653.0</td>\n",
       "      <td>Iran</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iran</td>\n",
       "      <td>[14653, 272865, 338883, 8294810, 46823116, 126...</td>\n",
       "      <td>[794, 184602, 207991, 1465546, 932162, 1042614...</td>\n",
       "      <td>[Iran, Iran_national_football_team, Pahlavi_dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>turkey</td>\n",
       "      <td>[11125639, 743577, 72821, 24513964, 297071, 22...</td>\n",
       "      <td>[43, 483856, 43794, 4200953, 26844, 12560, 848...</td>\n",
       "      <td>[Turkey, Turkey_national_football_team, Turkey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iraq</td>\n",
       "      <td>7515928.0</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iraq</td>\n",
       "      <td>[7515928, 1039652, 5043324, 26215470, 2900620,...</td>\n",
       "      <td>[796, 186243, 545449, 3108185, 149805, 107802,...</td>\n",
       "      <td>[Iraq, Iraq_national_football_team, Iraq_War, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Kurdish</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kurdish_people</td>\n",
       "      <td>17068.0</td>\n",
       "      <td>Kurdish people</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>[17068, 40316, 80777, 3821855, 4314285, 354232...</td>\n",
       "      <td>[12223, 36368, 41470, 1792998, 1117020, 121801...</td>\n",
       "      <td>[Kurds, Kurdish_languages, Kurdistan, Kurds_in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention full_mention                                wikipedia_URL  \\\n",
       "0       B         Iran            http://en.wikipedia.org/wiki/Iran   \n",
       "1       B       Turkey                                          NaN   \n",
       "2       B         Iraq            http://en.wikipedia.org/wiki/Iraq   \n",
       "3       B      Kurdish  http://en.wikipedia.org/wiki/Kurdish_people   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            14653.0            Iran           90      11   \n",
       "1                NaN             NaN           90      11   \n",
       "2          7515928.0            Iraq           90      11   \n",
       "3            17068.0  Kurdish people           90      11   \n",
       "\n",
       "              congruent_mentions norm_full_mention  \\\n",
       "0  [Iran, Turkey, Iraq, Kurdish]              iran   \n",
       "1  [Iran, Turkey, Iraq, Kurdish]            turkey   \n",
       "2  [Iran, Turkey, Iraq, Kurdish]              iraq   \n",
       "3  [Iran, Turkey, Iraq, Kurdish]           kurdish   \n",
       "\n",
       "                    mention_candidate_pools_page_ids  \\\n",
       "0  [14653, 272865, 338883, 8294810, 46823116, 126...   \n",
       "1  [11125639, 743577, 72821, 24513964, 297071, 22...   \n",
       "2  [7515928, 1039652, 5043324, 26215470, 2900620,...   \n",
       "3  [17068, 40316, 80777, 3821855, 4314285, 354232...   \n",
       "\n",
       "                    mention_candidate_pools_item_ids  \\\n",
       "0  [794, 184602, 207991, 1465546, 932162, 1042614...   \n",
       "1  [43, 483856, 43794, 4200953, 26844, 12560, 848...   \n",
       "2  [796, 186243, 545449, 3108185, 149805, 107802,...   \n",
       "3  [12223, 36368, 41470, 1792998, 1117020, 121801...   \n",
       "\n",
       "                              candidate_pools_titles  \n",
       "0  [Iran, Iran_national_football_team, Pahlavi_dy...  \n",
       "1  [Turkey, Turkey_national_football_team, Turkey...  \n",
       "2  [Iraq, Iraq_national_football_team, Iraq_War, ...  \n",
       "3  [Kurds, Kurdish_languages, Kurdistan, Kurds_in...  "
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on manually selected sentence\n",
    "sentence_predictions = predictions[predictions['sentence_id'] == sentence_id].drop_duplicates(['full_mention', 'wikipedia_page_ID', 'sentence_id']).reset_index(drop=True)\n",
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iran', 'Turkey', 'Iraq', 'Kurdish']\n"
     ]
    }
   ],
   "source": [
    "# Congruent Mention\n",
    "print(sentence_predictions['congruent_mentions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numerical for easier recursive logic later\n",
    "sentence_mention_nums = np.arange(len(sentence_predictions['congruent_mentions'][0]))\n",
    "sentence_mention_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate lists of vectors\n",
    "def get_candidate_pool_vectors(candidate_pool_titles, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return entity vectors from Wikipedia2Vec\n",
    "    Takes as input a list of page titles, representing the candidate pool\n",
    "    Normalizes each page title to match necessary input format\n",
    "    Returns entity vector or empty vector if no match\n",
    "    \"\"\"\n",
    "    # Track failed vector queries\n",
    "    no_vector_count = 0\n",
    "    candidate_pool_vectors = []\n",
    "    for candidate in candidate_pool_titles:\n",
    "        candidate = normalize_text(candidate)\n",
    "        try:\n",
    "            candidate_vectors = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            # Keep empty vector representation to maintain index locations\n",
    "            candidate_vectors = np.zeros(100)\n",
    "            no_vector_count += 1\n",
    "        candidate_pool_vectors.append(candidate_vectors)\n",
    "    \n",
    "    if len(candidate_pool_titles) == 0:\n",
    "        candidate_pool_vectors = [np.zeros(100), np.zeros(100), np.zeros(100)]\n",
    "    \n",
    "    if verbose: print(f\"Failed Wikipedia2Vec Entity Vector Queries: {no_vector_count}\")\n",
    "    return candidate_pool_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n"
     ]
    }
   ],
   "source": [
    "# Save vectors in dictionary\n",
    "vector_dict = {}\n",
    "\n",
    "# For each full mention we are analyzing in the contextual domain (i.e. sentence)\n",
    "for m in sentence_mention_nums:\n",
    "    \n",
    "    # Retrieve candidate pool titles\n",
    "    candidate_pool_titles = sentence_predictions['candidate_pools_titles'][m]\n",
    "    \n",
    "    # Convert candidate pool titles to candidate pool vectors\n",
    "    candidate_pool_vectors = get_candidate_pool_vectors(candidate_pool_titles, verbose=True)\n",
    "    \n",
    "    # Save candidate pool vectors to dictionary\n",
    "    vector_dict[m] = candidate_pool_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[memmap([-0.41593286,  0.13665228,  0.30964687,  0.1446976 , -0.91503936,\n",
       "         -0.9192706 ,  0.6723699 , -1.0184863 ,  0.25666   , -0.62258315,\n",
       "          0.9431152 , -1.0316942 ,  0.8324313 , -0.12361886, -1.1284543 ,\n",
       "          0.15290001,  0.40446466, -0.59380203,  0.52926475,  0.22164401,\n",
       "          0.21759713, -0.36450097,  0.4831973 ,  1.0148715 , -0.13544856,\n",
       "          0.09841013, -0.4246438 , -0.19373104,  0.37280568,  0.47325772,\n",
       "         -0.70658845, -0.5905401 , -0.04496856,  0.64020395,  0.7886382 ,\n",
       "         -0.31401226,  0.18823853, -0.38187152, -0.26825568, -0.46739033,\n",
       "          1.0024314 , -0.47513625, -0.03922334,  0.50150806, -1.6830856 ,\n",
       "         -1.2636914 ,  0.15564115,  0.9450831 , -1.2964325 , -0.53473103,\n",
       "          0.11903791,  0.9918431 , -0.43928528, -1.6046988 ,  1.2605379 ,\n",
       "          0.50484955, -0.33525765, -0.6033156 ,  0.14649568,  0.57480156,\n",
       "          0.4950362 ,  0.5380983 , -0.28802627, -0.12908663,  1.4525272 ,\n",
       "         -0.6721034 , -0.290234  , -0.11490036,  0.01995435,  0.11645721,\n",
       "          0.04848159,  0.4440147 ,  0.27698714,  0.08773614,  0.08299857,\n",
       "         -0.73583114, -0.1584269 , -0.45736134, -0.09971279, -0.5497644 ,\n",
       "          0.13490988, -0.70827127,  0.2486583 ,  0.14524224, -0.7952723 ,\n",
       "          0.43419433,  0.09179252, -0.82017624,  0.5181579 ,  0.48109585,\n",
       "          0.96765083,  0.47523257,  0.23172535, -0.5741588 ,  0.50994116,\n",
       "         -0.10740528, -0.7343756 ,  0.6545396 , -0.21317248,  0.55940336],\n",
       "        dtype=float32),\n",
       " memmap([-1.1159617 , -0.27841982,  0.57177955,  0.20441858, -0.29262638,\n",
       "         -1.0531373 ,  0.60736835, -0.4639362 ,  0.44201985, -1.7095635 ,\n",
       "          0.22555596, -0.31589904,  0.32147363, -0.46468848, -1.2892634 ,\n",
       "         -0.58072615,  0.3474097 , -1.5502653 ,  0.7229641 , -0.7435039 ,\n",
       "          0.36417606,  0.11367782,  0.6670594 , -0.5738307 , -1.0573424 ,\n",
       "          1.0372667 , -1.462953  ,  0.31349507,  0.8273122 ,  1.1717463 ,\n",
       "          0.1462681 , -1.6937348 , -0.36892757, -0.17849673,  0.7581419 ,\n",
       "         -0.890877  , -1.0459245 ,  0.517499  ,  0.65313536,  0.08525603,\n",
       "          0.39829925, -0.0742837 , -0.67839634, -0.04590607, -0.71639425,\n",
       "          0.01694711, -1.2485676 ,  0.563037  , -1.1846268 , -0.17041978,\n",
       "          0.2563005 ,  0.9392488 , -0.9062931 , -1.5293895 ,  1.0435704 ,\n",
       "          0.10974556, -0.55445194,  0.04419024,  0.09491808,  0.39076138,\n",
       "          0.52578074,  0.47701007, -0.304879  , -0.944539  ,  1.6868008 ,\n",
       "         -0.75463986, -0.26461047, -0.6694756 , -1.5553831 , -0.9174785 ,\n",
       "         -0.79758936,  0.89538765, -0.350236  , -0.5097094 ,  0.45113224,\n",
       "         -0.93327   ,  0.44969043, -0.6902167 , -0.2869747 , -0.3090694 ,\n",
       "          1.2814541 ,  0.07236756,  0.67859674, -1.0177056 , -0.87511903,\n",
       "          0.06018486, -0.6498316 , -0.725553  ,  0.3764946 ,  0.32924947,\n",
       "          0.8337092 ,  1.1587523 ,  0.47149795, -0.29512417,  0.46067873,\n",
       "         -1.1089634 ,  0.20894852,  0.53075904, -0.2519846 ,  1.0331831 ],\n",
       "        dtype=float32),\n",
       " memmap([-0.7371393 ,  0.2534227 , -0.18593816,  0.31432718, -1.1629965 ,\n",
       "         -1.3408155 ,  0.8738906 , -0.48496652, -0.4252081 ,  0.07150214,\n",
       "          0.40623233, -1.4960675 ,  0.05830925, -0.85367775, -1.2672446 ,\n",
       "         -0.2032334 ,  0.29821384,  0.22565311,  0.3314879 ,  0.12707135,\n",
       "          1.0971501 , -0.05550116,  0.09066786,  0.9825777 , -0.39574137,\n",
       "         -0.32237157, -0.05840002, -0.55169475,  0.70655614,  0.44602013,\n",
       "         -0.15797526, -0.26645103, -0.17848757,  0.03104896, -0.16806604,\n",
       "         -0.44744787,  0.08407208, -0.33633792, -0.09104513, -1.2362616 ,\n",
       "          0.71831274, -0.3241704 , -0.0430138 , -0.31054685, -2.4150014 ,\n",
       "         -1.6766561 ,  0.13179173,  0.9003171 , -1.5617385 , -0.5764906 ,\n",
       "          0.90408   ,  0.43315667, -0.36051562, -0.9044738 ,  1.0176809 ,\n",
       "         -0.42954695, -0.4074808 ,  0.14410259, -0.04023024,  0.5906389 ,\n",
       "          0.7190322 ,  0.35261086, -0.9625732 , -0.5062101 ,  0.7163708 ,\n",
       "         -1.3839554 , -1.058556  , -0.4858811 , -0.31216985,  0.31123114,\n",
       "         -0.07045492, -0.2532692 ,  0.33327055, -0.41272688, -0.6768232 ,\n",
       "         -0.67011845, -0.00281671, -0.46505502, -0.6653756 , -0.14428186,\n",
       "          0.18664536, -0.559874  , -0.4917949 ,  0.44397327, -0.18041183,\n",
       "          0.10508408,  0.03165118, -0.49450594,  0.36643174,  0.35303584,\n",
       "          0.3156008 ,  0.3815851 ,  0.24665245, -0.9655056 ,  0.8413072 ,\n",
       "          0.26493683,  0.47024637,  0.94496834, -0.05703713,  0.4457689 ],\n",
       "        dtype=float32),\n",
       " memmap([-0.34100324,  0.42185667,  0.5785385 ,  0.84067583, -0.45993713,\n",
       "         -1.438784  ,  0.7713169 , -1.042519  ,  0.19324568, -0.7957025 ,\n",
       "          1.761714  , -0.5873656 , -0.16250566, -1.0463696 , -0.88448983,\n",
       "         -0.6322298 ,  0.6489709 , -1.0355457 ,  0.25473237, -0.48361212,\n",
       "          0.00905676, -0.0880992 ,  0.19314787,  1.4386214 , -0.3656257 ,\n",
       "          0.45674595, -0.9828467 , -0.7468222 ,  0.90941405, -0.30251166,\n",
       "         -0.29912958,  0.5270436 , -1.3943654 ,  0.8754322 ,  1.3925024 ,\n",
       "         -0.74686444,  0.9560073 , -0.8097373 ,  0.30586138, -0.45476982,\n",
       "          0.54833496, -0.7683649 , -0.90036714, -0.9481706 , -1.444934  ,\n",
       "         -1.4442264 , -0.5767632 ,  0.9032551 , -1.2494087 , -1.1660552 ,\n",
       "          0.43199456,  0.800689  , -1.1695781 , -1.270339  ,  0.5675729 ,\n",
       "          0.60414696,  0.86046356,  0.67152095, -0.7893245 ,  0.17730229,\n",
       "          0.8703422 ,  0.17720252, -0.6922777 , -1.0273672 ,  1.7171588 ,\n",
       "         -1.6024047 , -0.56945026,  0.47085908,  0.5999223 , -0.6130383 ,\n",
       "         -0.39092606,  0.53209907,  0.5500211 , -1.3487468 , -0.63748586,\n",
       "         -1.0583477 , -1.2949276 , -0.24264108,  0.07687531, -0.83868873,\n",
       "          0.35647714, -0.02222841,  1.3060693 ,  1.0444076 , -0.41950116,\n",
       "          1.2952095 , -0.7504832 , -0.23989595,  0.6436033 , -0.25507247,\n",
       "          1.7593372 ,  1.7669122 ,  0.5336706 , -0.7434506 ,  0.32397994,\n",
       "         -1.0240334 ,  0.331074  ,  0.0623074 , -0.10455163,  1.0349092 ],\n",
       "        dtype=float32),\n",
       " memmap([-0.09138161, -1.319355  ,  0.5385742 ,  1.0082095 , -0.05608816,\n",
       "         -0.37482765,  0.78907263, -0.77586275,  0.5711703 , -1.4694979 ,\n",
       "          0.31968242, -0.81846315,  0.86286646, -0.8779677 , -0.5085559 ,\n",
       "         -0.19047585,  0.5524385 , -0.9898212 ,  1.0309675 , -0.46021318,\n",
       "          0.1766261 , -0.6795144 , -0.23885463,  0.8506999 , -0.819984  ,\n",
       "          0.35345003, -0.8785692 ,  0.50234026,  1.2287613 , -0.3256239 ,\n",
       "         -1.1032274 , -0.41135022, -0.08732568,  0.5406481 ,  1.202143  ,\n",
       "         -0.5654105 ,  0.17760815, -1.4304559 , -0.8906757 ,  0.4613867 ,\n",
       "          0.7331401 , -1.2374282 , -0.51983374,  0.12507585, -0.49149817,\n",
       "         -0.66854453, -0.6694361 ,  1.1480503 , -1.4170173 , -1.4045323 ,\n",
       "          0.29872617,  1.1781284 , -0.65768105, -0.7440632 ,  0.0982742 ,\n",
       "          1.0662471 ,  0.41650516, -0.45985383, -0.6978111 , -0.55785406,\n",
       "          0.2848098 ,  1.1152122 , -0.16220658, -0.28236476,  2.0570185 ,\n",
       "         -1.7350963 , -0.2711392 ,  1.0856675 , -0.02326099, -0.4014558 ,\n",
       "          0.34600842,  0.82853585, -0.00341254, -0.08148903,  0.512778  ,\n",
       "         -0.717427  , -0.13336453, -0.9348326 ,  0.72312015,  0.06145532,\n",
       "          0.96181375, -0.94073737,  0.66842884, -0.5064411 , -0.3832849 ,\n",
       "          1.2360903 , -0.44427046,  0.52455115,  1.1370583 , -0.27960733,\n",
       "          1.1402147 ,  1.7306937 , -0.60538363,  0.08502948,  0.2668583 ,\n",
       "         -0.62767625, -0.0710615 ,  0.82854503, -0.6860935 ,  0.2445395 ],\n",
       "        dtype=float32),\n",
       " memmap([-3.1661740e-01,  5.5169898e-01,  2.6657900e-01,  9.1765869e-01,\n",
       "         -5.8748955e-01, -1.3671771e+00,  1.3194578e+00, -4.5333409e-01,\n",
       "         -2.0446265e-01,  2.5093892e-01,  7.5015074e-01, -1.4360473e+00,\n",
       "          7.0488960e-02, -1.1504756e+00, -1.2168479e+00, -1.7920537e-02,\n",
       "          5.7550126e-01, -8.8093346e-01,  4.4998246e-01,  2.2134204e-01,\n",
       "          1.1175730e+00,  7.4970752e-02,  4.0599066e-01,  8.3198768e-01,\n",
       "         -5.2381164e-01, -3.3981115e-01,  2.5637853e-01, -3.4112585e-01,\n",
       "          1.0946499e+00,  6.8430394e-01, -7.7141684e-01,  5.5745807e-02,\n",
       "         -4.2628819e-01,  3.1421927e-01,  2.2361742e-01, -4.7266266e-01,\n",
       "         -7.9636164e-02, -1.6954868e-01, -6.2024686e-02, -1.3607314e+00,\n",
       "         -7.1969950e-01, -7.5488210e-01, -1.1690723e-03, -7.9035662e-02,\n",
       "         -2.8942194e+00, -1.8062453e+00,  1.1327633e-01,  1.0650195e+00,\n",
       "         -1.3557250e+00, -7.3976123e-01,  7.6489872e-01,  7.1766809e-02,\n",
       "         -4.7446424e-01, -1.5127689e+00,  1.4229747e+00, -6.6698545e-01,\n",
       "         -3.4234339e-01,  5.2510291e-01, -2.7924359e-01,  4.2949784e-01,\n",
       "          9.4981736e-01,  3.0571136e-01, -7.8635216e-01, -9.9164331e-01,\n",
       "          1.1202308e+00, -1.6745362e+00, -1.1017189e+00, -3.1498089e-01,\n",
       "         -8.0443364e-01,  4.4779703e-01, -2.7940872e-01,  1.6023358e-02,\n",
       "          4.0107527e-01, -8.0248076e-01, -5.9418231e-01, -9.6612525e-01,\n",
       "         -5.7819921e-01, -6.4878392e-01, -6.1432076e-01, -4.1599727e-01,\n",
       "         -2.9323804e-01, -3.8905621e-01,  1.7845926e-01,  4.5211676e-01,\n",
       "         -2.3181939e-01, -2.7307356e-02,  1.4407519e-01, -1.8267727e-01,\n",
       "          5.0851804e-01,  1.9370039e-01,  1.1010702e+00,  6.1197340e-01,\n",
       "          1.0163975e-01, -1.5069360e-01,  9.6838474e-01, -7.2610274e-02,\n",
       "          1.5622988e-01,  1.7243738e-01, -4.7505778e-01,  3.0435821e-01],\n",
       "        dtype=float32),\n",
       " memmap([ 0.11820883, -0.65421253,  0.30629024, -0.06262122, -0.11108041,\n",
       "         -0.8265929 ,  0.868199  , -0.3486849 , -0.56474   , -0.31055424,\n",
       "          0.17285667, -0.8777469 ,  0.690635  ,  0.35963413, -0.24158823,\n",
       "          0.1148226 ,  0.7014736 , -0.14757536,  0.8410807 , -0.4812745 ,\n",
       "         -0.50931096, -1.2314316 , -0.04902185,  1.154138  , -1.4924021 ,\n",
       "          0.08218435, -0.48480752, -0.11909224,  0.76382905, -0.12682031,\n",
       "         -0.84011674, -0.08387095, -0.4084884 ,  1.132811  ,  1.8055055 ,\n",
       "          0.02154944, -0.7237318 , -0.65670687,  0.26489624,  1.0299158 ,\n",
       "          0.9025487 ,  0.2268227 ,  0.26317748,  0.4421138 , -0.9570721 ,\n",
       "         -1.2159553 , -0.5026656 ,  0.50552315, -0.45796677,  0.14504585,\n",
       "          0.539937  ,  1.2156525 , -0.6221837 , -0.35680878,  1.4603213 ,\n",
       "          0.17200089,  0.500556  , -1.49469   , -0.50575507,  0.59958494,\n",
       "          0.19342169,  0.06789967,  0.3918265 ,  0.15948318,  2.0844026 ,\n",
       "         -0.38805097, -1.0807841 ,  0.30639908, -0.46048722, -0.5906864 ,\n",
       "          0.44623533,  1.5414749 , -0.0454932 ,  0.6174166 ,  1.0449845 ,\n",
       "         -0.28111917, -0.34805566, -0.10819758,  0.6334711 ,  0.15836032,\n",
       "          0.12280069, -1.4085735 ,  0.88176167, -0.13740821,  0.18956591,\n",
       "         -0.08020102,  0.04794646, -1.0661993 ,  1.6758004 , -0.18570934,\n",
       "          0.82196033,  0.15420021, -0.4343446 ,  0.49588677,  0.43304434,\n",
       "         -0.088819  , -0.12810358,  1.1088201 , -0.24590597, -0.18438102],\n",
       "        dtype=float32),\n",
       " memmap([ 0.23440616, -0.21726683,  0.7776984 , -0.02454381, -0.2920685 ,\n",
       "         -0.32019207,  1.2239375 , -0.7506277 ,  1.2461907 , -1.0412436 ,\n",
       "          0.31216547, -1.1201718 ,  1.7968149 , -0.7508304 , -0.37676618,\n",
       "         -0.1068878 ,  0.9094093 , -1.2124561 ,  1.4049157 , -1.0739831 ,\n",
       "         -0.00470636, -1.0467138 , -0.44911322,  0.7325196 , -1.0065839 ,\n",
       "          0.633494  , -1.5822374 , -0.01519139,  1.4173305 , -0.6230875 ,\n",
       "         -1.4946345 , -0.78072715, -0.5466961 ,  0.8223306 ,  0.24849534,\n",
       "         -0.63535327,  0.06235089,  0.06951095, -0.692532  ,  0.60140276,\n",
       "          0.6338228 , -1.4606888 ,  0.0568345 ,  0.68689823, -0.85828227,\n",
       "         -0.73225945,  0.42411575,  0.87570727, -1.6512158 , -0.6101868 ,\n",
       "          0.39894345,  1.7081993 , -1.3585122 , -0.42412907,  1.1024823 ,\n",
       "          0.4360147 ,  0.8186481 , -0.46406287,  0.01548181,  1.3104005 ,\n",
       "          0.05874015,  1.0538849 , -0.4909232 , -1.0009881 ,  2.1141973 ,\n",
       "         -1.6028996 , -0.96254414,  0.8574959 , -0.2882734 , -0.04749738,\n",
       "          0.43001798,  1.3973469 ,  0.6228096 ,  0.3461934 ,  0.37026682,\n",
       "         -0.6882782 ,  0.11172792, -0.7030948 ,  0.6446736 , -0.43595135,\n",
       "          1.7380942 , -0.7767595 ,  0.07760947, -0.2737619 ,  0.145203  ,\n",
       "          0.9712227 , -0.589624  ,  0.61328894,  0.7265253 , -0.12694208,\n",
       "          0.9448361 ,  1.7982811 , -0.06387769,  0.01689452,  0.1805695 ,\n",
       "         -1.027861  ,  0.4350159 ,  0.7816336 , -0.85583013,  0.6384769 ],\n",
       "        dtype=float32),\n",
       " memmap([-8.97943616e-01, -1.05764866e+00,  9.66371000e-02,\n",
       "         -3.36344719e-01, -1.15692091e+00, -1.54849064e+00,\n",
       "         -1.63625419e-01, -6.57790005e-01,  8.14485967e-01,\n",
       "         -1.25473034e+00,  1.96269870e-01, -4.87674505e-01,\n",
       "          1.22945476e-02, -1.45316684e+00,  2.11888716e-01,\n",
       "         -1.23710239e+00,  4.19404268e-01, -1.48520589e+00,\n",
       "          1.11501122e+00, -4.13505763e-01,  8.93907249e-01,\n",
       "          2.42446944e-01, -7.03199953e-02, -8.25092077e-01,\n",
       "          1.62616715e-01,  8.10366869e-01, -1.07045734e+00,\n",
       "         -7.46286631e-01,  6.71424329e-01,  9.44951475e-01,\n",
       "          5.25167882e-02, -2.45317221e+00,  2.57100493e-01,\n",
       "         -5.35669565e-01,  1.37173963e+00, -5.65748036e-01,\n",
       "         -1.31225073e+00,  4.64864671e-01,  5.70748627e-01,\n",
       "         -8.06716740e-01,  6.30638540e-01,  1.98290512e-01,\n",
       "         -1.02703226e+00,  6.80866420e-01, -1.18119967e+00,\n",
       "          6.07492864e-01, -8.46041203e-01,  8.73097599e-01,\n",
       "         -3.65339428e-01, -5.49610138e-01, -4.60332245e-01,\n",
       "          8.44629765e-01, -1.01531729e-01, -1.01348245e+00,\n",
       "          1.22312295e+00, -5.16762257e-01,  1.94400594e-01,\n",
       "          3.98477316e-01,  2.17011124e-01, -1.60654813e-01,\n",
       "         -3.60758185e-01,  2.98666656e-01,  1.32509647e-03,\n",
       "         -4.25157815e-01,  1.47917187e+00, -4.01859730e-01,\n",
       "         -9.42175210e-01, -1.20314038e+00, -1.90815997e+00,\n",
       "         -7.07352042e-01, -5.05477548e-01,  1.01368964e+00,\n",
       "          2.48701423e-01,  8.28297734e-02,  2.66935434e-02,\n",
       "         -1.26912546e+00,  5.81633270e-01,  1.13496654e-01,\n",
       "         -7.74907112e-01, -5.89542747e-01,  1.07480097e+00,\n",
       "         -4.78180826e-01,  7.71526694e-01, -1.30941540e-01,\n",
       "         -8.91620278e-01, -6.90568447e-01, -7.11626768e-01,\n",
       "         -5.81067204e-01,  2.39266872e-01, -2.99056619e-02,\n",
       "          1.10693955e+00,  1.66799414e+00,  5.56525111e-01,\n",
       "         -2.44169191e-01, -4.89281386e-01, -9.27821159e-01,\n",
       "          8.09028372e-02,  1.69530585e-01,  3.62108380e-01,\n",
       "          1.18090284e+00], dtype=float32),\n",
       " memmap([-0.05256829, -1.1900868 ,  0.8463123 ,  0.55630404, -0.38295048,\n",
       "          0.3030792 ,  1.446561  , -0.874689  ,  0.78379154, -1.3310835 ,\n",
       "          0.21731924, -2.0592465 ,  0.9441372 , -1.4945257 , -0.9692662 ,\n",
       "         -1.2919279 , -0.02450725, -0.7099371 ,  2.3937502 ,  0.38701847,\n",
       "          1.7596933 , -0.71947527, -0.19080961,  0.1459159 , -1.2970747 ,\n",
       "          0.15583915, -1.0436518 , -0.08567408,  1.1485854 ,  0.4977309 ,\n",
       "         -1.5355189 , -0.596609  ,  0.282029  ,  0.4409472 ,  0.9454438 ,\n",
       "          0.16102794, -0.72914153, -1.2756822 , -0.7623536 , -0.1307791 ,\n",
       "         -0.11917009, -0.8245112 ,  0.38697466,  0.44052538, -1.1977652 ,\n",
       "         -1.9938952 ,  0.6527374 ,  0.3876409 , -0.7338232 , -0.5586495 ,\n",
       "         -0.23390971,  1.0797048 , -1.8556522 ,  0.71394384,  0.32402408,\n",
       "          0.51647145,  0.5759301 ,  0.40887228, -0.7323356 , -0.06143119,\n",
       "         -0.5186226 ,  0.45761427,  0.70221514, -0.6665472 ,  2.1231735 ,\n",
       "         -2.2823625 , -1.3009341 ,  0.20341165,  0.09028662,  0.24267596,\n",
       "          1.7720608 ,  0.48496422,  0.9003498 ,  0.6059479 ,  0.83348435,\n",
       "         -1.2312958 , -0.71723735, -1.7046403 , -0.58453953, -0.11703283,\n",
       "          1.7981133 , -0.92645085,  0.61134934, -0.3900195 ,  0.7730233 ,\n",
       "          0.9332946 , -1.0424061 ,  0.6288942 ,  1.270219  , -0.19761904,\n",
       "          0.93196684,  1.7900413 , -0.32015115,  0.16350302, -0.09592598,\n",
       "          0.01603928,  0.2916919 ,  0.79648614, -0.19330513,  0.6040051 ],\n",
       "        dtype=float32)]"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display vector_dict output\n",
    "print(vector_dict.keys())\n",
    "# Preview one candidate vector from a candidate pool vectors\n",
    "vector_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "1 2\n",
      "1 3\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# Calculate congruence metric for each candidate vector for each mention's candidate pool\n",
    "# This notebook uses cosine similarity as the congruence metric\n",
    "\n",
    "## Save congruence measurements in a two-level dictionary\n",
    "# Create first-level dictionary\n",
    "congruence_dict = {}\n",
    "\n",
    "# Always work low numbers to high\n",
    "m = 0\n",
    "while m < len(sentence_mention_nums)-1:\n",
    "    \n",
    "    # Save second-level congruence measurement dictionary\n",
    "    m_dict = {}\n",
    "    # Compare each mention against mentions after it\n",
    "    for n in sentence_mention_nums[m+1:]:\n",
    "        print(m, n)\n",
    "        # Calculate congruence measurement - cosine similarity\n",
    "        congruence_measurement = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "        # Save congruence measurement to second-level dictionary\n",
    "        m_dict[n] = congruence_measurement\n",
    "    \n",
    "    # Save second-level dictionary to first-level\n",
    "    congruence_dict[m] = m_dict\n",
    "    \n",
    "    # Increment mention\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2])\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Display congruence_dict output\n",
    "# This should be one less than congruent mention count, since we are comparing low to high\n",
    "# and thus don't compare the highest value to anything\n",
    "print(congruence_dict.keys())\n",
    "# Preview congruence matrix derived from comparing Mention 1 to Mention 2\n",
    "for k in congruence_dict.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_e6412b0c_2db3_11eb_8a72_a683e72745abrow1_col1{\n",
       "            background:  skyblue;\n",
       "        }</style><table id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745ab\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>        <th class=\"col_heading level0 col8\" >8</th>        <th class=\"col_heading level0 col9\" >9</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745ablevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow0_col0\" class=\"data row0 col0\" >0.562130</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow0_col1\" class=\"data row0 col1\" >0.361488</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow0_col2\" class=\"data row0 col2\" >0.311327</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow0_col3\" class=\"data row0 col3\" >0.384978</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow0_col4\" class=\"data row0 col4\" >0.473528</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow0_col5\" class=\"data row0 col5\" >0.257875</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow0_col6\" class=\"data row0 col6\" >0.463969</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow0_col7\" class=\"data row0 col7\" >0.431226</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow0_col8\" class=\"data row0 col8\" >0.336301</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow0_col9\" class=\"data row0 col9\" >0.330874</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745ablevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow1_col0\" class=\"data row1 col0\" >0.416903</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow1_col1\" class=\"data row1 col1\" >0.694778</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow1_col2\" class=\"data row1 col2\" >0.315274</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow1_col3\" class=\"data row1 col3\" >0.374723</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow1_col4\" class=\"data row1 col4\" >0.403256</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow1_col5\" class=\"data row1 col5\" >0.280973</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow1_col6\" class=\"data row1 col6\" >0.270957</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow1_col7\" class=\"data row1 col7\" >0.438154</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow1_col8\" class=\"data row1 col8\" >0.593326</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow1_col9\" class=\"data row1 col9\" >0.305063</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745ablevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow2_col0\" class=\"data row2 col0\" >0.293179</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow2_col1\" class=\"data row2 col1\" >0.241113</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow2_col2\" class=\"data row2 col2\" >0.292719</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow2_col3\" class=\"data row2 col3\" >0.221636</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow2_col4\" class=\"data row2 col4\" >0.208077</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow2_col5\" class=\"data row2 col5\" >0.340750</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow2_col6\" class=\"data row2 col6\" >0.213199</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow2_col7\" class=\"data row2 col7\" >0.320365</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow2_col8\" class=\"data row2 col8\" >0.130415</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow2_col9\" class=\"data row2 col9\" >0.126078</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745ablevel0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow3_col0\" class=\"data row3 col0\" >0.232744</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow3_col1\" class=\"data row3 col1\" >0.255579</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow3_col2\" class=\"data row3 col2\" >0.175357</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow3_col3\" class=\"data row3 col3\" >0.203990</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow3_col4\" class=\"data row3 col4\" >0.230758</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow3_col5\" class=\"data row3 col5\" >0.282234</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow3_col6\" class=\"data row3 col6\" >0.272506</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow3_col7\" class=\"data row3 col7\" >0.231788</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow3_col8\" class=\"data row3 col8\" >0.169106</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow3_col9\" class=\"data row3 col9\" >0.170027</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745ablevel0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow4_col0\" class=\"data row4 col0\" >0.282068</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow4_col1\" class=\"data row4 col1\" >0.186678</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow4_col2\" class=\"data row4 col2\" >0.278595</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow4_col3\" class=\"data row4 col3\" >0.168653</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow4_col4\" class=\"data row4 col4\" >0.126405</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow4_col5\" class=\"data row4 col5\" >0.268739</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow4_col6\" class=\"data row4 col6\" >0.168239</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow4_col7\" class=\"data row4 col7\" >0.222682</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow4_col8\" class=\"data row4 col8\" >0.133077</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow4_col9\" class=\"data row4 col9\" >0.100177</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745ablevel0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow5_col0\" class=\"data row5 col0\" >0.474558</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow5_col1\" class=\"data row5 col1\" >0.397564</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow5_col2\" class=\"data row5 col2\" >0.310787</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow5_col3\" class=\"data row5 col3\" >0.417405</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow5_col4\" class=\"data row5 col4\" >0.575604</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow5_col5\" class=\"data row5 col5\" >0.339006</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow5_col6\" class=\"data row5 col6\" >0.484861</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow5_col7\" class=\"data row5 col7\" >0.562239</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow5_col8\" class=\"data row5 col8\" >0.361319</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow5_col9\" class=\"data row5 col9\" >0.468122</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745ablevel0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow6_col0\" class=\"data row6 col0\" >0.230455</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow6_col1\" class=\"data row6 col1\" >0.225503</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow6_col2\" class=\"data row6 col2\" >0.251244</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow6_col3\" class=\"data row6 col3\" >0.224700</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow6_col4\" class=\"data row6 col4\" >0.197092</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow6_col5\" class=\"data row6 col5\" >0.283705</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow6_col6\" class=\"data row6 col6\" >0.269798</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow6_col7\" class=\"data row6 col7\" >0.233957</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow6_col8\" class=\"data row6 col8\" >0.185102</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow6_col9\" class=\"data row6 col9\" >0.120745</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745ablevel0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow7_col0\" class=\"data row7 col0\" >0.359480</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow7_col1\" class=\"data row7 col1\" >0.537696</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow7_col2\" class=\"data row7 col2\" >0.290624</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow7_col3\" class=\"data row7 col3\" >0.274178</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow7_col4\" class=\"data row7 col4\" >0.386575</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow7_col5\" class=\"data row7 col5\" >0.268141</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow7_col6\" class=\"data row7 col6\" >0.255528</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow7_col7\" class=\"data row7 col7\" >0.354225</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow7_col8\" class=\"data row7 col8\" >0.483556</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow7_col9\" class=\"data row7 col9\" >0.364825</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745ablevel0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow8_col0\" class=\"data row8 col0\" >0.342776</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow8_col1\" class=\"data row8 col1\" >0.551446</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow8_col2\" class=\"data row8 col2\" >0.274877</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow8_col3\" class=\"data row8 col3\" >0.315835</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow8_col4\" class=\"data row8 col4\" >0.360571</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow8_col5\" class=\"data row8 col5\" >0.232937</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow8_col6\" class=\"data row8 col6\" >0.262038</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow8_col7\" class=\"data row8 col7\" >0.370209</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow8_col8\" class=\"data row8 col8\" >0.574802</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow8_col9\" class=\"data row8 col9\" >0.303808</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745ablevel0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow9_col0\" class=\"data row9 col0\" >0.000000</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow9_col1\" class=\"data row9 col1\" >0.000000</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow9_col2\" class=\"data row9 col2\" >0.000000</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow9_col3\" class=\"data row9 col3\" >0.000000</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow9_col4\" class=\"data row9 col4\" >0.000000</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow9_col5\" class=\"data row9 col5\" >0.000000</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow9_col6\" class=\"data row9 col6\" >0.000000</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow9_col7\" class=\"data row9 col7\" >0.000000</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow9_col8\" class=\"data row9 col8\" >0.000000</td>\n",
       "                        <td id=\"T_e6412b0c_2db3_11eb_8a72_a683e72745abrow9_col9\" class=\"data row9 col9\" >0.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9a4d697a50>"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congruence_test_df = pd.DataFrame(congruence_dict[1][2])\n",
    "max_num = max(np.max(congruence_test_df))\n",
    "congruence_test_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  4\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "Length:  4\n",
      "1 2\n",
      "1 3\n",
      "Length:  4\n",
      "2 3\n",
      "Length:  4\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of logic to ensure unique mention congruence only calculated once\n",
    "m = 0\n",
    "while m < len(sentence_mention_nums):\n",
    "    print(\"Length: \", len(sentence_mention_nums))\n",
    "    for i in sentence_mention_nums[m+1:]:\n",
    "        print(m, i)\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 1), 0.8189154129945448)\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 2), 0.6412995855581739)\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((1, 1), 0.6947784687002414)\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((1, 1), 0.6947784687002414), 1, 2)\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "most_congruent_pair = (None, 0.0, 0, 0) # Most Congruent Candidates, Congruence Metric, Mention A, Mention B\n",
    "\n",
    "for m in sentence_mention_nums:\n",
    "    for n in sentence_mention_nums[m+1:]:\n",
    "        print(f\"Comparing {m} & {n}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m][n])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, m, n\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1}"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save max congruent pair estimates for each mention\n",
    "mention_predictions = {}\n",
    "mention_predictions[most_congruent_pair[1]] = most_congruent_pair[0][0][0]\n",
    "mention_predictions[most_congruent_pair[2]] = most_congruent_pair[0][0][1]\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have predictions for  dict_keys([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# With two mentions set in their predictions, we must filter the other congruent matrices to find the next most\n",
    "print(\"We have predictions for \", mention_predictions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3]"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(sentence_mention_nums) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "MAX: (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "MAX: (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 1), 0.8189154129945448)\n",
      "MAX: (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "MAX: (((1, 1), 0.8189154129945448), (0, 2))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((1, 1), 0.8189154129945448), (0, 2))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1, 0: 1}"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "MAX: (((None, None), 0.0), (0, 0))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "MAX: (((0, 4), 0.6645409154870272), (1, 3))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 2), 0.6412995855581739)\n",
      "MAX: (((0, 6), 0.78401095), (2, 3))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((0, 6), 0.78401095), (2, 3))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1, 0: 1, 3: 6}"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've predicted everything!\n"
     ]
    }
   ],
   "source": [
    "if len(mentions_remaining) == 0:\n",
    "    print(\"You've predicted everything!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
