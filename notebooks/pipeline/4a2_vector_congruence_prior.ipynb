{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congruence via Entity Vector Similarity\n",
    "\n",
    "In this notebook, we calculate the cosine similarity measure between vectors of each entity candidate. The vectors will be retrieved from Wikipedia2vec's pre-trained API, which creates vectors for the entire Wikipedia page. Comparing two vectors in this way thus lets us make a statement about similar pages and update our likelihood scores based on that.\n",
    "\n",
    "#### (i) Via Weights\n",
    "\n",
    "In Phase 4, we calculate congruence between candidates in pools for mentions in the same sentence. We then update the likelihood values from Phase 3 using congruence as a form of \"weights\". This should adjust our pool without eliminating the prior knowledge we have of the calculated similarity. In this notebook, we import prior knowledge generated from Anchor Link statistics and weight those with our numerical calculations from congruence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 21347120, 9477, 1882861, 3261189,...</td>\n",
       "      <td>[458, 46, 211593, 1396, 363404, 3327447, 40537...</td>\n",
       "      <td>['European_Union', 'Europe', 'Eu,_Seine-Mariti...</td>\n",
       "      <td>[0.9227799, 0.024651, 0.020196, 0.005346, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 11884, 152735, 21212, 12674, 290327, 1...</td>\n",
       "      <td>[183, 188, 42884, 7318, 43287, 141817, 181287,...</td>\n",
       "      <td>['Germany', 'German_language', 'Germans', 'Naz...</td>\n",
       "      <td>[0.4191423, 0.2892399, 0.14703, 0.0382718, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>british</td>\n",
       "      <td>[31717, 19097669, 13530298, 4721, 158019, 1522...</td>\n",
       "      <td>[145, 842438, 23666, 8680, 161885, 174193, 354...</td>\n",
       "      <td>['United_Kingdom', 'British_people', 'Great_Br...</td>\n",
       "      <td>[0.610078, 0.1146438, 0.0681775, 0.0366451, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.5, 0.3, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.5, 0.3, 0.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention     full_mention                                wikipedia_URL  \\\n",
       "0       B               EU                                          NaN   \n",
       "1       B           German         http://en.wikipedia.org/wiki/Germany   \n",
       "2       B          British  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "3       B  Peter Blackburn                                          NaN   \n",
       "4       I  Peter Blackburn                                          NaN   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0                NaN             NaN            0       0   \n",
       "1            11867.0         Germany            0       0   \n",
       "2            31717.0  United Kingdom            0       0   \n",
       "3                NaN             NaN            1       0   \n",
       "4                NaN             NaN            1       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0                        ['EU', 'German', 'British']                eu   \n",
       "1                        ['EU', 'German', 'British']            german   \n",
       "2                        ['EU', 'German', 'British']           british   \n",
       "3  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "4  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [9317, 9239, 21347120, 9477, 1882861, 3261189,...   \n",
       "1  [11867, 11884, 152735, 21212, 12674, 290327, 1...   \n",
       "2  [31717, 19097669, 13530298, 4721, 158019, 1522...   \n",
       "3                      [56783206, 9643132, 56873217]   \n",
       "4                      [56783206, 9643132, 56873217]   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [458, 46, 211593, 1396, 363404, 3327447, 40537...   \n",
       "1  [183, 188, 42884, 7318, 43287, 141817, 181287,...   \n",
       "2  [145, 842438, 23666, 8680, 161885, 174193, 354...   \n",
       "3                       [2073954, 7172840, 26634508]   \n",
       "4                       [2073954, 7172840, 26634508]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  ['European_Union', 'Europe', 'Eu,_Seine-Mariti...   \n",
       "1  ['Germany', 'German_language', 'Germans', 'Naz...   \n",
       "2  ['United_Kingdom', 'British_people', 'Great_Br...   \n",
       "3  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "4  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.9227799, 0.024651, 0.020196, 0.005346, 0.00...  \n",
       "1  [0.4191423, 0.2892399, 0.14703, 0.0382718, 0.0...  \n",
       "2  [0.610078, 0.1146438, 0.0681775, 0.0366451, 0....  \n",
       "3                                    [0.5, 0.3, 0.2]  \n",
       "4                                    [0.5, 0.3, 0.2]  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base path to input\n",
    "preds_path = '../../predictions/'\n",
    "\n",
    "# Load data\n",
    "predictions = pd.read_csv(os.path.join(preds_path, \"anchortext_frequency.csv\"), delimiter=\",\")\n",
    "predictions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Saved Candidate Pool\n",
    "\n",
    "Candidate pools when exported are typically stored as the string of a list. The below function parses the string back into a list with proper formatted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate that list is string\n",
    "type(predictions['candidate_pool_page_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse list as string\n",
    "def parse_list_string(list_string, value_type=int):\n",
    "    \n",
    "    parsed_list = []\n",
    "    \n",
    "    # If candidate pool is empty\n",
    "    if list_string == \"[]\" or isinstance(list_string, float):\n",
    "        pass\n",
    "    # Else parse\n",
    "    else:\n",
    "        # Parses lists of titles as strings\n",
    "        if value_type==str:\n",
    "            # Eliminate bracket and parenthesis on either side, split by comma pattern\n",
    "            parsed_list = re.split(\"', '|\\\", \\\"|', \\\"|\\\", \\'\", list_string[2:-2])\n",
    "\n",
    "        # Parses lists of IDs as ints\n",
    "        elif value_type==int:\n",
    "            # Eliminate brackets and convert each number from string to int\n",
    "            parsed_list = list(map(int, list_string[1:-1].split(', ')))\n",
    "        elif value_type==float:\n",
    "            # Eliminate brackets and convert each number from string to int\n",
    "            parsed_list = list(map(float, list_string[1:-1].split(', ')))\n",
    "            \n",
    "        \n",
    "    return parsed_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['European_Union',\n",
       " 'Europe',\n",
       " 'Eu,_Seine-Maritime',\n",
       " 'Europium',\n",
       " 'Citizenship_of_the_European_Union',\n",
       " 'United_Left_(Galicia)',\n",
       " 'EU_(group)',\n",
       " 'European_Union_law',\n",
       " 'Eu_station',\n",
       " 'Entropy']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "# 0 is the hard one. See how some value is stored with '' and some with \"\". Unsure why.\n",
    "parse_list_string(predictions['candidate_pool_titles'][0], value_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9317,\n",
       " 9239,\n",
       " 21347120,\n",
       " 9477,\n",
       " 1882861,\n",
       " 3261189,\n",
       " 14024977,\n",
       " 276436,\n",
       " 27532324,\n",
       " 9891]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "# 0 is the hard one. See how some value is stored with '' and some with \"\". Unsure why.\n",
    "parse_list_string(predictions['candidate_pool_page_ids'][0], value_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "parse_list_string(predictions['candidate_pool_page_ids'][13], value_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9227799,\n",
       " 0.024651,\n",
       " 0.020196,\n",
       " 0.005346,\n",
       " 0.002079,\n",
       " 0.001782,\n",
       " 0.001485,\n",
       " 0.001188,\n",
       " 0.001188,\n",
       " 0.000891]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "parse_list_string(predictions['candidate_pool_likelihoods'][0], value_type=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "After ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "Before [56783206, 9643132, 56873217]\n",
      "After [56783206, 9643132, 56873217]\n",
      "Before [2073954, 7172840, 26634508]\n",
      "After [2073954, 7172840, 26634508]\n",
      "Before ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(bishop)', 'Peter_Blackburn_(MP)']\n",
      "After ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(bishop)', 'Peter_Blackburn_(MP)']\n",
      "Before [0.5, 0.3, 0.2]\n",
      "After [0.5, 0.3, 0.2]\n",
      "CPU times: user 726 ms, sys: 270 ms, total: 996 ms\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Apply defined function to entire dataframe for all candidate pool columns\n",
    "\n",
    "column = 'congruent_mentions'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=str)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "column = 'candidate_pool_page_ids'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=int)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_item_ids'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=int)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_titles'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=str)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "column = 'candidate_pool_likelihoods'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=float)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Entity Vectors from Wikipedia2Vec\n",
    "\n",
    "For provided wikipedia pages, we retrieve a representative entity vector from Wikipedia2vec. This involves passing the normalized title into their get_entity_vector() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package\n",
    "from wikipedia2vec import Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 111 ms, sys: 145 ms, total: 256 ms\n",
      "Wall time: 333 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load unzipped pkl file with word embeddings\n",
    "w2v = Wikipedia2Vec.load(\"../../embeddings/enwiki_20180420_100d.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Coverage of Candidate Pools in Wikipedia2vec\n",
    "\n",
    "We need to measure what percent of candidates in our candidate pools successfully return a vector from Wikipedia2vec. This should conceivably be 100% given we're passing known Wikipedia pages into this package trained over Wikipedia pages, but there may be some drop-off due to different creation dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text normalization function\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    We define normalized as:\n",
    "    - strip whitespace\n",
    "    - Spaces, not underlines\n",
    "    \"\"\"\n",
    "    return str(text).strip().replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29312/29312 [00:02<00:00, 10249.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia2vec returned an entity vector for 93.614% of 155,553 searches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over candidate pool titles to see what can be returned\n",
    "\n",
    "found_entity = 0\n",
    "searched_entity = 0\n",
    "\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    \n",
    "    # Retrieve candidate pool\n",
    "    candidate_pool = predictions['candidate_pool_titles'][i]\n",
    "    \n",
    "    # Query for each candidate\n",
    "    for candidate in candidate_pool:\n",
    "        # Normalize candidate title to form necessary to input into Wikipedia2vec\n",
    "        candidate = normalize_text(candidate)\n",
    "        \n",
    "        # Query Wikipedia2vec get_entity_vector()\n",
    "        try:\n",
    "            entity_vector = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            entity_vector = None\n",
    "        \n",
    "        # Check if result\n",
    "        if entity_vector is not None:\n",
    "            found_entity += 1\n",
    "        \n",
    "        # Increment count\n",
    "        searched_entity += 1\n",
    "\n",
    "print(f\"Wikipedia2vec returned an entity vector for {round(found_entity/searched_entity*100,3)}% of {searched_entity:,} searches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Congruence Metric between Congruent Entities\n",
    "\n",
    "First, let's get a sense for what the upper bound of congruent calculations might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the maximum number of congruent entities in a single sentence\n",
    "max(predictions['congruent_mentions'].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What is the distribution of congruent mention counts\n",
    "plt.figure(figsize=(20,5))\n",
    "predictions['congruent_mentions'].apply(len).hist(bins=50)\n",
    "plt.title(\"distribution of congruent mention counts\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Current Design Thinking\n",
    "\n",
    "In order to allow this to become recursive for N many tables, you will need to capture a congruence table for every candidate pool to every other candidate pool in a two-level dictionary so you can retrieve values using `matrix[3][1]`. This would entail duplication except to save that, we sort by value so you always search [small][large]. Saves us computation and storage. To see a sequential example of our logic, scroll to the end of this notebook. Below is our function-based implementation of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Iran</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iran</td>\n",
       "      <td>14653.0</td>\n",
       "      <td>Iran</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iran</td>\n",
       "      <td>[14653, 272865, 338883, 8294810, 46823116, 126...</td>\n",
       "      <td>[794, 184602, 207991, 1465546, 932162, 1042614...</td>\n",
       "      <td>[Iran, Iran_national_football_team, Pahlavi_dy...</td>\n",
       "      <td>[0.9790373, 0.0079465, 0.0016392, 0.0005319, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>turkey</td>\n",
       "      <td>[11125639, 743577, 72821, 24513964, 297071, 22...</td>\n",
       "      <td>[43, 483856, 43794, 4200953, 26844, 12560, 848...</td>\n",
       "      <td>[Turkey, Turkey_national_football_team, Turkey...</td>\n",
       "      <td>[0.9168911, 0.0269033, 0.0082007, 0.0038923, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iraq</td>\n",
       "      <td>7515928.0</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iraq</td>\n",
       "      <td>[7515928, 1039652, 5043324, 26215470, 2900620,...</td>\n",
       "      <td>[796, 186243, 545449, 3108185, 149805, 107802,...</td>\n",
       "      <td>[Iraq, Iraq_national_football_team, Iraq_War, ...</td>\n",
       "      <td>[0.8794007, 0.0341074, 0.0292135, 0.0119351, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Kurdish</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kurdish_people</td>\n",
       "      <td>17068.0</td>\n",
       "      <td>Kurdish people</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>[17068, 40316, 80777, 3821855, 4314285, 354232...</td>\n",
       "      <td>[12223, 36368, 41470, 1792998, 1117020, 121801...</td>\n",
       "      <td>[Kurds, Kurdish_languages, Kurdistan, Kurds_in...</td>\n",
       "      <td>[0.565625, 0.3114583, 0.0159722, 0.0142361, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention full_mention                                wikipedia_URL  \\\n",
       "0       B         Iran            http://en.wikipedia.org/wiki/Iran   \n",
       "1       B       Turkey                                          NaN   \n",
       "2       B         Iraq            http://en.wikipedia.org/wiki/Iraq   \n",
       "3       B      Kurdish  http://en.wikipedia.org/wiki/Kurdish_people   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            14653.0            Iran           90      11   \n",
       "1                NaN             NaN           90      11   \n",
       "2          7515928.0            Iraq           90      11   \n",
       "3            17068.0  Kurdish people           90      11   \n",
       "\n",
       "              congruent_mentions norm_full_mention  \\\n",
       "0  [Iran, Turkey, Iraq, Kurdish]              iran   \n",
       "1  [Iran, Turkey, Iraq, Kurdish]            turkey   \n",
       "2  [Iran, Turkey, Iraq, Kurdish]              iraq   \n",
       "3  [Iran, Turkey, Iraq, Kurdish]           kurdish   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [14653, 272865, 338883, 8294810, 46823116, 126...   \n",
       "1  [11125639, 743577, 72821, 24513964, 297071, 22...   \n",
       "2  [7515928, 1039652, 5043324, 26215470, 2900620,...   \n",
       "3  [17068, 40316, 80777, 3821855, 4314285, 354232...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [794, 184602, 207991, 1465546, 932162, 1042614...   \n",
       "1  [43, 483856, 43794, 4200953, 26844, 12560, 848...   \n",
       "2  [796, 186243, 545449, 3108185, 149805, 107802,...   \n",
       "3  [12223, 36368, 41470, 1792998, 1117020, 121801...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [Iran, Iran_national_football_team, Pahlavi_dy...   \n",
       "1  [Turkey, Turkey_national_football_team, Turkey...   \n",
       "2  [Iraq, Iraq_national_football_team, Iraq_War, ...   \n",
       "3  [Kurds, Kurdish_languages, Kurdistan, Kurds_in...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.9790373, 0.0079465, 0.0016392, 0.0005319, 0...  \n",
       "1  [0.9168911, 0.0269033, 0.0082007, 0.0038923, 0...  \n",
       "2  [0.8794007, 0.0341074, 0.0292135, 0.0119351, 0...  \n",
       "3  [0.565625, 0.3114583, 0.0159722, 0.0142361, 0....  "
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define testing sentence_id\n",
    "sentence_id = 90\n",
    "sentence_predictions = predictions[predictions['sentence_id'] == sentence_id].reset_index(drop=True)\n",
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iran', 'Turkey', 'Iraq', 'Kurdish']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_predictions['congruent_mentions'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to Create Modular Congruent Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate lists of vectors\n",
    "def get_candidate_pool_vectors(candidate_pool_titles, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return entity vectors from Wikipedia2Vec\n",
    "    Takes as input a list of page titles, representing the candidate pool\n",
    "    Normalizes each page title to match necessary input format\n",
    "    Returns entity vector or empty vector if no match\n",
    "    \"\"\"\n",
    "    # Track failed vector queries\n",
    "    no_vector_count = 0\n",
    "    candidate_pool_vectors = []\n",
    "    for candidate in candidate_pool_titles:\n",
    "        candidate = normalize_text(candidate)\n",
    "        if verbose: print(candidate)\n",
    "        try:\n",
    "            candidate_vectors = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            # Keep empty vector representation to maintain index locations\n",
    "            candidate_vectors = np.zeros(100)\n",
    "            no_vector_count += 1\n",
    "        candidate_pool_vectors.append(candidate_vectors)\n",
    "    \n",
    "    # Handle case where candidate pool is empty from Phase 3\n",
    "    # Add arbitrarily chosen 3 arrays of zeros\n",
    "    if len(candidate_pool_titles) == 0:\n",
    "        candidate_pool_vectors = [np.zeros(100), np.zeros(100), np.zeros(100)]\n",
    "    \n",
    "    if verbose: print(f\"Failed Wikipedia2Vec Entity Vector Queries: {no_vector_count}\")\n",
    "    return candidate_pool_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to retrieve entity vectors\n",
    "def create_entity_vector_dict(sentence_mention_ids, sentence_predictions, verbose=False):\n",
    "    \"\"\"\n",
    "    Function iterates over a provided list of congruent mentions,\n",
    "    finds the associated candidate pool for each mention\n",
    "    and returns the candidate pool vector\n",
    "    \"\"\"\n",
    "    # Save vectors in dictionary\n",
    "    vector_dict = {}\n",
    "    \n",
    "    # For each full mention we are analyzing in the contextual domain\n",
    "    for m in sentence_mention_ids:\n",
    "        \n",
    "        # Retrieve candidate pool titles\n",
    "        candidate_pool_titles = sentence_predictions['candidate_pool_titles'][m]\n",
    "        if verbose: print(candidate_pool_titles)\n",
    "        \n",
    "        # Convert candidate pool titles to candidate pool vectors\n",
    "        candidate_pool_vectors = get_candidate_pool_vectors(candidate_pool_titles, verbose=verbose)\n",
    "        \n",
    "        # Save candidate pool vectors to dictionary\n",
    "        vector_dict[m] = candidate_pool_vectors\n",
    "    \n",
    "    if verbose:\n",
    "        print(vector_dict.keys())\n",
    "        for k in vector_dict.keys():\n",
    "            print(len(vector_dict[k]))\n",
    "    return vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate congruent metric for each candidate in every mention's candidate pool\n",
    "def get_congruence_dict(vector_dict, sentence_mention_ids, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return congruence metric calculations between all candidates for all mentions\n",
    "    Input:\n",
    "    - vector_dict: todo this could be generalized to allow comparison between text/ints/vectors\n",
    "    - Sentence Mentions Numerical Representation: Integers representing congruent mentions in a context domain\n",
    "    Outputs:\n",
    "    - Dictionary with congruence metric calculations for everyone\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Save congruence metrics in a two-level dictionary\n",
    "    # Create first-level dictionary to be returned\n",
    "    congruence_dict = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level congruence metric dictionary\n",
    "        m_dict = {}\n",
    "\n",
    "        # Compare eaech mention against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            if verbose: print(f\"Comparing mentions {m} & {n}\")\n",
    "            # Calculate congruence metric - cosine similarity\n",
    "            congruence_metric = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "            # Save congruence metric to second-level dictionary\n",
    "            m_dict[n] = congruence_metric\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        congruence_dict[m] = m_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "    \n",
    "#     if verbose: print(congruence_dict)\n",
    "    return congruence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to combine mention probabilities\n",
    "def combine_probabilities(mid_1, mid_2, sentence_probabilities):\n",
    "    \"\"\"\n",
    "    Function takes two mentions (numerically represented),\n",
    "    finds their candidate pool probabilities,\n",
    "    and combines them using the chosen logic,\n",
    "    returning a matrix of combined pair-wise probabilities\n",
    "    \"\"\"\n",
    "    # Prepare matrix to return\n",
    "    weights_matrix = []\n",
    "    \n",
    "    # Combine every candidate's probability for one mention with each for the second\n",
    "    for a in sentence_probabilities[mid_1]:\n",
    "        weights_row = []\n",
    "        for b in sentence_probabilities[mid_2]:\n",
    "            \n",
    "            ## Combination logic\n",
    "            weights_row.append(np.mean([a, b])) # Take mean of two prior probabilities\n",
    "        \n",
    "        weights_matrix.append(weights_row)\n",
    "        \n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create a weighted dictionary from priors\n",
    "def create_weighted_dict(sentence_mention_ids, sentence_predictions, verbose=False):\n",
    "    \"\"\"\n",
    "    Function iterates over a provided list of congruent mentions,\n",
    "    finds the associated candidate pool prior probabilities for each mention\n",
    "    and returns a dictionary for the combined probability of every pair of candidates\n",
    "    \"\"\"\n",
    "    # Save weights in dictionary\n",
    "    weights_dict = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level weights dictionary\n",
    "        w_dict = {}\n",
    "        \n",
    "        # Compare each mention probabilities against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            if verbose: print(f\"Comparing mentions {m} & {n}\")\n",
    "            # Return candidate pool probabilities - todo replace likelihood with probabilities\n",
    "            weights_matrix = combine_probabilities(m, n, sentence_predictions['candidate_pool_likelihoods'])\n",
    "            if not weights_matrix: # Handles error where candidate pool is empty\n",
    "                weights_matrix = [0.0]\n",
    "            # Save weights matrix to second-level dictionary\n",
    "            w_dict[n] = weights_matrix\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        weights_dict[m] = w_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "                \n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to combine congruence and prior weights\n",
    "def combine_congruence_priors(sentence_mention_ids, congruence_dict, weights_dict, verbose=False):\n",
    "    \"\"\"\n",
    "    Function takes two dictionaries representing recursive metric calculations,\n",
    "    Multiplies them element-wise together,\n",
    "    Returns the updated table\n",
    "    \"\"\"\n",
    "   # Combine congruence with prior weights in first-level dictionary\n",
    "    weighted_congruence = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level dictionary\n",
    "        w_c_dict = {}\n",
    "        \n",
    "        # Compare each mention probabilities against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            try:\n",
    "                weighted_table = congruence_dict[m][n] * np.array(weights_dict[m][n])\n",
    "            except ValueError: # Handles error when candidate pool is empty\n",
    "                weighted_table = congruence_dict[m][n]\n",
    "            w_c_dict[n] = weighted_table\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        weighted_congruence[m] = w_c_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "    \n",
    "    return weighted_congruence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standardize on form lookup always row then column\n",
    "def get_most_congruent_pair(congruence_matrix, verbose=False):\n",
    "    \"\"\"\n",
    "    This function takes a congruence matrix and returns the indices\n",
    "    of the two most congruent candidates using your chosen metric.\n",
    "    These indices can be plugged back into the candidate pool lists\n",
    "    to determine which candidates are most similar.\n",
    "    \"\"\"\n",
    "    # Get max values for every row\n",
    "    max_row_values = congruence_matrix.max(axis=1)\n",
    "    max_row_idxs = congruence_matrix.argmax(axis=1)\n",
    "    \n",
    "    # Get overall max value and the row it is in\n",
    "    max_value = max_row_values.max()\n",
    "    max_row_idx = max_row_values.argmax()\n",
    "    \n",
    "    # Get column max value is in\n",
    "    max_column_idx = max_row_idxs[max_row_idx]\n",
    "    \n",
    "    return (max_row_idx, max_column_idx), max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the most congruent pair amongst remaining mentions\n",
    "def find_most_congruent_pair(mention_predictions, mentions_remaining, congruence_dict, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to search the congruence matrices for mentions without predictions\n",
    "    and return the most congruent pair of candidates and associated mentions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with empty most_congruent_pair\n",
    "    # (Candidate Pair, Congruence Metric), (Mention A, Mention B)\n",
    "    most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "    \n",
    "    # Assess whether first pass or recursive pass\n",
    "    if len(mention_predictions) == 0:\n",
    "\n",
    "        # First pass        \n",
    "        for m in mentions_remaining:\n",
    "            for n in mentions_remaining[m+1:]:\n",
    "\n",
    "                # Get most congruent pair in one matrix\n",
    "                congruent_pair = get_most_congruent_pair(congruence_dict[m][n], verbose=verbose)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Comparing {m} & {n}\")\n",
    "                    print(\"Congruent Pair: \", congruent_pair)\n",
    "                    print(\"Current Most Congruent: \", most_congruent_pair)\n",
    "                \n",
    "                if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "                    # Save most congruent candidate pair and mentions\n",
    "                    most_congruent_pair = congruent_pair, (m, n)\n",
    "\n",
    "    elif len(mention_predictions) > 0:\n",
    "        \n",
    "        # Second+, recursive pass\n",
    "        for m in mention_predictions.keys():\n",
    "            for n in mentions_remaining:\n",
    "                \n",
    "                # Becauase we always assume search small mention to large to save computation/storage,\n",
    "                # we must sort incrementing variables to be in increasing order for query\n",
    "                m_tmp, n_tmp = np.sort((m, n))\n",
    "                \n",
    "                # Get most congruent pair in one matrix\n",
    "                congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp], verbose=verbose)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "                    print(\"Congruent Pair: \", congruent_pair)\n",
    "                    print(\"Current Most Congruent: \", most_congruent_pair)\n",
    "                    \n",
    "                if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "                    # Save most congruent candidate pair and mentions\n",
    "                    most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "                \n",
    "    if verbose: print(\"Final Most Congruent Pair: \", most_congruent_pair)\n",
    "    return most_congruent_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congruent Predictions Function\n",
    "\n",
    "This is our main function that takes a sentence ID, calculates congruence for all candidates, updates the prior likelihood from Phase 3 with that congruence and selects predictions iteratively based on that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate congruent predictions\n",
    "def get_congruent_predictions(sentence_id, dataframe, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to calculate congruence metrics over a set of entity full mentions\n",
    "    and return the predicted candidates based on the congruent metric\n",
    "    Input:\n",
    "    - todo Should I pass the dataframe as well?\n",
    "    - Sentence ID used to filter dataframe\n",
    "    Output:\n",
    "    - Prediction for each entity mention\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to dataframe representing single sentence\n",
    "    # Drop duplicates necessary for sentences with the same mention included twice\n",
    "    sentence_predictions = dataframe[dataframe['sentence_id'] == sentence_id]\\\n",
    "                        .drop_duplicates(['full_mention', 'wikipedia_URL', 'wikipedia_page_ID', 'wikipedia_title'])\\\n",
    "                        .reset_index(drop=True)\n",
    "    if verbose: display(sentence_predictions)\n",
    "    \n",
    "    # Define numerical representation of congruent mention list\n",
    "    sentence_congruent_mentions = sentence_predictions['congruent_mentions'][0]\n",
    "    sentence_mention_ids = np.arange(len(sentence_congruent_mentions))\n",
    "    if verbose:\n",
    "        print(\"Congruent Mentions: \", sentence_congruent_mentions)\n",
    "        print(\"Congruent Mentions as numbers: \", sentence_mention_ids)\n",
    "    \n",
    "    # Retrieve dictionary of candidate pool vectors for each mention\n",
    "    vectors_dict = create_entity_vector_dict(sentence_mention_ids, sentence_predictions, verbose=verbose)\n",
    "    if verbose: print(\"Mentions with Vectors: \", vectors_dict.keys())\n",
    "    \n",
    "    # Calculate congruence metric for each candidate vector for each mention's candidate pool\n",
    "    # This notebook uses cosine similarity as the congruence metric\n",
    "    congruence_dict = get_congruence_dict(vectors_dict, sentence_mention_ids, verbose=verbose)\n",
    "    if verbose: print(\"First-Level Congruence Keys: \", congruence_dict.keys())\n",
    "    # This should be one less than congruent mention count, since we are comparing low to high\n",
    "    # and thus don't compare the highest value to anything\n",
    "    \n",
    "    # Calculate weights matric for every pairwise combination of candidates for every mention\n",
    "    weights_dict = create_weighted_dict(sentence_mention_ids, sentence_predictions, verbose=verbose)\n",
    "    \n",
    "    # Combine congruence with prior weights\n",
    "    weighted_congruence = combine_congruence_priors(sentence_mention_ids, congruence_dict, weights_dict, verbose=verbose)\n",
    "    \n",
    "    # Create predictions dictionary\n",
    "    mention_predictions = {}\n",
    "    \n",
    "    # Create copy of sentence_mention_ids to iterate through\n",
    "    mentions_remaining = sentence_mention_ids.copy()\n",
    "    \n",
    "    # Iterate through congruent entity mentions to retrieve predictions\n",
    "    # where a prediction is the most congruent candidate between two mentions\n",
    "    while len(mentions_remaining) > 0:\n",
    "        \n",
    "        # Analyze congruence matrices to identify the most congruent pair\n",
    "        most_congruent_pair = find_most_congruent_pair(mention_predictions, mentions_remaining, weighted_congruence, verbose=verbose)\n",
    "        \n",
    "        # Save most congrent pair prediction for associated mentions\n",
    "        if len(mention_predictions) == 0:\n",
    "            # First pass\n",
    "            if verbose: print(most_congruent_pair)\n",
    "            # Handles error when nearly all candidate pools are empty\n",
    "            if most_congruent_pair == (((None, None), 0.0), (0, 0)):\n",
    "                mention_predictions[0] = 0\n",
    "            else:\n",
    "                mention_predictions[most_congruent_pair[1][0]] = most_congruent_pair[0][0][0]\n",
    "                mention_predictions[most_congruent_pair[1][1]] = most_congruent_pair[0][0][1]\n",
    "        elif len(mention_predictions) > 0:\n",
    "            # Second_, recursive pass\n",
    "            \n",
    "            # Find new mention you're predicting for\n",
    "            try:\n",
    "                # The number left over in the mention tuple if you remove anything in the prediction dict\n",
    "                new_mention_num = most_congruent_pair[1].index(\\\n",
    "                                                               list(set(most_congruent_pair[1])\\\n",
    "                                                                    - set(mention_predictions.keys())))\n",
    "\n",
    "                # Save new prediction\n",
    "                mention_predictions[most_congruent_pair[1][new_mention_num]] = most_congruent_pair[0][0][new_mention_num]\n",
    "            except ValueError:\n",
    "                for mention in mentions_remaining:\n",
    "                    mention_predictions[mention] = 0 # Zero produces error in final section to produce None prediction\n",
    "            \n",
    "        # Update remaining mentions to mentions without a prediction stored in the dictionary\n",
    "        mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "        if verbose: print(mentions_remaining, mention_predictions.keys())\n",
    "    \n",
    "    if verbose: print(mention_predictions)\n",
    "    \n",
    "    # Use mention predictions to return titles\n",
    "    readable_predictions = {}\n",
    "    for k, v in mention_predictions.items():\n",
    "        if verbose: print(k, v)\n",
    "        readable_key = sentence_congruent_mentions[k]\n",
    "        try:\n",
    "            readable_value = sentence_predictions['candidate_pool_titles'][k][v]\n",
    "            readable_id = sentence_predictions['candidate_pool_page_ids'][k][v]\n",
    "        except IndexError: # Handles case where no candidate pool was provided from Phase 3\n",
    "            readable_value = None \n",
    "            readable_id = None\n",
    "        except TypeError:\n",
    "            # Handles case where no congruence can be calculated\n",
    "            # Either due to one mention in sentence or two mentions but one with no candidate pool\n",
    "            readable_value = sentence_predictions['candidate_pool_titles'][0][0] # Just return top value from Phase 3\n",
    "            readable_id = sentence_predictions['candidate_pool_page_ids'][0][0]\n",
    "        if verbose: print(readable_key, readable_value, readable_id)\n",
    "        readable_predictions[readable_key] = (readable_value, readable_id)\n",
    "    \n",
    "    # Output dictionary with predictions for each entity mention based on congruence\n",
    "    return readable_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Iran</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iran</td>\n",
       "      <td>14653.0</td>\n",
       "      <td>Iran</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iran</td>\n",
       "      <td>[14653, 272865, 338883, 8294810, 46823116, 126...</td>\n",
       "      <td>[794, 184602, 207991, 1465546, 932162, 1042614...</td>\n",
       "      <td>[Iran, Iran_national_football_team, Pahlavi_dy...</td>\n",
       "      <td>[0.9790373, 0.0079465, 0.0016392, 0.0005319, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>turkey</td>\n",
       "      <td>[11125639, 743577, 72821, 24513964, 297071, 22...</td>\n",
       "      <td>[43, 483856, 43794, 4200953, 26844, 12560, 848...</td>\n",
       "      <td>[Turkey, Turkey_national_football_team, Turkey...</td>\n",
       "      <td>[0.9168911, 0.0269033, 0.0082007, 0.0038923, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iraq</td>\n",
       "      <td>7515928.0</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iraq</td>\n",
       "      <td>[7515928, 1039652, 5043324, 26215470, 2900620,...</td>\n",
       "      <td>[796, 186243, 545449, 3108185, 149805, 107802,...</td>\n",
       "      <td>[Iraq, Iraq_national_football_team, Iraq_War, ...</td>\n",
       "      <td>[0.8794007, 0.0341074, 0.0292135, 0.0119351, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Kurdish</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kurdish_people</td>\n",
       "      <td>17068.0</td>\n",
       "      <td>Kurdish people</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>[17068, 40316, 80777, 3821855, 4314285, 354232...</td>\n",
       "      <td>[12223, 36368, 41470, 1792998, 1117020, 121801...</td>\n",
       "      <td>[Kurds, Kurdish_languages, Kurdistan, Kurds_in...</td>\n",
       "      <td>[0.565625, 0.3114583, 0.0159722, 0.0142361, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention full_mention                                wikipedia_URL  \\\n",
       "0       B         Iran            http://en.wikipedia.org/wiki/Iran   \n",
       "1       B       Turkey                                          NaN   \n",
       "2       B         Iraq            http://en.wikipedia.org/wiki/Iraq   \n",
       "3       B      Kurdish  http://en.wikipedia.org/wiki/Kurdish_people   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            14653.0            Iran           90      11   \n",
       "1                NaN             NaN           90      11   \n",
       "2          7515928.0            Iraq           90      11   \n",
       "3            17068.0  Kurdish people           90      11   \n",
       "\n",
       "              congruent_mentions norm_full_mention  \\\n",
       "0  [Iran, Turkey, Iraq, Kurdish]              iran   \n",
       "1  [Iran, Turkey, Iraq, Kurdish]            turkey   \n",
       "2  [Iran, Turkey, Iraq, Kurdish]              iraq   \n",
       "3  [Iran, Turkey, Iraq, Kurdish]           kurdish   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [14653, 272865, 338883, 8294810, 46823116, 126...   \n",
       "1  [11125639, 743577, 72821, 24513964, 297071, 22...   \n",
       "2  [7515928, 1039652, 5043324, 26215470, 2900620,...   \n",
       "3  [17068, 40316, 80777, 3821855, 4314285, 354232...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [794, 184602, 207991, 1465546, 932162, 1042614...   \n",
       "1  [43, 483856, 43794, 4200953, 26844, 12560, 848...   \n",
       "2  [796, 186243, 545449, 3108185, 149805, 107802,...   \n",
       "3  [12223, 36368, 41470, 1792998, 1117020, 121801...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [Iran, Iran_national_football_team, Pahlavi_dy...   \n",
       "1  [Turkey, Turkey_national_football_team, Turkey...   \n",
       "2  [Iraq, Iraq_national_football_team, Iraq_War, ...   \n",
       "3  [Kurds, Kurdish_languages, Kurdistan, Kurds_in...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.9790373, 0.0079465, 0.0016392, 0.0005319, 0...  \n",
       "1  [0.9168911, 0.0269033, 0.0082007, 0.0038923, 0...  \n",
       "2  [0.8794007, 0.0341074, 0.0292135, 0.0119351, 0...  \n",
       "3  [0.565625, 0.3114583, 0.0159722, 0.0142361, 0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congruent Mentions:  ['Iran', 'Turkey', 'Iraq', 'Kurdish']\n",
      "Congruent Mentions as numbers:  [0 1 2 3]\n",
      "['Iran', 'Iran_national_football_team', 'Pahlavi_dynasty', \"Iran_men's_national_basketball_team\", \"Iran_men's_national_volleyball_team\", 'Football_Federation_Islamic_Republic_of_Iran', 'Iran_national_beach_soccer_team', 'Qajar_dynasty', 'COVID-19_pandemic_in_Iran', 'Iran_national_futsal_team']\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "['Turkey', 'Turkey_national_football_team', 'Turkey_(bird)', 'Turkey_as_food', 'Wild_turkey', 'Ottoman_Empire', 'Domestic_turkey', \"Turkey_men's_national_basketball_team\", 'Turkey_national_under-21_football_team', 'Turkey_national_cricket_team']\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "['Iraq', 'Iraq_national_football_team', 'Iraq_War', \"Ba'athist_Iraq\", 'Kingdom_of_Iraq', '2003_invasion_of_Iraq', 'Lower_Mesopotamia', 'Mandatory_Iraq', 'Iraq_national_under-23_football_team', 'Anglo-Iraqi_War']\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['Kurds', 'Kurdish_languages', 'Kurdistan', 'Kurds_in_Syria', 'Kurds_in_Turkey', 'Kurds_in_Iraq', 'Iraqi_Kurdistan', 'Kurdish_population', 'Kurdish_music', 'Kurdish_alphabets']\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "dict_keys([0, 1, 2, 3])\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "Mentions with Vectors:  dict_keys([0, 1, 2, 3])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 0 & 3\n",
      "Comparing mentions 1 & 2\n",
      "Comparing mentions 1 & 3\n",
      "Comparing mentions 2 & 3\n",
      "First-Level Congruence Keys:  dict_keys([0, 1, 2])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 0 & 3\n",
      "Comparing mentions 1 & 2\n",
      "Comparing mentions 1 & 3\n",
      "Comparing mentions 2 & 3\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((0, 0), 0.6162219484138987)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((0, 0), 0.633228198648344)\n",
      "Current Most Congruent:  (((0, 0), 0.6162219484138987), (0, 1))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 0), 0.47160023330230033)\n",
      "Current Most Congruent:  (((0, 0), 0.633228198648344), (0, 2))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.5048746816925956)\n",
      "Current Most Congruent:  (((0, 0), 0.633228198648344), (0, 2))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 0), 0.4337822074951751)\n",
      "Current Most Congruent:  (((0, 0), 0.633228198648344), (0, 2))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.4935587665849268)\n",
      "Current Most Congruent:  (((0, 0), 0.633228198648344), (0, 2))\n",
      "Final Most Congruent Pair:  (((0, 0), 0.633228198648344), (0, 2))\n",
      "(((0, 0), 0.633228198648344), (0, 2))\n",
      "[1, 3] dict_keys([0, 2])\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((0, 0), 0.6162219484138987)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 0), 0.47160023330230033)\n",
      "Current Most Congruent:  (((0, 0), 0.6162219484138987), (0, 1))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.5048746816925956)\n",
      "Current Most Congruent:  (((0, 0), 0.6162219484138987), (0, 1))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.4935587665849268)\n",
      "Current Most Congruent:  (((0, 0), 0.6162219484138987), (0, 1))\n",
      "Final Most Congruent Pair:  (((0, 0), 0.6162219484138987), (0, 1))\n",
      "[3] dict_keys([0, 2, 1])\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 0), 0.47160023330230033)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.4935587665849268)\n",
      "Current Most Congruent:  (((0, 0), 0.47160023330230033), (0, 3))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 0), 0.4337822074951751)\n",
      "Current Most Congruent:  (((0, 0), 0.4935587665849268), (2, 3))\n",
      "Final Most Congruent Pair:  (((0, 0), 0.4935587665849268), (2, 3))\n",
      "[] dict_keys([0, 2, 1, 3])\n",
      "{0: 0, 2: 0, 1: 0, 3: 0}\n",
      "0 0\n",
      "Iran Iran 14653\n",
      "2 0\n",
      "Iraq Iraq 7515928\n",
      "1 0\n",
      "Turkey Turkey 11125639\n",
      "3 0\n",
      "Kurdish Kurds 17068\n",
      "{'Iran': ('Iran', 14653), 'Iraq': ('Iraq', 7515928), 'Turkey': ('Turkey', 11125639), 'Kurdish': ('Kurds', 17068)}\n",
      "CPU times: user 31.1 ms, sys: 3.97 ms, total: 35.1 ms\n",
      "Wall time: 32.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test out function\n",
    "congruent_predictions = get_congruent_predictions(sentence_id=90, dataframe=predictions, verbose=True)\n",
    "print(congruent_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Iran</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iran</td>\n",
       "      <td>14653.0</td>\n",
       "      <td>Iran</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iran</td>\n",
       "      <td>[14653, 272865, 338883, 8294810, 46823116, 126...</td>\n",
       "      <td>[794, 184602, 207991, 1465546, 932162, 1042614...</td>\n",
       "      <td>[Iran, Iran_national_football_team, Pahlavi_dy...</td>\n",
       "      <td>[0.9790373, 0.0079465, 0.0016392, 0.0005319, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>turkey</td>\n",
       "      <td>[11125639, 743577, 72821, 24513964, 297071, 22...</td>\n",
       "      <td>[43, 483856, 43794, 4200953, 26844, 12560, 848...</td>\n",
       "      <td>[Turkey, Turkey_national_football_team, Turkey...</td>\n",
       "      <td>[0.9168911, 0.0269033, 0.0082007, 0.0038923, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iraq</td>\n",
       "      <td>7515928.0</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iraq</td>\n",
       "      <td>[7515928, 1039652, 5043324, 26215470, 2900620,...</td>\n",
       "      <td>[796, 186243, 545449, 3108185, 149805, 107802,...</td>\n",
       "      <td>[Iraq, Iraq_national_football_team, Iraq_War, ...</td>\n",
       "      <td>[0.8794007, 0.0341074, 0.0292135, 0.0119351, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Kurdish</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kurdish_people</td>\n",
       "      <td>17068.0</td>\n",
       "      <td>Kurdish people</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>[17068, 40316, 80777, 3821855, 4314285, 354232...</td>\n",
       "      <td>[12223, 36368, 41470, 1792998, 1117020, 121801...</td>\n",
       "      <td>[Kurds, Kurdish_languages, Kurdistan, Kurds_in...</td>\n",
       "      <td>[0.565625, 0.3114583, 0.0159722, 0.0142361, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention full_mention                                wikipedia_URL  \\\n",
       "0       B         Iran            http://en.wikipedia.org/wiki/Iran   \n",
       "1       B       Turkey                                          NaN   \n",
       "2       B         Iraq            http://en.wikipedia.org/wiki/Iraq   \n",
       "3       B      Kurdish  http://en.wikipedia.org/wiki/Kurdish_people   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            14653.0            Iran           90      11   \n",
       "1                NaN             NaN           90      11   \n",
       "2          7515928.0            Iraq           90      11   \n",
       "3            17068.0  Kurdish people           90      11   \n",
       "\n",
       "              congruent_mentions norm_full_mention  \\\n",
       "0  [Iran, Turkey, Iraq, Kurdish]              iran   \n",
       "1  [Iran, Turkey, Iraq, Kurdish]            turkey   \n",
       "2  [Iran, Turkey, Iraq, Kurdish]              iraq   \n",
       "3  [Iran, Turkey, Iraq, Kurdish]           kurdish   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [14653, 272865, 338883, 8294810, 46823116, 126...   \n",
       "1  [11125639, 743577, 72821, 24513964, 297071, 22...   \n",
       "2  [7515928, 1039652, 5043324, 26215470, 2900620,...   \n",
       "3  [17068, 40316, 80777, 3821855, 4314285, 354232...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [794, 184602, 207991, 1465546, 932162, 1042614...   \n",
       "1  [43, 483856, 43794, 4200953, 26844, 12560, 848...   \n",
       "2  [796, 186243, 545449, 3108185, 149805, 107802,...   \n",
       "3  [12223, 36368, 41470, 1792998, 1117020, 121801...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [Iran, Iran_national_football_team, Pahlavi_dy...   \n",
       "1  [Turkey, Turkey_national_football_team, Turkey...   \n",
       "2  [Iraq, Iraq_national_football_team, Iraq_War, ...   \n",
       "3  [Kurds, Kurdish_languages, Kurdistan, Kurds_in...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.9790373, 0.0079465, 0.0016392, 0.0005319, 0...  \n",
       "1  [0.9168911, 0.0269033, 0.0082007, 0.0038923, 0...  \n",
       "2  [0.8794007, 0.0341074, 0.0292135, 0.0119351, 0...  \n",
       "3  [0.565625, 0.3114583, 0.0159722, 0.0142361, 0....  "
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview dataframe again\n",
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iran , Iran\n",
      "Iraq , Iraq\n",
      "Turkey , Turkey\n",
      "Kurdish , Kurds\n",
      "*************************************************\n",
      "This congruent experiment is 50.0% accurate comparing page titles.\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for mention, pred in congruent_predictions.items():\n",
    "    pred_title = normalize_text(pred[0])\n",
    "    print(mention, \",\", pred_title)\n",
    "    if sentence_predictions[sentence_predictions['full_mention'] == mention]['wikipedia_title'].values[0] == pred_title:\n",
    "        accuracy += 1\n",
    "print(\"*************************************************\")\n",
    "print(f\"This congruent experiment is {round(accuracy/len(congruent_predictions)*100,3)}% accurate comparing page titles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iran , 14653\n",
      "Iraq , 7515928\n",
      "Turkey , 11125639\n",
      "Kurdish , 17068\n",
      "*************************************************\n",
      "This congruent experiment is 75.0% accurate comparing page IDs.\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for mention, pred in congruent_predictions.items():\n",
    "    pred_page_id = pred[1]\n",
    "    print(mention, \",\", pred_page_id)\n",
    "    if sentence_predictions[sentence_predictions['full_mention'] == mention]['wikipedia_page_ID'].values[0] == pred_page_id:\n",
    "        accuracy += 1\n",
    "print(\"*************************************************\")\n",
    "print(f\"This congruent experiment is {round(accuracy/len(congruent_predictions)*100,3)}% accurate comparing page IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Congruence Predictions and Assess Accuracy over Entire Dataframe\n",
    "\n",
    "We now apply the per-sentence structure over the whole ACY dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5,447 sentences to predict.\n"
     ]
    }
   ],
   "source": [
    "# Max sentence_id in dataframe\n",
    "max_sentence_id = max(predictions['sentence_id'])\n",
    "print(\"We have {:,} sentences to predict.\".format(max_sentence_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4516/4516 [02:27<00:00, 30.68it/s] \n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "dataframe_predictions = {}\n",
    "for sid in tqdm(predictions['sentence_id'].unique()):\n",
    "    dataframe_predictions[sid] = get_congruent_predictions(sid, dataframe=predictions, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29312/29312 [00:03<00:00, 7504.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After congruence, we have achieved 53.384% accuracy comparing title.\n",
      "After congruence, we have achieved 57.028% accuracy comparing page ID.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to calculate accuracy\n",
    "accurate_predictions_title = 0\n",
    "accurate_predictions_id = 0\n",
    "for row in tqdm(range(len(predictions))):\n",
    "    mention_df = predictions.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    page_id = mention_df['wikipedia_page_ID']\n",
    "    pred = dataframe_predictions[sid][fm]\n",
    "    norm_pred_title = normalize_text(pred[0])\n",
    "    pred_page_id = pred[1]\n",
    "#     print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred_title, \"|||\", norm_pred_title==title,\\\n",
    "#                  \"||| True ID: \", page_id, \"==? Pred:\", pred_page_id, \"|||\", pred_page_id==page_id)\n",
    "    if title == norm_pred_title:\n",
    "        accurate_predictions_title += 1\n",
    "    if page_id == pred_page_id:\n",
    "        accurate_predictions_id += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing title.\".format(round(accurate_predictions_title/len(predictions)*100, 3)))\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing page ID.\".format(round(accurate_predictions_id/len(predictions)*100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jim Edmonds 929 ||| True: Jim Edmonds ==? Pred: Jim Edmonds ||| True ||| True ID:  881905.0 ==? Pred: 881905 ||| True\n",
      "Jim Edmonds 929 ||| True: Jim Edmonds ==? Pred: Jim Edmonds ||| True ||| True ID:  881905.0 ==? Pred: 881905 ||| True\n",
      "California Angels 929 ||| True: Los Angeles Angels of Anaheim ==? Pred: Los Angeles Angels ||| False ||| True ID:  1360083.0 ==? Pred: 1360083 ||| True\n",
      "California Angels 929 ||| True: Los Angeles Angels of Anaheim ==? Pred: Los Angeles Angels ||| False ||| True ID:  1360083.0 ==? Pred: 1360083 ||| True\n",
      "Yankees 929 ||| True: New York Yankees ==? Pred: New York Yankees ||| True ||| True ID:  4848143.0 ==? Pred: 4848143 ||| True\n",
      "Angels 930 ||| True: Los Angeles Angels of Anaheim ==? Pred: Angels (Robbie Williams song) ||| False ||| True ID:  1360083.0 ==? Pred: 2029170 ||| False\n",
      "Kenny Rogers 930 ||| True: Kenny Rogers (baseball) ==? Pred: Kenny Rogers ||| False ||| True ID:  599018.0 ==? Pred: 265030 ||| False\n",
      "Kenny Rogers 930 ||| True: Kenny Rogers (baseball) ==? Pred: Kenny Rogers ||| False ||| True ID:  599018.0 ==? Pred: 265030 ||| False\n",
      "Yankees 931 ||| True: New York Yankees ==? Pred: New York Yankees ||| True ||| True ID:  4848143.0 ==? Pred: 4848143 ||| True\n",
      "Chuck Finley 932 ||| True: Chuck Finley ==? Pred: Chuck Finley ||| True ||| True ID:  869667.0 ==? Pred: 869667 ||| True\n",
      "After congruence, we have achieved 50.0% accuracy comparing title.\n",
      "After congruence, we have achieved 70.0% accuracy comparing page ID.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over subset of dataframe to calculate accuracy and study output results\n",
    "accurate_predictions_title = 0\n",
    "accurate_predictions_id = 0\n",
    "rand_idx = np.random.randint(len(predictions)) # Observe random subsection of predictions\n",
    "window_size = 10\n",
    "for row in range(rand_idx, rand_idx+window_size):\n",
    "    mention_df = predictions.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    page_id = mention_df['wikipedia_page_ID']\n",
    "    pred = dataframe_predictions[sid][fm]\n",
    "    norm_pred_title = normalize_text(pred[0])\n",
    "    pred_page_id = pred[1]\n",
    "    print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred_title, \"|||\", norm_pred_title==title,\\\n",
    "                 \"||| True ID: \", page_id, \"==? Pred:\", pred_page_id, \"|||\", pred_page_id==page_id)\n",
    "    if title == norm_pred_title:\n",
    "        accurate_predictions_title += 1\n",
    "    if page_id == pred_page_id:\n",
    "        accurate_predictions_id += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing title.\".format(round(accurate_predictions_title/window_size*100, 3)))\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing page ID.\".format(round(accurate_predictions_id/window_size*100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy only with Full Mentions with Known True\n",
    "\n",
    "This is a better reflection of our success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22257\n"
     ]
    }
   ],
   "source": [
    "# Calculate length of input with known true values\n",
    "known_true = sum(predictions['wikipedia_page_ID'].notnull())\n",
    "print(known_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29312/29312 [00:03<00:00, 7481.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After congruence, we have achieved 70.306% accuracy comparing title.\n",
      "After congruence, we have achieved 75.104% accuracy comparing page ID.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to calculate accuracy\n",
    "accurate_predictions_title = 0\n",
    "accurate_predictions_id = 0\n",
    "for row in tqdm(range(len(predictions))):\n",
    "    mention_df = predictions.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    page_id = mention_df['wikipedia_page_ID']\n",
    "    if page_id is None:\n",
    "        pass\n",
    "    else:\n",
    "        pred = dataframe_predictions[sid][fm]\n",
    "        norm_pred_title = normalize_text(pred[0])\n",
    "        pred_page_id = pred[1]\n",
    "    #     print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred_title, \"|||\", norm_pred_title==title,\\\n",
    "    #                  \"||| True ID: \", page_id, \"==? Pred:\", pred_page_id, \"|||\", pred_page_id==page_id)\n",
    "        if title == norm_pred_title:\n",
    "            accurate_predictions_title += 1\n",
    "        if page_id == pred_page_id:\n",
    "            accurate_predictions_id += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing title.\".format(round(accurate_predictions_title/known_true*100, 3)))\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing page ID.\".format(round(accurate_predictions_id/known_true*100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Hagelin 1193 ||| True: John Hagelin ==? Pred: John Hagelin ||| True ||| True ID:  16077.0 ==? Pred: 16077 ||| True\n",
      "TM 1196 ||| True: nan ==? Pred: Telekom Malaysia ||| False ||| True ID:  nan ==? Pred: 1138839 ||| False\n",
      "TM 1197 ||| True: nan ==? Pred: Telekom Malaysia ||| False ||| True ID:  nan ==? Pred: 1138839 ||| False\n",
      "Windows 95 1199 ||| True: Windows 95 ==? Pred: Windows 95 ||| True ||| True ID:  34064.0 ==? Pred: 34064 ||| True\n",
      "Windows 95 1199 ||| True: Windows 95 ==? Pred: Windows 95 ||| True ||| True ID:  34064.0 ==? Pred: 34064 ||| True\n",
      "Martin Wolk 1200 ||| True: nan ==? Pred: None ||| False ||| True ID:  nan ==? Pred: None ||| False\n",
      "Martin Wolk 1200 ||| True: nan ==? Pred: None ||| False ||| True ID:  nan ==? Pred: None ||| False\n",
      "SEATTLE 1200 ||| True: Seattle ==? Pred: Seattle ||| True ||| True ID:  11388236.0 ==? Pred: 11388236 ||| True\n",
      "Microsoft Corp. 1200 ||| True: Microsoft ==? Pred: Microsoft ||| True ||| True ID:  19001.0 ==? Pred: 19001 ||| True\n",
      "Microsoft Corp. 1200 ||| True: Microsoft ==? Pred: Microsoft ||| True ||| True ID:  19001.0 ==? Pred: 19001 ||| True\n",
      "After congruence, we have achieved 60.0% accuracy comparing title.\n",
      "After congruence, we have achieved 60.0% accuracy comparing page ID.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over subset of dataframe to calculate accuracy and study output results\n",
    "accurate_predictions_title = 0\n",
    "accurate_predictions_id = 0\n",
    "rand_idx = np.random.randint(len(predictions)) # Observe random subsection of predictions\n",
    "window_size = 10\n",
    "for row in range(rand_idx, rand_idx+window_size):\n",
    "    mention_df = predictions.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    page_id = mention_df['wikipedia_page_ID']\n",
    "    if page_id is None:\n",
    "        pass\n",
    "    else:\n",
    "        pred = dataframe_predictions[sid][fm]\n",
    "        norm_pred_title = normalize_text(pred[0])\n",
    "        pred_page_id = pred[1]\n",
    "        print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred_title, \"|||\", norm_pred_title==title,\\\n",
    "                     \"||| True ID: \", page_id, \"==? Pred:\", pred_page_id, \"|||\", pred_page_id==page_id)\n",
    "        if title == norm_pred_title:\n",
    "            accurate_predictions_title += 1\n",
    "        if page_id == pred_page_id:\n",
    "            accurate_predictions_id += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing title.\".format(round(accurate_predictions_title/window_size*100, 3)))\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing page ID.\".format(round(accurate_predictions_id/window_size*100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate with *Popularity* Predictions Known True Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 9891, 9472, 10890716, 2780146, 18...</td>\n",
       "      <td>[458, 46, 45003, 4916, 185441, 932442, 8268, 8...</td>\n",
       "      <td>['European_Union', 'Europe', 'Entropy', 'Euro'...</td>\n",
       "      <td>[0.2237612, 0.2008411, 0.0962244, 0.0766248, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 11867, 27318, 21148, 21212, 21212, 269...</td>\n",
       "      <td>[183, 183, 334, 55, 7318, 7318, 40, 12548, 825...</td>\n",
       "      <td>['Germany', 'Germany', 'Singapore', 'Netherlan...</td>\n",
       "      <td>[0.0494115, 0.0494115, 0.0485629, 0.0390487, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>british</td>\n",
       "      <td>[3434750, 31717, 31717, 19344654, 26061, 85699...</td>\n",
       "      <td>[30, 145, 145, 9531, 172771, 1860, 21, 22, 868...</td>\n",
       "      <td>['United_States', 'United_Kingdom', 'United_Ki...</td>\n",
       "      <td>[0.115186, 0.0611816, 0.0611816, 0.0294143, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 56873217, 9643132]</td>\n",
       "      <td>[2073954, 26634508, 7172840]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.6296296, 0.3703704, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 56873217, 9643132]</td>\n",
       "      <td>[2073954, 26634508, 7172840]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.6296296, 0.3703704, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention     full_mention                                wikipedia_URL  \\\n",
       "0       B               EU                                          NaN   \n",
       "1       B           German         http://en.wikipedia.org/wiki/Germany   \n",
       "2       B          British  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "3       B  Peter Blackburn                                          NaN   \n",
       "4       I  Peter Blackburn                                          NaN   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0                NaN             NaN            0       0   \n",
       "1            11867.0         Germany            0       0   \n",
       "2            31717.0  United Kingdom            0       0   \n",
       "3                NaN             NaN            1       0   \n",
       "4                NaN             NaN            1       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0                        ['EU', 'German', 'British']                eu   \n",
       "1                        ['EU', 'German', 'British']            german   \n",
       "2                        ['EU', 'German', 'British']           british   \n",
       "3  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "4  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [9317, 9239, 9891, 9472, 10890716, 2780146, 18...   \n",
       "1  [11867, 11867, 27318, 21148, 21212, 21212, 269...   \n",
       "2  [3434750, 31717, 31717, 19344654, 26061, 85699...   \n",
       "3                      [56783206, 56873217, 9643132]   \n",
       "4                      [56783206, 56873217, 9643132]   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [458, 46, 45003, 4916, 185441, 932442, 8268, 8...   \n",
       "1  [183, 183, 334, 55, 7318, 7318, 40, 12548, 825...   \n",
       "2  [30, 145, 145, 9531, 172771, 1860, 21, 22, 868...   \n",
       "3                       [2073954, 26634508, 7172840]   \n",
       "4                       [2073954, 26634508, 7172840]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  ['European_Union', 'Europe', 'Entropy', 'Euro'...   \n",
       "1  ['Germany', 'Germany', 'Singapore', 'Netherlan...   \n",
       "2  ['United_States', 'United_Kingdom', 'United_Ki...   \n",
       "3  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "4  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.2237612, 0.2008411, 0.0962244, 0.0766248, 0...  \n",
       "1  [0.0494115, 0.0494115, 0.0485629, 0.0390487, 0...  \n",
       "2  [0.115186, 0.0611816, 0.0611816, 0.0294143, 0....  \n",
       "3                        [0.6296296, 0.3703704, 0.0]  \n",
       "4                        [0.6296296, 0.3703704, 0.0]  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "predictions_pop = pd.read_csv(os.path.join(preds_path, \"anchortext_popularity.csv\"), delimiter=\",\")\n",
    "predictions_pop.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "After ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "Before [56783206, 56873217, 9643132]\n",
      "After [56783206, 56873217, 9643132]\n",
      "Before [2073954, 26634508, 7172840]\n",
      "After [2073954, 26634508, 7172840]\n",
      "Before ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(MP)', 'Peter_Blackburn_(bishop)']\n",
      "After ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(MP)', 'Peter_Blackburn_(bishop)']\n",
      "Before [0.6296296, 0.3703704, 0.0]\n",
      "After [0.6296296, 0.3703704, 0.0]\n",
      "CPU times: user 683 ms, sys: 306 ms, total: 990 ms\n",
      "Wall time: 1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Apply defined function to entire dataframe for all candidate pool columns\n",
    "\n",
    "column = 'congruent_mentions'\n",
    "print(\"Before\", predictions_pop[column][3])\n",
    "parsed_candidate_pool = predictions_pop[column].apply(parse_list_string, value_type=str)\n",
    "predictions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions_pop[column][3])\n",
    "\n",
    "column = 'candidate_pool_page_ids'\n",
    "print(\"Before\", predictions_pop[column][3])\n",
    "parsed_candidate_pool = predictions_pop[column].apply(parse_list_string, value_type=int)\n",
    "predictions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions_pop[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_item_ids'\n",
    "print(\"Before\", predictions_pop[column][3])\n",
    "parsed_candidate_pool = predictions_pop[column].apply(parse_list_string, value_type=int)\n",
    "predictions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions_pop[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_titles'\n",
    "print(\"Before\", predictions_pop[column][3])\n",
    "parsed_candidate_pool = predictions_pop[column].apply(parse_list_string, value_type=str)\n",
    "predictions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions_pop[column][3])\n",
    "\n",
    "column = 'candidate_pool_likelihoods'\n",
    "print(\"Before\", predictions_pop[column][3])\n",
    "parsed_candidate_pool = predictions_pop[column].apply(parse_list_string, value_type=float)\n",
    "predictions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions_pop[column][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22257\n"
     ]
    }
   ],
   "source": [
    "# Calculate length of input with known true values\n",
    "known_true = sum(predictions_pop['wikipedia_page_ID'].notnull())\n",
    "print(known_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[EU, German, British]</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 9891, 9472, 10890716, 2780146, 18...</td>\n",
       "      <td>[458, 46, 45003, 4916, 185441, 932442, 8268, 8...</td>\n",
       "      <td>[European_Union, Europe, Entropy, Euro, Member...</td>\n",
       "      <td>[0.2237612, 0.2008411, 0.0962244, 0.0766248, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[EU, German, British]</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 11867, 27318, 21148, 21212, 21212, 269...</td>\n",
       "      <td>[183, 183, 334, 55, 7318, 7318, 40, 12548, 825...</td>\n",
       "      <td>[Germany, Germany, Singapore, Netherlands, Naz...</td>\n",
       "      <td>[0.0494115, 0.0494115, 0.0485629, 0.0390487, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[EU, German, British]</td>\n",
       "      <td>british</td>\n",
       "      <td>[3434750, 31717, 31717, 19344654, 26061, 85699...</td>\n",
       "      <td>[30, 145, 145, 9531, 172771, 1860, 21, 22, 868...</td>\n",
       "      <td>[United_States, United_Kingdom, United_Kingdom...</td>\n",
       "      <td>[0.115186, 0.0611816, 0.0611816, 0.0294143, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention full_mention                                wikipedia_URL  \\\n",
       "0       B           EU                                          NaN   \n",
       "1       B       German         http://en.wikipedia.org/wiki/Germany   \n",
       "2       B      British  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0                NaN             NaN            0       0   \n",
       "1            11867.0         Germany            0       0   \n",
       "2            31717.0  United Kingdom            0       0   \n",
       "\n",
       "      congruent_mentions norm_full_mention  \\\n",
       "0  [EU, German, British]                eu   \n",
       "1  [EU, German, British]            german   \n",
       "2  [EU, German, British]           british   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [9317, 9239, 9891, 9472, 10890716, 2780146, 18...   \n",
       "1  [11867, 11867, 27318, 21148, 21212, 21212, 269...   \n",
       "2  [3434750, 31717, 31717, 19344654, 26061, 85699...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [458, 46, 45003, 4916, 185441, 932442, 8268, 8...   \n",
       "1  [183, 183, 334, 55, 7318, 7318, 40, 12548, 825...   \n",
       "2  [30, 145, 145, 9531, 172771, 1860, 21, 22, 868...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [European_Union, Europe, Entropy, Euro, Member...   \n",
       "1  [Germany, Germany, Singapore, Netherlands, Naz...   \n",
       "2  [United_States, United_Kingdom, United_Kingdom...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.2237612, 0.2008411, 0.0962244, 0.0766248, 0...  \n",
       "1  [0.0494115, 0.0494115, 0.0485629, 0.0390487, 0...  \n",
       "2  [0.115186, 0.0611816, 0.0611816, 0.0294143, 0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congruent Mentions:  ['EU', 'German', 'British']\n",
      "Congruent Mentions as numbers:  [0 1 2]\n",
      "['European_Union', 'Europe', 'Entropy', 'Euro', 'Member_state_of_the_European_Union', 'European_emission_standards', 'Eurozone', 'European_Parliament', 'European_Commission', 'Europe,_the_Middle_East_and_Africa']\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['Germany', 'Germany', 'Singapore', 'Netherlands', 'Nazi_Germany', 'Nazi_Germany', 'Austria', 'Holy_Roman_Empire', 'Bundesliga', 'Age_of_Enlightenment']\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['United_States', 'United_Kingdom', 'United_Kingdom', 'BBC', 'Royal_Navy', 'English_language', 'England', 'Scotland', 'British_Empire', 'Commonwealth_of_Nations']\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "dict_keys([0, 1, 2])\n",
      "10\n",
      "10\n",
      "10\n",
      "Mentions with Vectors:  dict_keys([0, 1, 2])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 1 & 2\n",
      "First-Level Congruence Keys:  dict_keys([0, 1])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 1 & 2\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((1, 3), 0.05630195596694946)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 0), 0.07658241383750587)\n",
      "Current Most Congruent:  (((1, 3), 0.05630195596694946), (0, 1))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.032764191008694474)\n",
      "Current Most Congruent:  (((1, 0), 0.07658241383750587), (0, 2))\n",
      "Final Most Congruent Pair:  (((1, 0), 0.07658241383750587), (0, 2))\n",
      "(((1, 0), 0.07658241383750587), (0, 2))\n",
      "[1] dict_keys([0, 2])\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((1, 3), 0.05630195596694946)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.032764191008694474)\n",
      "Current Most Congruent:  (((1, 3), 0.05630195596694946), (0, 1))\n",
      "Final Most Congruent Pair:  (((1, 3), 0.05630195596694946), (0, 1))\n",
      "[] dict_keys([0, 2, 1])\n",
      "{0: 1, 2: 0, 1: 3}\n",
      "0 1\n",
      "EU Europe 9239\n",
      "2 0\n",
      "British United_States 3434750\n",
      "1 3\n",
      "German Netherlands 21148\n",
      "{'EU': ('Europe', 9239), 'British': ('United_States', 3434750), 'German': ('Netherlands', 21148)}\n",
      "CPU times: user 26.4 ms, sys: 10 ms, total: 36.4 ms\n",
      "Wall time: 36.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test out function\n",
    "congruent_predictions = get_congruent_predictions(sentence_id=0, dataframe=predictions_pop, verbose=True)\n",
    "print(congruent_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4516/4516 [02:34<00:00, 29.14it/s] \n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "dataframe_predictions_pop = {}\n",
    "for sid in tqdm(predictions_pop['sentence_id'].unique()):\n",
    "    dataframe_predictions_pop[sid] = get_congruent_predictions(sid, dataframe=predictions_pop, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29312/29312 [00:03<00:00, 7468.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After congruence, we have achieved 57.959% accuracy comparing title.\n",
      "After congruence, we have achieved 62.066% accuracy comparing page ID.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to calculate accuracy\n",
    "accurate_predictions_title = 0\n",
    "accurate_predictions_id = 0\n",
    "for row in tqdm(range(len(predictions_pop))):\n",
    "    mention_df = predictions_pop.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    page_id = mention_df['wikipedia_page_ID']\n",
    "    if page_id is None:\n",
    "        pass\n",
    "    else:\n",
    "        pred = dataframe_predictions_pop[sid][fm]\n",
    "        norm_pred_title = normalize_text(pred[0])\n",
    "        pred_page_id = pred[1]\n",
    "    #     print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred_title, \"|||\", norm_pred_title==title,\\\n",
    "    #                  \"||| True ID: \", page_id, \"==? Pred:\", pred_page_id, \"|||\", pred_page_id==page_id)\n",
    "        if title == norm_pred_title:\n",
    "            accurate_predictions_title += 1\n",
    "        if page_id == pred_page_id:\n",
    "            accurate_predictions_id += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing title.\".format(round(accurate_predictions_title/known_true*100, 3)))\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing page ID.\".format(round(accurate_predictions_id/known_true*100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical Flow Demonstration\n",
    "\n",
    "The cells below have been included as a more easily understood logical flow to understand how we designed the recursive congruence algorithm for an arbitrary length of full mentions in a sentence. We manually select a sentence and work through that. This is identical to the above but with more printed out breaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Known Oddities\n",
    "1. Test with sentence_id == 1. We only return unique mentions in a single sentence. Is that ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Germany</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>germany</td>\n",
       "      <td>[11867, 250204, 21212, 12674, 33685, 662281, 1...</td>\n",
       "      <td>[183, 43310, 7318, 43287, 41304, 154408, 12031...</td>\n",
       "      <td>[Germany, Germany_national_football_team, Nazi...</td>\n",
       "      <td>[0.8896856, 0.021721, 0.0153527, 0.0140082, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>European Union</td>\n",
       "      <td>http://en.wikipedia.org/wiki/European_Union</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>European Union</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>european union</td>\n",
       "      <td>[9317, 1933156, 9317, 10890716, 276436, 265743...</td>\n",
       "      <td>[458, 1376407, 458, 185441, 208202, 319328, 36...</td>\n",
       "      <td>[European_Union, European_Boxing_Union, Europe...</td>\n",
       "      <td>[0.9884673, 0.0026042, 0.0011161, 0.0009566, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Werner Zwingmann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>werner zwingmann</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Britain</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>britain</td>\n",
       "      <td>[31717, 13530298, 152256, 158019, 13525, 4721,...</td>\n",
       "      <td>[145, 23666, 174193, 161885, 185103, 8680, 977...</td>\n",
       "      <td>[United_Kingdom, Great_Britain, United_Kingdom...</td>\n",
       "      <td>[0.5200594, 0.2035661, 0.0822639, 0.0451168, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                wikipedia_URL  \\\n",
       "0       B           Germany         http://en.wikipedia.org/wiki/Germany   \n",
       "1       B    European Union  http://en.wikipedia.org/wiki/European_Union   \n",
       "2       B  Werner Zwingmann                                          NaN   \n",
       "3       B           Britain  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            11867.0         Germany            2       0   \n",
       "1             9317.0  European Union            2       0   \n",
       "2                NaN             NaN            2       0   \n",
       "3            31717.0  United Kingdom            2       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [Germany, European Union, Werner Zwingmann, Br...           germany   \n",
       "1  [Germany, European Union, Werner Zwingmann, Br...    european union   \n",
       "2  [Germany, European Union, Werner Zwingmann, Br...  werner zwingmann   \n",
       "3  [Germany, European Union, Werner Zwingmann, Br...           britain   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [11867, 250204, 21212, 12674, 33685, 662281, 1...   \n",
       "1  [9317, 1933156, 9317, 10890716, 276436, 265743...   \n",
       "2                                                 []   \n",
       "3  [31717, 13530298, 152256, 158019, 13525, 4721,...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [183, 43310, 7318, 43287, 41304, 154408, 12031...   \n",
       "1  [458, 1376407, 458, 185441, 208202, 319328, 36...   \n",
       "2                                                 []   \n",
       "3  [145, 23666, 174193, 161885, 185103, 8680, 977...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [Germany, Germany_national_football_team, Nazi...   \n",
       "1  [European_Union, European_Boxing_Union, Europe...   \n",
       "2                                                 []   \n",
       "3  [United_Kingdom, Great_Britain, United_Kingdom...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.8896856, 0.021721, 0.0153527, 0.0140082, 0....  \n",
       "1  [0.9884673, 0.0026042, 0.0011161, 0.0009566, 0...  \n",
       "2                                                 []  \n",
       "3  [0.5200594, 0.2035661, 0.0822639, 0.0451168, 0...  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on manually selected sentence\n",
    "sentence_predictions = predictions[predictions['sentence_id'] == sentence_id].drop_duplicates(['full_mention', 'wikipedia_page_ID', 'sentence_id']).reset_index(drop=True)\n",
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Germany', 'European Union', 'Werner Zwingmann', 'Britain']\n"
     ]
    }
   ],
   "source": [
    "# Congruent Mention\n",
    "print(sentence_predictions['congruent_mentions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numerical for easier recursive logic later\n",
    "sentence_mention_nums = np.arange(len(sentence_predictions['congruent_mentions'][0]))\n",
    "sentence_mention_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate lists of vectors\n",
    "def get_candidate_pool_vectors(candidate_pool_titles, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return entity vectors from Wikipedia2Vec\n",
    "    Takes as input a list of page titles, representing the candidate pool\n",
    "    Normalizes each page title to match necessary input format\n",
    "    Returns entity vector or empty vector if no match\n",
    "    \"\"\"\n",
    "    # Track failed vector queries\n",
    "    no_vector_count = 0\n",
    "    candidate_pool_vectors = []\n",
    "    for candidate in candidate_pool_titles:\n",
    "        candidate = normalize_text(candidate)\n",
    "        try:\n",
    "            candidate_vectors = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            # Keep empty vector representation to maintain index locations\n",
    "            candidate_vectors = np.zeros(100)\n",
    "            no_vector_count += 1\n",
    "        candidate_pool_vectors.append(candidate_vectors)\n",
    "    \n",
    "    if len(candidate_pool_titles) == 0:\n",
    "        candidate_pool_vectors = [np.zeros(100), np.zeros(100), np.zeros(100)]\n",
    "    \n",
    "    if verbose: print(f\"Failed Wikipedia2Vec Entity Vector Queries: {no_vector_count}\")\n",
    "    return candidate_pool_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n"
     ]
    }
   ],
   "source": [
    "# Save vectors in dictionary\n",
    "vector_dict = {}\n",
    "\n",
    "# For each full mention we are analyzing in the contextual domain (i.e. sentence)\n",
    "for m in sentence_mention_nums:\n",
    "    \n",
    "    # Retrieve candidate pool titles\n",
    "    candidate_pool_titles = sentence_predictions['candidate_pool_titles'][m]\n",
    "    \n",
    "    # Convert candidate pool titles to candidate pool vectors\n",
    "    candidate_pool_vectors = get_candidate_pool_vectors(candidate_pool_titles, verbose=True)\n",
    "    \n",
    "    # Save candidate pool vectors to dictionary\n",
    "    vector_dict[m] = candidate_pool_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display vector_dict output\n",
    "print(vector_dict.keys())\n",
    "# Preview one candidate vector from a candidate pool vectors\n",
    "vector_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "1 2\n",
      "1 3\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# Calculate congruence metric for each candidate vector for each mention's candidate pool\n",
    "# This notebook uses cosine similarity as the congruence metric\n",
    "\n",
    "## Save congruence measurements in a two-level dictionary\n",
    "# Create first-level dictionary\n",
    "congruence_dict = {}\n",
    "\n",
    "# Always work low numbers to high\n",
    "m = 0\n",
    "while m < len(sentence_mention_nums)-1:\n",
    "    \n",
    "    # Save second-level congruence measurement dictionary\n",
    "    m_dict = {}\n",
    "    # Compare each mention against mentions after it\n",
    "    for n in sentence_mention_nums[m+1:]:\n",
    "        print(m, n)\n",
    "        # Calculate congruence measurement - cosine similarity\n",
    "        congruence_measurement = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "        # Save congruence measurement to second-level dictionary\n",
    "        m_dict[n] = congruence_measurement\n",
    "    \n",
    "    # Save second-level dictionary to first-level\n",
    "    congruence_dict[m] = m_dict\n",
    "    \n",
    "    # Increment mention\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2])\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Display congruence_dict output\n",
    "# This should be one less than congruent mention count, since we are comparing low to high\n",
    "# and thus don't compare the highest value to anything\n",
    "print(congruence_dict.keys())\n",
    "# Preview congruence matrix derived from comparing Mention 1 to Mention 2\n",
    "for k in congruence_dict.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Germany</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>germany</td>\n",
       "      <td>[11867, 250204, 21212, 12674, 33685, 662281, 1...</td>\n",
       "      <td>[183, 43310, 7318, 43287, 41304, 154408, 12031...</td>\n",
       "      <td>[Germany, Germany_national_football_team, Nazi...</td>\n",
       "      <td>[0.8896856, 0.021721, 0.0153527, 0.0140082, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>European Union</td>\n",
       "      <td>http://en.wikipedia.org/wiki/European_Union</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>European Union</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>european union</td>\n",
       "      <td>[9317, 1933156, 9317, 10890716, 276436, 265743...</td>\n",
       "      <td>[458, 1376407, 458, 185441, 208202, 319328, 36...</td>\n",
       "      <td>[European_Union, European_Boxing_Union, Europe...</td>\n",
       "      <td>[0.9884673, 0.0026042, 0.0011161, 0.0009566, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Werner Zwingmann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>werner zwingmann</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Britain</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>britain</td>\n",
       "      <td>[31717, 13530298, 152256, 158019, 13525, 4721,...</td>\n",
       "      <td>[145, 23666, 174193, 161885, 185103, 8680, 977...</td>\n",
       "      <td>[United_Kingdom, Great_Britain, United_Kingdom...</td>\n",
       "      <td>[0.5200594, 0.2035661, 0.0822639, 0.0451168, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                wikipedia_URL  \\\n",
       "0       B           Germany         http://en.wikipedia.org/wiki/Germany   \n",
       "1       B    European Union  http://en.wikipedia.org/wiki/European_Union   \n",
       "2       B  Werner Zwingmann                                          NaN   \n",
       "3       B           Britain  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            11867.0         Germany            2       0   \n",
       "1             9317.0  European Union            2       0   \n",
       "2                NaN             NaN            2       0   \n",
       "3            31717.0  United Kingdom            2       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [Germany, European Union, Werner Zwingmann, Br...           germany   \n",
       "1  [Germany, European Union, Werner Zwingmann, Br...    european union   \n",
       "2  [Germany, European Union, Werner Zwingmann, Br...  werner zwingmann   \n",
       "3  [Germany, European Union, Werner Zwingmann, Br...           britain   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [11867, 250204, 21212, 12674, 33685, 662281, 1...   \n",
       "1  [9317, 1933156, 9317, 10890716, 276436, 265743...   \n",
       "2                                                 []   \n",
       "3  [31717, 13530298, 152256, 158019, 13525, 4721,...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [183, 43310, 7318, 43287, 41304, 154408, 12031...   \n",
       "1  [458, 1376407, 458, 185441, 208202, 319328, 36...   \n",
       "2                                                 []   \n",
       "3  [145, 23666, 174193, 161885, 185103, 8680, 977...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [Germany, Germany_national_football_team, Nazi...   \n",
       "1  [European_Union, European_Boxing_Union, Europe...   \n",
       "2                                                 []   \n",
       "3  [United_Kingdom, Great_Britain, United_Kingdom...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.8896856, 0.021721, 0.0153527, 0.0140082, 0....  \n",
       "1  [0.9884673, 0.0026042, 0.0011161, 0.0009566, 0...  \n",
       "2                                                 []  \n",
       "3  [0.5200594, 0.2035661, 0.0822639, 0.0451168, 0...  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "# Pick mentions to compare\n",
    "men1 = np.random.choice(sentence_mention_nums[:-1])\n",
    "men2 = np.random.choice(sentence_mention_nums)\n",
    "while men1 == men2:\n",
    "    men2 = np.random.choice(sentence_mention_nums)\n",
    "men1, men2 = np.sort([men1, men2])\n",
    "print(men1, men2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_8f508dee_35ae_11eb_bf35_acde48001122row0_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row0_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row0_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row1_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row1_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row1_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row2_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row2_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row2_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row3_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row3_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row3_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row4_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row4_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row4_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row5_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row5_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row5_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row6_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row6_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row6_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row7_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row7_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row7_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row8_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row8_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row8_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row9_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row9_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row9_col2{\n",
       "            background:  skyblue;\n",
       "        }</style><table id=\"T_8f508dee_35ae_11eb_bf35_acde48001122\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row0_col2\" class=\"data row0 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row3_col0\" class=\"data row3 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row3_col1\" class=\"data row3 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row3_col2\" class=\"data row3 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row4_col0\" class=\"data row4 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row4_col1\" class=\"data row4 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row4_col2\" class=\"data row4 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row5_col0\" class=\"data row5 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row5_col1\" class=\"data row5 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row5_col2\" class=\"data row5 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row6_col0\" class=\"data row6 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row6_col1\" class=\"data row6 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row6_col2\" class=\"data row6 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row7_col0\" class=\"data row7 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row7_col1\" class=\"data row7 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row7_col2\" class=\"data row7 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row8_col0\" class=\"data row8 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row8_col1\" class=\"data row8 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row8_col2\" class=\"data row8 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row9_col0\" class=\"data row9 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row9_col1\" class=\"data row9 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row9_col2\" class=\"data row9 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f8bbee74f50>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congruence_test_df = pd.DataFrame(congruence_dict[men1][men2])\n",
    "max_num = max(np.max(congruence_test_df))\n",
    "congruence_test_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-2ee20f6677ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmatrix_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmatrix_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmax_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmatrix_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"background: skyblue\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_num\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "matrix_test = []\n",
    "for a in sentence_predictions['candidate_pool_likelihoods'][men1]:\n",
    "    row_test = []\n",
    "    for b in sentence_predictions['candidate_pool_likelihoods'][men2]:\n",
    "        row_test.append(np.mean([a, b]))\n",
    "    matrix_test.append(row_test)\n",
    "matrix_df = pd.DataFrame(matrix_test)\n",
    "max_num = max(np.max(matrix_df))\n",
    "matrix_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,0) (10,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-fe69072df3f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweighted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcongruence_test_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mweighted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweighted_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"background: skyblue\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_num\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,0) (10,3) "
     ]
    }
   ],
   "source": [
    "weighted_df = np.array(matrix_df) * np.array(congruence_test_df)\n",
    "weighted_df = pd.DataFrame(weighted_df)\n",
    "max_num = max(np.max(weighted_df))\n",
    "weighted_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  4\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "Length:  4\n",
      "1 2\n",
      "1 3\n",
      "Length:  4\n",
      "2 3\n",
      "Length:  4\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of logic to ensure unique mention congruence only calculated once\n",
    "m = 0\n",
    "while m < len(sentence_mention_nums):\n",
    "    print(\"Length: \", len(sentence_mention_nums))\n",
    "    for i in sentence_mention_nums[m+1:]:\n",
    "        print(m, i)\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 1), 0.8189154129945448)\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 2), 0.6412995855581739)\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((1, 1), 0.6947784687002414)\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((1, 1), 0.6947784687002414), 1, 2)\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "most_congruent_pair = (None, 0.0, 0, 0) # Most Congruent Candidates, Congruence Metric, Mention A, Mention B\n",
    "\n",
    "for m in sentence_mention_nums:\n",
    "    for n in sentence_mention_nums[m+1:]:\n",
    "        print(f\"Comparing {m} & {n}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m][n])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, m, n\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save max congruent pair estimates for each mention\n",
    "mention_predictions = {}\n",
    "mention_predictions[most_congruent_pair[1]] = most_congruent_pair[0][0][0]\n",
    "mention_predictions[most_congruent_pair[2]] = most_congruent_pair[0][0][1]\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have predictions for  dict_keys([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# With two mentions set in their predictions, we must filter the other congruent matrices to find the next most\n",
    "print(\"We have predictions for \", mention_predictions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(sentence_mention_nums) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "MAX: (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "MAX: (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 1), 0.8189154129945448)\n",
      "MAX: (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "MAX: (((1, 1), 0.8189154129945448), (0, 2))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((1, 1), 0.8189154129945448), (0, 2))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1, 0: 1}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "MAX: (((None, None), 0.0), (0, 0))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "MAX: (((0, 4), 0.6645409154870272), (1, 3))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 2), 0.6412995855581739)\n",
      "MAX: (((0, 6), 0.78401095), (2, 3))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((0, 6), 0.78401095), (2, 3))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1, 0: 1, 3: 6}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've predicted everything!\n"
     ]
    }
   ],
   "source": [
    "if len(mentions_remaining) == 0:\n",
    "    print(\"You've predicted everything!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
