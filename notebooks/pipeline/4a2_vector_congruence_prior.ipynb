{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congruence via Entity Vector Similarity\n",
    "\n",
    "In this notebook, we calculate the cosine similarity measure between vectors of each entity candidate. The vectors will be retrieved from Wikipedia2vec's pre-trained API, which creates vectors for the entire Wikipedia page. Comparing two vectors in this way thus lets us make a statement about similar pages and update our likelihood scores based on that.\n",
    "\n",
    "#### (i) Via Weights\n",
    "\n",
    "In Phase 4, we calculate congruence between candidates in pools for mentions in the same sentence. We then update the likelihood values from Phase 3 using congruence as a form of \"weights\". This should adjust our pool without eliminating the prior knowledge we have of the calculated similarity. In this notebook, we import prior knowledge generated from Anchor Link statistics and weight those with our numerical calculations from congruence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 21347120, 9477, 1882861, 3261189,...</td>\n",
       "      <td>[458, 46, 211593, 1396, 363404, 3327447, 40537...</td>\n",
       "      <td>['European_Union', 'Europe', 'Eu,_Seine-Mariti...</td>\n",
       "      <td>[0.9227799, 0.024651, 0.020196, 0.005346, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 11884, 152735, 21212, 12674, 290327, 1...</td>\n",
       "      <td>[183, 188, 42884, 7318, 43287, 141817, 181287,...</td>\n",
       "      <td>['Germany', 'German_language', 'Germans', 'Naz...</td>\n",
       "      <td>[0.4192066, 0.2893363, 0.1470461, 0.03832, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>british</td>\n",
       "      <td>[31717, 19097669, 13530298, 4721, 158019, 1522...</td>\n",
       "      <td>[145, 842438, 23666, 8680, 161885, 174193, 354...</td>\n",
       "      <td>['United_Kingdom', 'British_people', 'Great_Br...</td>\n",
       "      <td>[0.6101256, 0.1146913, 0.0681775, 0.0366451, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.5, 0.3, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.5, 0.3, 0.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention     full_mention                                wikipedia_URL  \\\n",
       "0       B               EU                                          NaN   \n",
       "1       B           German         http://en.wikipedia.org/wiki/Germany   \n",
       "2       B          British  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "3       B  Peter Blackburn                                          NaN   \n",
       "4       I  Peter Blackburn                                          NaN   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0                NaN             NaN            0       0   \n",
       "1            11867.0         Germany            0       0   \n",
       "2            31717.0  United Kingdom            0       0   \n",
       "3                NaN             NaN            1       0   \n",
       "4                NaN             NaN            1       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0                        ['EU', 'German', 'British']                eu   \n",
       "1                        ['EU', 'German', 'British']            german   \n",
       "2                        ['EU', 'German', 'British']           british   \n",
       "3  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "4  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [9317, 9239, 21347120, 9477, 1882861, 3261189,...   \n",
       "1  [11867, 11884, 152735, 21212, 12674, 290327, 1...   \n",
       "2  [31717, 19097669, 13530298, 4721, 158019, 1522...   \n",
       "3                      [56783206, 9643132, 56873217]   \n",
       "4                      [56783206, 9643132, 56873217]   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [458, 46, 211593, 1396, 363404, 3327447, 40537...   \n",
       "1  [183, 188, 42884, 7318, 43287, 141817, 181287,...   \n",
       "2  [145, 842438, 23666, 8680, 161885, 174193, 354...   \n",
       "3                       [2073954, 7172840, 26634508]   \n",
       "4                       [2073954, 7172840, 26634508]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  ['European_Union', 'Europe', 'Eu,_Seine-Mariti...   \n",
       "1  ['Germany', 'German_language', 'Germans', 'Naz...   \n",
       "2  ['United_Kingdom', 'British_people', 'Great_Br...   \n",
       "3  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "4  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.9227799, 0.024651, 0.020196, 0.005346, 0.00...  \n",
       "1  [0.4192066, 0.2893363, 0.1470461, 0.03832, 0.0...  \n",
       "2  [0.6101256, 0.1146913, 0.0681775, 0.0366451, 0...  \n",
       "3                                    [0.5, 0.3, 0.2]  \n",
       "4                                    [0.5, 0.3, 0.2]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base path to input\n",
    "preds_path = '../../predictions/'\n",
    "\n",
    "# Load data\n",
    "predictions = pd.read_csv(os.path.join(preds_path, \"anchortext_frequency.csv\"), delimiter=\",\")\n",
    "predictions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 21347120, 9477, 1882861, 3261189,...</td>\n",
       "      <td>[458, 46, 211593, 1396, 363404, 3327447, 40537...</td>\n",
       "      <td>['European_Union', 'Europe', 'Eu,_Seine-Mariti...</td>\n",
       "      <td>[0.9227799, 0.024651, 0.020196, 0.005346, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 11884, 152735, 21212, 12674, 290327, 1...</td>\n",
       "      <td>[183, 188, 42884, 7318, 43287, 141817, 181287,...</td>\n",
       "      <td>['Germany', 'German_language', 'Germans', 'Naz...</td>\n",
       "      <td>[0.4192066, 0.2893363, 0.1470461, 0.03832, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>british</td>\n",
       "      <td>[31717, 19097669, 13530298, 4721, 158019, 1522...</td>\n",
       "      <td>[145, 842438, 23666, 8680, 161885, 174193, 354...</td>\n",
       "      <td>['United_Kingdom', 'British_people', 'Great_Br...</td>\n",
       "      <td>[0.6101256, 0.1146913, 0.0681775, 0.0366451, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.5, 0.3, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.5, 0.3, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29307</th>\n",
       "      <td>B</td>\n",
       "      <td>England</td>\n",
       "      <td>http://en.wikipedia.org/wiki/England_national_...</td>\n",
       "      <td>9904.0</td>\n",
       "      <td>England national football team</td>\n",
       "      <td>5447</td>\n",
       "      <td>1009</td>\n",
       "      <td>['Leeds United', 'England', '1966 World Cup', ...</td>\n",
       "      <td>england</td>\n",
       "      <td>[9316, 9904, 759125, 691024, 990422, 407950, 2...</td>\n",
       "      <td>[21, 47762, 1321565, 378628, 3589698, 179876, ...</td>\n",
       "      <td>['England', 'England_national_football_team', ...</td>\n",
       "      <td>[0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29308</th>\n",
       "      <td>B</td>\n",
       "      <td>1966 World Cup</td>\n",
       "      <td>http://en.wikipedia.org/wiki/1966_FIFA_World_Cup</td>\n",
       "      <td>61629.0</td>\n",
       "      <td>1966 FIFA World Cup</td>\n",
       "      <td>5447</td>\n",
       "      <td>1009</td>\n",
       "      <td>['Leeds United', 'England', '1966 World Cup', ...</td>\n",
       "      <td>1966 world cup</td>\n",
       "      <td>[61629, 34657]</td>\n",
       "      <td>[134202, 1065912]</td>\n",
       "      <td>['1966_FIFA_World_Cup', '1966_FIFA_World_Cup_F...</td>\n",
       "      <td>[0.9846154, 0.0153846]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29309</th>\n",
       "      <td>I</td>\n",
       "      <td>1966 World Cup</td>\n",
       "      <td>http://en.wikipedia.org/wiki/1966_FIFA_World_Cup</td>\n",
       "      <td>61629.0</td>\n",
       "      <td>1966 FIFA World Cup</td>\n",
       "      <td>5447</td>\n",
       "      <td>1009</td>\n",
       "      <td>['Leeds United', 'England', '1966 World Cup', ...</td>\n",
       "      <td>1966 world cup</td>\n",
       "      <td>[61629, 34657]</td>\n",
       "      <td>[134202, 1065912]</td>\n",
       "      <td>['1966_FIFA_World_Cup', '1966_FIFA_World_Cup_F...</td>\n",
       "      <td>[0.9846154, 0.0153846]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29310</th>\n",
       "      <td>I</td>\n",
       "      <td>1966 World Cup</td>\n",
       "      <td>http://en.wikipedia.org/wiki/1966_FIFA_World_Cup</td>\n",
       "      <td>61629.0</td>\n",
       "      <td>1966 FIFA World Cup</td>\n",
       "      <td>5447</td>\n",
       "      <td>1009</td>\n",
       "      <td>['Leeds United', 'England', '1966 World Cup', ...</td>\n",
       "      <td>1966 world cup</td>\n",
       "      <td>[61629, 34657]</td>\n",
       "      <td>[134202, 1065912]</td>\n",
       "      <td>['1966_FIFA_World_Cup', '1966_FIFA_World_Cup_F...</td>\n",
       "      <td>[0.9846154, 0.0153846]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29311</th>\n",
       "      <td>B</td>\n",
       "      <td>Bobby</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Bobby_Charlton</td>\n",
       "      <td>4224.0</td>\n",
       "      <td>Bobby Charlton</td>\n",
       "      <td>5447</td>\n",
       "      <td>1009</td>\n",
       "      <td>['Leeds United', 'England', '1966 World Cup', ...</td>\n",
       "      <td>bobby</td>\n",
       "      <td>[3096383, 2150352, 43801345, 6403550, 38905119...</td>\n",
       "      <td>[679657, 888434, 18015644, 2090662, 12985953, ...</td>\n",
       "      <td>['Bobby_(2006_film)', 'Bobby_(1973_film)', 'Bo...</td>\n",
       "      <td>[0.1538462, 0.1538462, 0.088141, 0.0753205, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29312 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mention     full_mention  \\\n",
       "0           B               EU   \n",
       "1           B           German   \n",
       "2           B          British   \n",
       "3           B  Peter Blackburn   \n",
       "4           I  Peter Blackburn   \n",
       "...       ...              ...   \n",
       "29307       B          England   \n",
       "29308       B   1966 World Cup   \n",
       "29309       I   1966 World Cup   \n",
       "29310       I   1966 World Cup   \n",
       "29311       B            Bobby   \n",
       "\n",
       "                                           wikipedia_URL  wikipedia_page_ID  \\\n",
       "0                                                    NaN                NaN   \n",
       "1                   http://en.wikipedia.org/wiki/Germany            11867.0   \n",
       "2            http://en.wikipedia.org/wiki/United_Kingdom            31717.0   \n",
       "3                                                    NaN                NaN   \n",
       "4                                                    NaN                NaN   \n",
       "...                                                  ...                ...   \n",
       "29307  http://en.wikipedia.org/wiki/England_national_...             9904.0   \n",
       "29308   http://en.wikipedia.org/wiki/1966_FIFA_World_Cup            61629.0   \n",
       "29309   http://en.wikipedia.org/wiki/1966_FIFA_World_Cup            61629.0   \n",
       "29310   http://en.wikipedia.org/wiki/1966_FIFA_World_Cup            61629.0   \n",
       "29311        http://en.wikipedia.org/wiki/Bobby_Charlton             4224.0   \n",
       "\n",
       "                      wikipedia_title  sentence_id  doc_id  \\\n",
       "0                                 NaN            0       0   \n",
       "1                             Germany            0       0   \n",
       "2                      United Kingdom            0       0   \n",
       "3                                 NaN            1       0   \n",
       "4                                 NaN            1       0   \n",
       "...                               ...          ...     ...   \n",
       "29307  England national football team         5447    1009   \n",
       "29308             1966 FIFA World Cup         5447    1009   \n",
       "29309             1966 FIFA World Cup         5447    1009   \n",
       "29310             1966 FIFA World Cup         5447    1009   \n",
       "29311                  Bobby Charlton         5447    1009   \n",
       "\n",
       "                                      congruent_mentions norm_full_mention  \\\n",
       "0                            ['EU', 'German', 'British']                eu   \n",
       "1                            ['EU', 'German', 'British']            german   \n",
       "2                            ['EU', 'German', 'British']           british   \n",
       "3      ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "4      ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "...                                                  ...               ...   \n",
       "29307  ['Leeds United', 'England', '1966 World Cup', ...           england   \n",
       "29308  ['Leeds United', 'England', '1966 World Cup', ...    1966 world cup   \n",
       "29309  ['Leeds United', 'England', '1966 World Cup', ...    1966 world cup   \n",
       "29310  ['Leeds United', 'England', '1966 World Cup', ...    1966 world cup   \n",
       "29311  ['Leeds United', 'England', '1966 World Cup', ...             bobby   \n",
       "\n",
       "                                 candidate_pool_page_ids  \\\n",
       "0      [9317, 9239, 21347120, 9477, 1882861, 3261189,...   \n",
       "1      [11867, 11884, 152735, 21212, 12674, 290327, 1...   \n",
       "2      [31717, 19097669, 13530298, 4721, 158019, 1522...   \n",
       "3                          [56783206, 9643132, 56873217]   \n",
       "4                          [56783206, 9643132, 56873217]   \n",
       "...                                                  ...   \n",
       "29307  [9316, 9904, 759125, 691024, 990422, 407950, 2...   \n",
       "29308                                     [61629, 34657]   \n",
       "29309                                     [61629, 34657]   \n",
       "29310                                     [61629, 34657]   \n",
       "29311  [3096383, 2150352, 43801345, 6403550, 38905119...   \n",
       "\n",
       "                                 candidate_pool_item_ids  \\\n",
       "0      [458, 46, 211593, 1396, 363404, 3327447, 40537...   \n",
       "1      [183, 188, 42884, 7318, 43287, 141817, 181287,...   \n",
       "2      [145, 842438, 23666, 8680, 161885, 174193, 354...   \n",
       "3                           [2073954, 7172840, 26634508]   \n",
       "4                           [2073954, 7172840, 26634508]   \n",
       "...                                                  ...   \n",
       "29307  [21, 47762, 1321565, 378628, 3589698, 179876, ...   \n",
       "29308                                  [134202, 1065912]   \n",
       "29309                                  [134202, 1065912]   \n",
       "29310                                  [134202, 1065912]   \n",
       "29311  [679657, 888434, 18015644, 2090662, 12985953, ...   \n",
       "\n",
       "                                   candidate_pool_titles  \\\n",
       "0      ['European_Union', 'Europe', 'Eu,_Seine-Mariti...   \n",
       "1      ['Germany', 'German_language', 'Germans', 'Naz...   \n",
       "2      ['United_Kingdom', 'British_people', 'Great_Br...   \n",
       "3      ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "4      ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "...                                                  ...   \n",
       "29307  ['England', 'England_national_football_team', ...   \n",
       "29308  ['1966_FIFA_World_Cup', '1966_FIFA_World_Cup_F...   \n",
       "29309  ['1966_FIFA_World_Cup', '1966_FIFA_World_Cup_F...   \n",
       "29310  ['1966_FIFA_World_Cup', '1966_FIFA_World_Cup_F...   \n",
       "29311  ['Bobby_(2006_film)', 'Bobby_(1973_film)', 'Bo...   \n",
       "\n",
       "                              candidate_pool_likelihoods  \n",
       "0      [0.9227799, 0.024651, 0.020196, 0.005346, 0.00...  \n",
       "1      [0.4192066, 0.2893363, 0.1470461, 0.03832, 0.0...  \n",
       "2      [0.6101256, 0.1146913, 0.0681775, 0.0366451, 0...  \n",
       "3                                        [0.5, 0.3, 0.2]  \n",
       "4                                        [0.5, 0.3, 0.2]  \n",
       "...                                                  ...  \n",
       "29307  [0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...  \n",
       "29308                             [0.9846154, 0.0153846]  \n",
       "29309                             [0.9846154, 0.0153846]  \n",
       "29310                             [0.9846154, 0.0153846]  \n",
       "29311  [0.1538462, 0.1538462, 0.088141, 0.0753205, 0....  \n",
       "\n",
       "[29312 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Saved Candidate Pool\n",
    "\n",
    "Candidate pools when exported are typically stored as the string of a list. The below function parses the string back into a list with proper formatted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate that list is string\n",
    "type(predictions['candidate_pool_page_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse list as string\n",
    "def parse_list_string(list_string, value_type=int):\n",
    "    \n",
    "    parsed_list = []\n",
    "    \n",
    "    # If candidate pool is empty\n",
    "    if list_string == \"[]\" or isinstance(list_string, float):\n",
    "        pass\n",
    "    # Else parse\n",
    "    else:\n",
    "        # Parses lists of titles as strings\n",
    "        if value_type==str:\n",
    "            # Eliminate bracket and parenthesis on either side, split by comma pattern\n",
    "            parsed_list = re.split(\"', '|\\\", \\\"|', \\\"|\\\", \\'\", list_string[2:-2])\n",
    "\n",
    "        # Parses lists of IDs as ints\n",
    "        elif value_type==int:\n",
    "            # Eliminate brackets and convert each number from string to int\n",
    "            parsed_list = list(map(int, list_string[1:-1].split(', ')))\n",
    "        elif value_type==float:\n",
    "            # Eliminate brackets and convert each number from string to int\n",
    "            parsed_list = list(map(float, list_string[1:-1].split(', ')))\n",
    "            \n",
    "        \n",
    "    return parsed_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['European_Union',\n",
       " 'Europe',\n",
       " 'Eu,_Seine-Maritime',\n",
       " 'Europium',\n",
       " 'Citizenship_of_the_European_Union',\n",
       " 'United_Left_(Galicia)',\n",
       " 'EU_(group)',\n",
       " 'European_Union_law',\n",
       " 'Eu_station',\n",
       " 'Counts_of_Eu']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "# 0 is the hard one. See how some value is stored with '' and some with \"\". Unsure why.\n",
    "parse_list_string(predictions['candidate_pool_titles'][0], value_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9317,\n",
       " 9239,\n",
       " 21347120,\n",
       " 9477,\n",
       " 1882861,\n",
       " 3261189,\n",
       " 14024977,\n",
       " 276436,\n",
       " 27532324,\n",
       " 2538757]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "# 0 is the hard one. See how some value is stored with '' and some with \"\". Unsure why.\n",
    "parse_list_string(predictions['candidate_pool_page_ids'][0], value_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "parse_list_string(predictions['candidate_pool_page_ids'][13], value_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9227799,\n",
       " 0.024651,\n",
       " 0.020196,\n",
       " 0.005346,\n",
       " 0.002079,\n",
       " 0.001782,\n",
       " 0.001485,\n",
       " 0.001188,\n",
       " 0.001188,\n",
       " 0.000891]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "parse_list_string(predictions['candidate_pool_likelihoods'][0], value_type=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "After ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "Before [56783206, 9643132, 56873217]\n",
      "After [56783206, 9643132, 56873217]\n",
      "Before [2073954, 7172840, 26634508]\n",
      "After [2073954, 7172840, 26634508]\n",
      "Before ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(bishop)', 'Peter_Blackburn_(MP)']\n",
      "After ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(bishop)', 'Peter_Blackburn_(MP)']\n",
      "Before [0.5, 0.3, 0.2]\n",
      "After [0.5, 0.3, 0.2]\n",
      "CPU times: user 842 ms, sys: 76.8 ms, total: 919 ms\n",
      "Wall time: 920 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Apply defined function to entire dataframe for all candidate pool columns\n",
    "\n",
    "column = 'congruent_mentions'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=str)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "column = 'candidate_pool_page_ids'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=int)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_item_ids'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=int)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_titles'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=str)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])\n",
    "\n",
    "column = 'candidate_pool_likelihoods'\n",
    "print(\"Before\", predictions[column][3])\n",
    "parsed_candidate_pool = predictions[column].apply(parse_list_string, value_type=float)\n",
    "predictions[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions[column][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Entity Vectors from Wikipedia2Vec\n",
    "\n",
    "For provided wikipedia pages, we retrieve a representative entity vector from Wikipedia2vec. This involves passing the normalized title into their get_entity_vector() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package\n",
    "from wikipedia2vec import Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 114 ms, sys: 163 ms, total: 277 ms\n",
      "Wall time: 298 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load unzipped pkl file with word embeddings\n",
    "w2v = Wikipedia2Vec.load(\"../../embeddings/enwiki_20180420_100d.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Coverage of Candidate Pools in Wikipedia2vec\n",
    "\n",
    "We need to measure what percent of candidates in our candidate pools successfully return a vector from Wikipedia2vec. This should conceivably be 100% given we're passing known Wikipedia pages into this package trained over Wikipedia pages, but there may be some drop-off due to different creation dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text normalization function\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    We define normalized as:\n",
    "    - strip whitespace\n",
    "    - Spaces, not underlines\n",
    "    \"\"\"\n",
    "    return str(text).strip().replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29312/29312 [00:03<00:00, 8022.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia2vec returned an entity vector for 93.269% of 153,860 searches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over candidate pool titles to see what can be returned\n",
    "\n",
    "found_entity = 0\n",
    "searched_entity = 0\n",
    "\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    \n",
    "    # Retrieve candidate pool\n",
    "    candidate_pool = predictions['candidate_pool_titles'][i]\n",
    "    \n",
    "    # Query for each candidate\n",
    "    for candidate in candidate_pool:\n",
    "        # Normalize candidate title to form necessary to input into Wikipedia2vec\n",
    "        candidate = normalize_text(candidate)\n",
    "        \n",
    "        # Query Wikipedia2vec get_entity_vector()\n",
    "        try:\n",
    "            entity_vector = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            entity_vector = None\n",
    "        \n",
    "        # Check if result\n",
    "        if entity_vector is not None:\n",
    "            found_entity += 1\n",
    "        \n",
    "        # Increment count\n",
    "        searched_entity += 1\n",
    "\n",
    "print(f\"Wikipedia2vec returned an entity vector for {round(found_entity/searched_entity*100,3)}% of {searched_entity:,} searches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Congruence Metric between Congruent Entities\n",
    "\n",
    "First, let's get a sense for what the upper bound of congruent calculations might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the maximum number of congruent entities in a single sentence\n",
    "max(predictions['congruent_mentions'].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What is the distribution of congruent mention counts\n",
    "plt.figure(figsize=(20,5))\n",
    "predictions['congruent_mentions'].apply(len).hist(bins=50)\n",
    "plt.title(\"distribution of congruent mention counts\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Current Design Thinking\n",
    "\n",
    "In order to allow this to become recursive for N many tables, you will need to capture a congruence table for every candidate pool to every other candidate pool in a two-level dictionary so you can retrieve values using `matrix[3][1]`. This would entail duplication except to save that, we sort by value so you always search [small][large]. Saves us computation and storage. To see a sequential example of our logic, scroll to the end of this notebook. Below is our function-based implementation of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Iran</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iran</td>\n",
       "      <td>14653.0</td>\n",
       "      <td>Iran</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iran</td>\n",
       "      <td>[14653, 272865, 338883, 8294810, 46823116, 126...</td>\n",
       "      <td>[794, 184602, 207991, 1465546, 932162, 1042614...</td>\n",
       "      <td>[Iran, Iran_national_football_team, Pahlavi_dy...</td>\n",
       "      <td>[0.9792327, 0.0079465, 0.0016392, 0.0005319, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>turkey</td>\n",
       "      <td>[11125639, 743577, 72821, 24513964, 297071, 22...</td>\n",
       "      <td>[43, 483856, 43794, 4200953, 26844, 12560, 848...</td>\n",
       "      <td>[Turkey, Turkey_national_football_team, Turkey...</td>\n",
       "      <td>[0.9168911, 0.0269033, 0.0087393, 0.0043574, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iraq</td>\n",
       "      <td>7515928.0</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iraq</td>\n",
       "      <td>[7515928, 1039652, 5043324, 26215470, 2900620,...</td>\n",
       "      <td>[796, 186243, 545449, 3108185, 149805, 107802,...</td>\n",
       "      <td>[Iraq, Iraq_national_football_team, Iraq_War, ...</td>\n",
       "      <td>[0.8794507, 0.0341074, 0.0292135, 0.0119351, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Kurdish</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kurdish_people</td>\n",
       "      <td>17068.0</td>\n",
       "      <td>Kurdish people</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>[17068, 40316, 80777, 3821855, 4314285, 354232...</td>\n",
       "      <td>[12223, 36368, 41470, 1792998, 1117020, 121801...</td>\n",
       "      <td>[Kurds, Kurdish_languages, Kurdistan, Kurds_in...</td>\n",
       "      <td>[0.565625, 0.3114583, 0.0159722, 0.0142361, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention full_mention                                wikipedia_URL  \\\n",
       "0       B         Iran            http://en.wikipedia.org/wiki/Iran   \n",
       "1       B       Turkey                                          NaN   \n",
       "2       B         Iraq            http://en.wikipedia.org/wiki/Iraq   \n",
       "3       B      Kurdish  http://en.wikipedia.org/wiki/Kurdish_people   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            14653.0            Iran           90      11   \n",
       "1                NaN             NaN           90      11   \n",
       "2          7515928.0            Iraq           90      11   \n",
       "3            17068.0  Kurdish people           90      11   \n",
       "\n",
       "              congruent_mentions norm_full_mention  \\\n",
       "0  [Iran, Turkey, Iraq, Kurdish]              iran   \n",
       "1  [Iran, Turkey, Iraq, Kurdish]            turkey   \n",
       "2  [Iran, Turkey, Iraq, Kurdish]              iraq   \n",
       "3  [Iran, Turkey, Iraq, Kurdish]           kurdish   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [14653, 272865, 338883, 8294810, 46823116, 126...   \n",
       "1  [11125639, 743577, 72821, 24513964, 297071, 22...   \n",
       "2  [7515928, 1039652, 5043324, 26215470, 2900620,...   \n",
       "3  [17068, 40316, 80777, 3821855, 4314285, 354232...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [794, 184602, 207991, 1465546, 932162, 1042614...   \n",
       "1  [43, 483856, 43794, 4200953, 26844, 12560, 848...   \n",
       "2  [796, 186243, 545449, 3108185, 149805, 107802,...   \n",
       "3  [12223, 36368, 41470, 1792998, 1117020, 121801...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [Iran, Iran_national_football_team, Pahlavi_dy...   \n",
       "1  [Turkey, Turkey_national_football_team, Turkey...   \n",
       "2  [Iraq, Iraq_national_football_team, Iraq_War, ...   \n",
       "3  [Kurds, Kurdish_languages, Kurdistan, Kurds_in...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.9792327, 0.0079465, 0.0016392, 0.0005319, 0...  \n",
       "1  [0.9168911, 0.0269033, 0.0087393, 0.0043574, 0...  \n",
       "2  [0.8794507, 0.0341074, 0.0292135, 0.0119351, 0...  \n",
       "3  [0.565625, 0.3114583, 0.0159722, 0.0142361, 0....  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define testing sentence_id\n",
    "sentence_id = 90\n",
    "sentence_predictions = predictions[predictions['sentence_id'] == sentence_id].reset_index(drop=True)\n",
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iran', 'Turkey', 'Iraq', 'Kurdish']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_predictions['congruent_mentions'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to Create Modular Congruent Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate lists of vectors\n",
    "def get_candidate_pool_vectors(candidate_pool_titles, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return entity vectors from Wikipedia2Vec\n",
    "    Takes as input a list of page titles, representing the candidate pool\n",
    "    Normalizes each page title to match necessary input format\n",
    "    Returns entity vector or empty vector if no match\n",
    "    \"\"\"\n",
    "    # Track failed vector queries\n",
    "    no_vector_count = 0\n",
    "    candidate_pool_vectors = []\n",
    "    for candidate in candidate_pool_titles:\n",
    "        candidate = normalize_text(candidate)\n",
    "        if verbose: print(candidate)\n",
    "        try:\n",
    "            candidate_vectors = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            # Keep empty vector representation to maintain index locations\n",
    "            candidate_vectors = np.zeros(100)\n",
    "            no_vector_count += 1\n",
    "        candidate_pool_vectors.append(candidate_vectors)\n",
    "    \n",
    "    # Handle case where candidate pool is empty from Phase 3\n",
    "    # Add arbitrarily chosen 3 arrays of zeros\n",
    "    if len(candidate_pool_titles) == 0:\n",
    "        candidate_pool_vectors = [np.zeros(100), np.zeros(100), np.zeros(100)]\n",
    "    \n",
    "    if verbose: print(f\"Failed Wikipedia2Vec Entity Vector Queries: {no_vector_count}\")\n",
    "    return candidate_pool_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to retrieve entity vectors\n",
    "def create_entity_vector_dict(sentence_mention_ids, sentence_predictions, verbose=False):\n",
    "    \"\"\"\n",
    "    Function iterates over a provided list of congruent mentions,\n",
    "    finds the associated candidate pool for each mention\n",
    "    and returns the candidate pool vector\n",
    "    \"\"\"\n",
    "    # Save vectors in dictionary\n",
    "    vector_dict = {}\n",
    "    \n",
    "    # For each full mention we are analyzing in the contextual domain\n",
    "    for m in sentence_mention_ids:\n",
    "        \n",
    "        # Retrieve candidate pool titles\n",
    "        candidate_pool_titles = sentence_predictions['candidate_pool_titles'][m]\n",
    "        if verbose: print(candidate_pool_titles)\n",
    "        \n",
    "        # Convert candidate pool titles to candidate pool vectors\n",
    "        candidate_pool_vectors = get_candidate_pool_vectors(candidate_pool_titles, verbose=verbose)\n",
    "        \n",
    "        # Save candidate pool vectors to dictionary\n",
    "        vector_dict[m] = candidate_pool_vectors\n",
    "    \n",
    "    if verbose:\n",
    "        print(vector_dict.keys())\n",
    "        for k in vector_dict.keys():\n",
    "            print(len(vector_dict[k]))\n",
    "    return vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate congruent metric for each candidate in every mention's candidate pool\n",
    "def get_congruence_dict(vector_dict, sentence_mention_ids, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return congruence metric calculations between all candidates for all mentions\n",
    "    Input:\n",
    "    - vector_dict: todo this could be generalized to allow comparison between text/ints/vectors\n",
    "    - Sentence Mentions Numerical Representation: Integers representing congruent mentions in a context domain\n",
    "    Outputs:\n",
    "    - Dictionary with congruence metric calculations for everyone\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Save congruence metrics in a two-level dictionary\n",
    "    # Create first-level dictionary to be returned\n",
    "    congruence_dict = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level congruence metric dictionary\n",
    "        m_dict = {}\n",
    "\n",
    "        # Compare eaech mention against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            if verbose: print(f\"Comparing mentions {m} & {n}\")\n",
    "            # Calculate congruence metric - cosine similarity\n",
    "            congruence_metric = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "            # Save congruence metric to second-level dictionary\n",
    "            m_dict[n] = congruence_metric\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        congruence_dict[m] = m_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "    \n",
    "#     if verbose: print(congruence_dict)\n",
    "    return congruence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to combine mention probabilities\n",
    "def combine_probabilities(mid_1, mid_2, sentence_probabilities):\n",
    "    \"\"\"\n",
    "    Function takes two mentions (numerically represented),\n",
    "    finds their candidate pool probabilities,\n",
    "    and combines them using the chosen logic,\n",
    "    returning a matrix of combined pair-wise probabilities\n",
    "    \"\"\"\n",
    "    # Prepare matrix to return\n",
    "    weights_matrix = []\n",
    "    \n",
    "    # Combine every candidate's probability for one mention with each for the second\n",
    "    for a in sentence_probabilities[mid_1]:\n",
    "        weights_row = []\n",
    "        for b in sentence_probabilities[mid_2]:\n",
    "            \n",
    "            ## Combination logic\n",
    "            weights_row.append(np.mean([a, b])) # Take mean of two prior probabilities\n",
    "        \n",
    "        weights_matrix.append(weights_row)\n",
    "        \n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create a weighted dictionary from priors\n",
    "def create_weighted_dict(sentence_mention_ids, sentence_predictions, verbose=False):\n",
    "    \"\"\"\n",
    "    Function iterates over a provided list of congruent mentions,\n",
    "    finds the associated candidate pool prior probabilities for each mention\n",
    "    and returns a dictionary for the combined probability of every pair of candidates\n",
    "    \"\"\"\n",
    "    # Save weights in dictionary\n",
    "    weights_dict = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level weights dictionary\n",
    "        w_dict = {}\n",
    "        \n",
    "        # Compare each mention probabilities against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            if verbose: print(f\"Comparing mentions {m} & {n}\")\n",
    "            # Return candidate pool probabilities - todo replace likelihood with probabilities\n",
    "            weights_matrix = combine_probabilities(m, n, sentence_predictions['candidate_pool_likelihoods'])\n",
    "            if not weights_matrix: # Handles error where candidate pool is empty\n",
    "                weights_matrix = [0.0]\n",
    "            # Save weights matrix to second-level dictionary\n",
    "            w_dict[n] = weights_matrix\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        weights_dict[m] = w_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "                \n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to combine congruence and prior weights\n",
    "def combine_congruence_priors(sentence_mention_ids, congruence_dict, weights_dict, verbose=False):\n",
    "    \"\"\"\n",
    "    Function takes two dictionaries representing recursive metric calculations,\n",
    "    Multiplies them element-wise together,\n",
    "    Returns the updated table\n",
    "    \"\"\"\n",
    "   # Combine congruence with prior weights in first-level dictionary\n",
    "    weighted_congruence = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level dictionary\n",
    "        w_c_dict = {}\n",
    "        \n",
    "        # Compare each mention probabilities against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            try:\n",
    "                weighted_table = congruence_dict[m][n] * np.array(weights_dict[m][n])\n",
    "            except ValueError: # Handles error when candidate pool is empty\n",
    "                weighted_table = congruence_dict[m][n]\n",
    "            w_c_dict[n] = weighted_table\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        weighted_congruence[m] = w_c_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "    \n",
    "    return weighted_congruence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standardize on form lookup always row then column\n",
    "def get_most_congruent_pair(congruence_matrix, verbose=False):\n",
    "    \"\"\"\n",
    "    This function takes a congruence matrix and returns the indices\n",
    "    of the two most congruent candidates using your chosen metric.\n",
    "    These indices can be plugged back into the candidate pool lists\n",
    "    to determine which candidates are most similar.\n",
    "    \"\"\"\n",
    "    # Get max values for every row\n",
    "    max_row_values = congruence_matrix.max(axis=1)\n",
    "    max_row_idxs = congruence_matrix.argmax(axis=1)\n",
    "    \n",
    "    # Get overall max value and the row it is in\n",
    "    max_value = max_row_values.max()\n",
    "    max_row_idx = max_row_values.argmax()\n",
    "    \n",
    "    # Get column max value is in\n",
    "    max_column_idx = max_row_idxs[max_row_idx]\n",
    "    \n",
    "    return (max_row_idx, max_column_idx), max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the most congruent pair amongst remaining mentions\n",
    "def find_most_congruent_pair(mention_predictions, mentions_remaining, congruence_dict, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to search the congruence matrices for mentions without predictions\n",
    "    and return the most congruent pair of candidates and associated mentions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with empty most_congruent_pair\n",
    "    # (Candidate Pair, Congruence Metric), (Mention A, Mention B)\n",
    "    most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "    \n",
    "    # Assess whether first pass or recursive pass\n",
    "    if len(mention_predictions) == 0:\n",
    "\n",
    "        # First pass        \n",
    "        for m in mentions_remaining:\n",
    "            for n in mentions_remaining[m+1:]:\n",
    "\n",
    "                # Get most congruent pair in one matrix\n",
    "                congruent_pair = get_most_congruent_pair(congruence_dict[m][n], verbose=verbose)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Comparing {m} & {n}\")\n",
    "                    print(\"Congruent Pair: \", congruent_pair)\n",
    "                    print(\"Current Most Congruent: \", most_congruent_pair)\n",
    "                \n",
    "                if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "                    # Save most congruent candidate pair and mentions\n",
    "                    most_congruent_pair = congruent_pair, (m, n)\n",
    "\n",
    "    elif len(mention_predictions) > 0:\n",
    "        \n",
    "        # Second+, recursive pass\n",
    "        for m in mention_predictions.keys():\n",
    "            for n in mentions_remaining:\n",
    "                \n",
    "                # Becauase we always assume search small mention to large to save computation/storage,\n",
    "                # we must sort incrementing variables to be in increasing order for query\n",
    "                m_tmp, n_tmp = np.sort((m, n))\n",
    "                \n",
    "                # Get most congruent pair in one matrix\n",
    "                congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp], verbose=verbose)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "                    print(\"Congruent Pair: \", congruent_pair)\n",
    "                    print(\"Current Most Congruent: \", most_congruent_pair)\n",
    "                    \n",
    "                if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "                    # Save most congruent candidate pair and mentions\n",
    "                    most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "                \n",
    "    if verbose: print(\"Final Most Congruent Pair: \", most_congruent_pair)\n",
    "    return most_congruent_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congruent Predictions Function\n",
    "\n",
    "This is our main function that takes a sentence ID, calculates congruence for all candidates, updates the prior likelihood from Phase 3 with that congruence and selects predictions iteratively based on that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate congruent predictions\n",
    "def get_congruent_predictions(sentence_id, dataframe, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to calculate congruence metrics over a set of entity full mentions\n",
    "    and return the predicted candidates based on the congruent metric\n",
    "    Input:\n",
    "    - todo Should I pass the dataframe as well?\n",
    "    - Sentence ID used to filter dataframe\n",
    "    Output:\n",
    "    - Prediction for each entity mention\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to dataframe representing single sentence\n",
    "    # Drop duplicates necessary for sentences with the same mention included twice\n",
    "    sentence_predictions = dataframe[dataframe['sentence_id'] == sentence_id]\\\n",
    "                        .drop_duplicates(['full_mention', 'wikipedia_URL', 'wikipedia_page_ID', 'wikipedia_title'])\\\n",
    "                        .reset_index(drop=True)\n",
    "    if verbose: display(sentence_predictions)\n",
    "    \n",
    "    # Define numerical representation of congruent mention list\n",
    "    sentence_congruent_mentions = sentence_predictions['congruent_mentions'][0]\n",
    "    sentence_mention_ids = np.arange(len(sentence_congruent_mentions))\n",
    "    if verbose:\n",
    "        print(\"Congruent Mentions: \", sentence_congruent_mentions)\n",
    "        print(\"Congruent Mentions as numbers: \", sentence_mention_ids)\n",
    "    \n",
    "    # Retrieve dictionary of candidate pool vectors for each mention\n",
    "    vectors_dict = create_entity_vector_dict(sentence_mention_ids, sentence_predictions, verbose=verbose)\n",
    "    if verbose: print(\"Mentions with Vectors: \", vectors_dict.keys())\n",
    "    \n",
    "    # Calculate congruence metric for each candidate vector for each mention's candidate pool\n",
    "    # This notebook uses cosine similarity as the congruence metric\n",
    "    congruence_dict = get_congruence_dict(vectors_dict, sentence_mention_ids, verbose=verbose)\n",
    "    if verbose: print(\"First-Level Congruence Keys: \", congruence_dict.keys())\n",
    "    # This should be one less than congruent mention count, since we are comparing low to high\n",
    "    # and thus don't compare the highest value to anything\n",
    "    \n",
    "    # Calculate weights matric for every pairwise combination of candidates for every mention\n",
    "    weights_dict = create_weighted_dict(sentence_mention_ids, sentence_predictions, verbose=verbose)\n",
    "    \n",
    "    # Combine congruence with prior weights\n",
    "    weighted_congruence = combine_congruence_priors(sentence_mention_ids, congruence_dict, weights_dict, verbose=verbose)\n",
    "    \n",
    "    # Create predictions dictionary\n",
    "    mention_predictions = {}\n",
    "    \n",
    "    # Create copy of sentence_mention_ids to iterate through\n",
    "    mentions_remaining = sentence_mention_ids.copy()\n",
    "    \n",
    "    # Iterate through congruent entity mentions to retrieve predictions\n",
    "    # where a prediction is the most congruent candidate between two mentions\n",
    "    while len(mentions_remaining) > 0:\n",
    "        \n",
    "        # Analyze congruence matrices to identify the most congruent pair\n",
    "        most_congruent_pair = find_most_congruent_pair(mention_predictions, mentions_remaining, weighted_congruence, verbose=verbose)\n",
    "        \n",
    "        # Save most congrent pair prediction for associated mentions\n",
    "        if len(mention_predictions) == 0:\n",
    "            # First pass\n",
    "            if verbose: print(most_congruent_pair)\n",
    "            # Handles error when nearly all candidate pools are empty\n",
    "            if most_congruent_pair == (((None, None), 0.0), (0, 0)):\n",
    "                mention_predictions[0] = 0\n",
    "            else:\n",
    "                mention_predictions[most_congruent_pair[1][0]] = most_congruent_pair[0][0][0]\n",
    "                mention_predictions[most_congruent_pair[1][1]] = most_congruent_pair[0][0][1]\n",
    "        elif len(mention_predictions) > 0:\n",
    "            # Second_, recursive pass\n",
    "            \n",
    "            # Find new mention you're predicting for\n",
    "            try:\n",
    "                # The number left over in the mention tuple if you remove anything in the prediction dict\n",
    "                new_mention_num = most_congruent_pair[1].index(\\\n",
    "                                                               list(set(most_congruent_pair[1])\\\n",
    "                                                                    - set(mention_predictions.keys())))\n",
    "\n",
    "                # Save new prediction\n",
    "                mention_predictions[most_congruent_pair[1][new_mention_num]] = most_congruent_pair[0][0][new_mention_num]\n",
    "            except ValueError:\n",
    "                for mention in mentions_remaining:\n",
    "                    mention_predictions[mention] = 0 # Zero produces error in final section to produce None prediction\n",
    "            \n",
    "        # Update remaining mentions to mentions without a prediction stored in the dictionary\n",
    "        mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "        if verbose: print(mentions_remaining, mention_predictions.keys())\n",
    "    \n",
    "    if verbose: print(mention_predictions)\n",
    "    \n",
    "    # Use mention predictions to return titles\n",
    "    readable_predictions = {}\n",
    "    for k, v in mention_predictions.items():\n",
    "        if verbose: print(k, v)\n",
    "        readable_key = sentence_congruent_mentions[k]\n",
    "        try:\n",
    "            readable_value = sentence_predictions['candidate_pool_titles'][k][v]\n",
    "            readable_id = sentence_predictions['candidate_pool_page_ids'][k][v]\n",
    "        except IndexError: # Handles case where no candidate pool was provided from Phase 3\n",
    "            readable_value = None \n",
    "            readable_id = None\n",
    "        except TypeError:\n",
    "            # Handles case where no congruence can be calculated\n",
    "            # Either due to one mention in sentence or two mentions but one with no candidate pool\n",
    "            readable_value = sentence_predictions['candidate_pool_titles'][0][0] # Just return top value from Phase 3\n",
    "            readable_id = sentence_predictions['candidate_pool_page_ids'][0][0]\n",
    "        if verbose: print(readable_key, readable_value, readable_id)\n",
    "        readable_predictions[readable_key] = (readable_value, readable_id)\n",
    "    \n",
    "    # Output dictionary with predictions for each entity mention based on congruence\n",
    "    return readable_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Iran</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iran</td>\n",
       "      <td>14653.0</td>\n",
       "      <td>Iran</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iran</td>\n",
       "      <td>[14653, 272865, 338883, 8294810, 46823116, 126...</td>\n",
       "      <td>[794, 184602, 207991, 1465546, 932162, 1042614...</td>\n",
       "      <td>[Iran, Iran_national_football_team, Pahlavi_dy...</td>\n",
       "      <td>[0.9792327, 0.0079465, 0.0016392, 0.0005319, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>turkey</td>\n",
       "      <td>[11125639, 743577, 72821, 24513964, 297071, 22...</td>\n",
       "      <td>[43, 483856, 43794, 4200953, 26844, 12560, 848...</td>\n",
       "      <td>[Turkey, Turkey_national_football_team, Turkey...</td>\n",
       "      <td>[0.9168911, 0.0269033, 0.0087393, 0.0043574, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iraq</td>\n",
       "      <td>7515928.0</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iraq</td>\n",
       "      <td>[7515928, 1039652, 5043324, 26215470, 2900620,...</td>\n",
       "      <td>[796, 186243, 545449, 3108185, 149805, 107802,...</td>\n",
       "      <td>[Iraq, Iraq_national_football_team, Iraq_War, ...</td>\n",
       "      <td>[0.8794507, 0.0341074, 0.0292135, 0.0119351, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Kurdish</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kurdish_people</td>\n",
       "      <td>17068.0</td>\n",
       "      <td>Kurdish people</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>[17068, 40316, 80777, 3821855, 4314285, 354232...</td>\n",
       "      <td>[12223, 36368, 41470, 1792998, 1117020, 121801...</td>\n",
       "      <td>[Kurds, Kurdish_languages, Kurdistan, Kurds_in...</td>\n",
       "      <td>[0.565625, 0.3114583, 0.0159722, 0.0142361, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention full_mention                                wikipedia_URL  \\\n",
       "0       B         Iran            http://en.wikipedia.org/wiki/Iran   \n",
       "1       B       Turkey                                          NaN   \n",
       "2       B         Iraq            http://en.wikipedia.org/wiki/Iraq   \n",
       "3       B      Kurdish  http://en.wikipedia.org/wiki/Kurdish_people   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            14653.0            Iran           90      11   \n",
       "1                NaN             NaN           90      11   \n",
       "2          7515928.0            Iraq           90      11   \n",
       "3            17068.0  Kurdish people           90      11   \n",
       "\n",
       "              congruent_mentions norm_full_mention  \\\n",
       "0  [Iran, Turkey, Iraq, Kurdish]              iran   \n",
       "1  [Iran, Turkey, Iraq, Kurdish]            turkey   \n",
       "2  [Iran, Turkey, Iraq, Kurdish]              iraq   \n",
       "3  [Iran, Turkey, Iraq, Kurdish]           kurdish   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [14653, 272865, 338883, 8294810, 46823116, 126...   \n",
       "1  [11125639, 743577, 72821, 24513964, 297071, 22...   \n",
       "2  [7515928, 1039652, 5043324, 26215470, 2900620,...   \n",
       "3  [17068, 40316, 80777, 3821855, 4314285, 354232...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [794, 184602, 207991, 1465546, 932162, 1042614...   \n",
       "1  [43, 483856, 43794, 4200953, 26844, 12560, 848...   \n",
       "2  [796, 186243, 545449, 3108185, 149805, 107802,...   \n",
       "3  [12223, 36368, 41470, 1792998, 1117020, 121801...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [Iran, Iran_national_football_team, Pahlavi_dy...   \n",
       "1  [Turkey, Turkey_national_football_team, Turkey...   \n",
       "2  [Iraq, Iraq_national_football_team, Iraq_War, ...   \n",
       "3  [Kurds, Kurdish_languages, Kurdistan, Kurds_in...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.9792327, 0.0079465, 0.0016392, 0.0005319, 0...  \n",
       "1  [0.9168911, 0.0269033, 0.0087393, 0.0043574, 0...  \n",
       "2  [0.8794507, 0.0341074, 0.0292135, 0.0119351, 0...  \n",
       "3  [0.565625, 0.3114583, 0.0159722, 0.0142361, 0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congruent Mentions:  ['Iran', 'Turkey', 'Iraq', 'Kurdish']\n",
      "Congruent Mentions as numbers:  [0 1 2 3]\n",
      "['Iran', 'Iran_national_football_team', 'Pahlavi_dynasty', \"Iran_men's_national_basketball_team\", \"Iran_men's_national_volleyball_team\", 'Football_Federation_Islamic_Republic_of_Iran', 'Iran_national_beach_soccer_team', 'Qajar_dynasty', 'COVID-19_pandemic_in_Iran', 'Iran_national_futsal_team']\n",
      "Iran\n",
      "Iran national football team\n",
      "Pahlavi dynasty\n",
      "Iran men's national basketball team\n",
      "Iran men's national volleyball team\n",
      "Football Federation Islamic Republic of Iran\n",
      "Iran national beach soccer team\n",
      "Qajar dynasty\n",
      "COVID-19 pandemic in Iran\n",
      "Iran national futsal team\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "['Turkey', 'Turkey_national_football_team', 'Turkey_(bird)', 'Turkey_as_food', 'Wild_turkey', 'Ottoman_Empire', 'Domestic_turkey', \"Turkey_men's_national_basketball_team\", 'Turkey_national_under-21_football_team', 'Turkey_national_cricket_team']\n",
      "Turkey\n",
      "Turkey national football team\n",
      "Turkey (bird)\n",
      "Turkey as food\n",
      "Wild turkey\n",
      "Ottoman Empire\n",
      "Domestic turkey\n",
      "Turkey men's national basketball team\n",
      "Turkey national under-21 football team\n",
      "Turkey national cricket team\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "['Iraq', 'Iraq_national_football_team', 'Iraq_War', \"Ba'athist_Iraq\", 'Kingdom_of_Iraq', '2003_invasion_of_Iraq', 'Lower_Mesopotamia', 'Mandatory_Iraq', 'Iraq_national_under-23_football_team', 'Anglo-Iraqi_War']\n",
      "Iraq\n",
      "Iraq national football team\n",
      "Iraq War\n",
      "Ba'athist Iraq\n",
      "Kingdom of Iraq\n",
      "2003 invasion of Iraq\n",
      "Lower Mesopotamia\n",
      "Mandatory Iraq\n",
      "Iraq national under-23 football team\n",
      "Anglo-Iraqi War\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['Kurds', 'Kurdish_languages', 'Kurdistan', 'Kurds_in_Syria', 'Kurds_in_Turkey', 'Kurds_in_Iraq', 'Iraqi_Kurdistan', 'Kurdish_population', 'Kurdish_alphabets', 'Kurdish_music']\n",
      "Kurds\n",
      "Kurdish languages\n",
      "Kurdistan\n",
      "Kurds in Syria\n",
      "Kurds in Turkey\n",
      "Kurds in Iraq\n",
      "Iraqi Kurdistan\n",
      "Kurdish population\n",
      "Kurdish alphabets\n",
      "Kurdish music\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "dict_keys([0, 1, 2, 3])\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "Mentions with Vectors:  dict_keys([0, 1, 2, 3])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 0 & 3\n",
      "Comparing mentions 1 & 2\n",
      "Comparing mentions 1 & 3\n",
      "Comparing mentions 2 & 3\n",
      "First-Level Congruence Keys:  dict_keys([0, 1, 2])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 0 & 3\n",
      "Comparing mentions 1 & 2\n",
      "Comparing mentions 1 & 3\n",
      "Comparing mentions 2 & 3\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((0, 0), 0.6162854580742424)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((0, 0), 0.6333118141361613)\n",
      "Current Most Congruent:  (((0, 0), 0.6162854580742424), (0, 1))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 0), 0.47165989079869114)\n",
      "Current Most Congruent:  (((0, 0), 0.6333118141361613), (0, 2))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.5048887349405616)\n",
      "Current Most Congruent:  (((0, 0), 0.6333118141361613), (0, 2))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 0), 0.4337822074951751)\n",
      "Current Most Congruent:  (((0, 0), 0.6333118141361613), (0, 2))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.49357584443920255)\n",
      "Current Most Congruent:  (((0, 0), 0.6333118141361613), (0, 2))\n",
      "Final Most Congruent Pair:  (((0, 0), 0.6333118141361613), (0, 2))\n",
      "(((0, 0), 0.6333118141361613), (0, 2))\n",
      "[1, 3] dict_keys([0, 2])\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((0, 0), 0.6162854580742424)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 0), 0.47165989079869114)\n",
      "Current Most Congruent:  (((0, 0), 0.6162854580742424), (0, 1))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.5048887349405616)\n",
      "Current Most Congruent:  (((0, 0), 0.6162854580742424), (0, 1))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.49357584443920255)\n",
      "Current Most Congruent:  (((0, 0), 0.6162854580742424), (0, 1))\n",
      "Final Most Congruent Pair:  (((0, 0), 0.6162854580742424), (0, 1))\n",
      "[3] dict_keys([0, 2, 1])\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 0), 0.47165989079869114)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.49357584443920255)\n",
      "Current Most Congruent:  (((0, 0), 0.47165989079869114), (0, 3))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 0), 0.4337822074951751)\n",
      "Current Most Congruent:  (((0, 0), 0.49357584443920255), (2, 3))\n",
      "Final Most Congruent Pair:  (((0, 0), 0.49357584443920255), (2, 3))\n",
      "[] dict_keys([0, 2, 1, 3])\n",
      "{0: 0, 2: 0, 1: 0, 3: 0}\n",
      "0 0\n",
      "Iran Iran 14653\n",
      "2 0\n",
      "Iraq Iraq 7515928\n",
      "1 0\n",
      "Turkey Turkey 11125639\n",
      "3 0\n",
      "Kurdish Kurds 17068\n",
      "{'Iran': ('Iran', 14653), 'Iraq': ('Iraq', 7515928), 'Turkey': ('Turkey', 11125639), 'Kurdish': ('Kurds', 17068)}\n",
      "CPU times: user 48.6 ms, sys: 8.74 ms, total: 57.3 ms\n",
      "Wall time: 56.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test out function\n",
    "congruent_predictions = get_congruent_predictions(sentence_id=90, dataframe=predictions, verbose=True)\n",
    "print(congruent_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Iran</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iran</td>\n",
       "      <td>14653.0</td>\n",
       "      <td>Iran</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iran</td>\n",
       "      <td>[14653, 272865, 338883, 8294810, 46823116, 126...</td>\n",
       "      <td>[794, 184602, 207991, 1465546, 932162, 1042614...</td>\n",
       "      <td>[Iran, Iran_national_football_team, Pahlavi_dy...</td>\n",
       "      <td>[0.9790373, 0.0079465, 0.0016392, 0.0005319, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>turkey</td>\n",
       "      <td>[11125639, 743577, 72821, 24513964, 297071, 22...</td>\n",
       "      <td>[43, 483856, 43794, 4200953, 26844, 12560, 848...</td>\n",
       "      <td>[Turkey, Turkey_national_football_team, Turkey...</td>\n",
       "      <td>[0.9168911, 0.0269033, 0.0082007, 0.0038923, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Iraq</td>\n",
       "      <td>7515928.0</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>iraq</td>\n",
       "      <td>[7515928, 1039652, 5043324, 26215470, 2900620,...</td>\n",
       "      <td>[796, 186243, 545449, 3108185, 149805, 107802,...</td>\n",
       "      <td>[Iraq, Iraq_national_football_team, Iraq_War, ...</td>\n",
       "      <td>[0.8794007, 0.0341074, 0.0292135, 0.0119351, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Kurdish</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kurdish_people</td>\n",
       "      <td>17068.0</td>\n",
       "      <td>Kurdish people</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>[Iran, Turkey, Iraq, Kurdish]</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>[17068, 40316, 80777, 3821855, 4314285, 354232...</td>\n",
       "      <td>[12223, 36368, 41470, 1792998, 1117020, 121801...</td>\n",
       "      <td>[Kurds, Kurdish_languages, Kurdistan, Kurds_in...</td>\n",
       "      <td>[0.565625, 0.3114583, 0.0159722, 0.0142361, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention full_mention                                wikipedia_URL  \\\n",
       "0       B         Iran            http://en.wikipedia.org/wiki/Iran   \n",
       "1       B       Turkey                                          NaN   \n",
       "2       B         Iraq            http://en.wikipedia.org/wiki/Iraq   \n",
       "3       B      Kurdish  http://en.wikipedia.org/wiki/Kurdish_people   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            14653.0            Iran           90      11   \n",
       "1                NaN             NaN           90      11   \n",
       "2          7515928.0            Iraq           90      11   \n",
       "3            17068.0  Kurdish people           90      11   \n",
       "\n",
       "              congruent_mentions norm_full_mention  \\\n",
       "0  [Iran, Turkey, Iraq, Kurdish]              iran   \n",
       "1  [Iran, Turkey, Iraq, Kurdish]            turkey   \n",
       "2  [Iran, Turkey, Iraq, Kurdish]              iraq   \n",
       "3  [Iran, Turkey, Iraq, Kurdish]           kurdish   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [14653, 272865, 338883, 8294810, 46823116, 126...   \n",
       "1  [11125639, 743577, 72821, 24513964, 297071, 22...   \n",
       "2  [7515928, 1039652, 5043324, 26215470, 2900620,...   \n",
       "3  [17068, 40316, 80777, 3821855, 4314285, 354232...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [794, 184602, 207991, 1465546, 932162, 1042614...   \n",
       "1  [43, 483856, 43794, 4200953, 26844, 12560, 848...   \n",
       "2  [796, 186243, 545449, 3108185, 149805, 107802,...   \n",
       "3  [12223, 36368, 41470, 1792998, 1117020, 121801...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [Iran, Iran_national_football_team, Pahlavi_dy...   \n",
       "1  [Turkey, Turkey_national_football_team, Turkey...   \n",
       "2  [Iraq, Iraq_national_football_team, Iraq_War, ...   \n",
       "3  [Kurds, Kurdish_languages, Kurdistan, Kurds_in...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.9790373, 0.0079465, 0.0016392, 0.0005319, 0...  \n",
       "1  [0.9168911, 0.0269033, 0.0082007, 0.0038923, 0...  \n",
       "2  [0.8794007, 0.0341074, 0.0292135, 0.0119351, 0...  \n",
       "3  [0.565625, 0.3114583, 0.0159722, 0.0142361, 0....  "
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview dataframe again\n",
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iran , Iran\n",
      "Iraq , Iraq\n",
      "Turkey , Turkey\n",
      "Kurdish , Kurds\n",
      "*************************************************\n",
      "This congruent experiment is 50.0% accurate comparing page titles.\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for mention, pred in congruent_predictions.items():\n",
    "    pred_title = normalize_text(pred[0])\n",
    "    print(mention, \",\", pred_title)\n",
    "    if sentence_predictions[sentence_predictions['full_mention'] == mention]['wikipedia_title'].values[0] == pred_title:\n",
    "        accuracy += 1\n",
    "print(\"*************************************************\")\n",
    "print(f\"This congruent experiment is {round(accuracy/len(congruent_predictions)*100,3)}% accurate comparing page titles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iran , 14653\n",
      "Iraq , 7515928\n",
      "Turkey , 11125639\n",
      "Kurdish , 17068\n",
      "*************************************************\n",
      "This congruent experiment is 75.0% accurate comparing page IDs.\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for mention, pred in congruent_predictions.items():\n",
    "    pred_page_id = pred[1]\n",
    "    print(mention, \",\", pred_page_id)\n",
    "    if sentence_predictions[sentence_predictions['full_mention'] == mention]['wikipedia_page_ID'].values[0] == pred_page_id:\n",
    "        accuracy += 1\n",
    "print(\"*************************************************\")\n",
    "print(f\"This congruent experiment is {round(accuracy/len(congruent_predictions)*100,3)}% accurate comparing page IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But really it's 100% accurate, because 1/4 mentions don't have a \"true\" target, but if it did, the target would most likely be what we predicted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Congruence Predictions and Assess Accuracy over Entire Dataframe\n",
    "\n",
    "We now apply the per-sentence structure over the whole ACY dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5,447 sentences to predict.\n"
     ]
    }
   ],
   "source": [
    "# Max sentence_id in dataframe\n",
    "max_sentence_id = max(predictions['sentence_id'])\n",
    "print(\"We have {:,} sentences to predict.\".format(max_sentence_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4516/4516 [03:04<00:00, 24.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "dataframe_predictions = {}\n",
    "for sid in tqdm(predictions['sentence_id'].unique()):\n",
    "    dataframe_predictions[sid] = get_congruent_predictions(sid, dataframe=predictions, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29312/29312 [00:04<00:00, 6775.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After congruence, we have achieved 53.391% accuracy comparing title.\n",
      "After congruence, we have achieved 57.031% accuracy comparing page ID.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to calculate accuracy\n",
    "accurate_predictions_title = 0\n",
    "accurate_predictions_id = 0\n",
    "for row in tqdm(range(len(predictions))):\n",
    "    mention_df = predictions.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    page_id = mention_df['wikipedia_page_ID']\n",
    "    pred = dataframe_predictions[sid][fm]\n",
    "    norm_pred_title = normalize_text(pred[0])\n",
    "    pred_page_id = pred[1]\n",
    "#     print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred_title, \"|||\", norm_pred_title==title,\\\n",
    "#                  \"||| True ID: \", page_id, \"==? Pred:\", pred_page_id, \"|||\", pred_page_id==page_id)\n",
    "    if title == norm_pred_title:\n",
    "        accurate_predictions_title += 1\n",
    "    if page_id == pred_page_id:\n",
    "        accurate_predictions_id += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing title.\".format(round(accurate_predictions_title/len(predictions)*100, 3)))\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing page ID.\".format(round(accurate_predictions_id/len(predictions)*100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republican 2498 ||| True: Republican Party (United States) ==? Pred: Republican Party (United States) ||| True ||| True ID:  32070.0 ==? Pred: 32070 ||| True\n",
      "Republican Party 2498 ||| True: Republican Party (United States) ==? Pred: Republican Party (United States) ||| True ||| True ID:  32070.0 ==? Pred: 32070 ||| True\n",
      "Republican Party 2498 ||| True: Republican Party (United States) ==? Pred: Republican Party (United States) ||| True ||| True ID:  32070.0 ==? Pred: 32070 ||| True\n",
      "Christopher Reeve 2499 ||| True: Christopher Reeve ==? Pred: Christopher Reeve ||| True ||| True ID:  73626.0 ==? Pred: 73626 ||| True\n",
      "Christopher Reeve 2499 ||| True: Christopher Reeve ==? Pred: Christopher Reeve ||| True ||| True ID:  73626.0 ==? Pred: 73626 ||| True\n",
      "Reeve 2499 ||| True: Christopher Reeve ==? Pred: Mayor ||| False ||| True ID:  73626.0 ==? Pred: 101146 ||| False\n",
      "Superman 2499 ||| True: Superman (film) ==? Pred: Superman ||| False ||| True ID:  362719.0 ==? Pred: 28381 ||| False\n",
      "Reeve 2500 ||| True: Christopher Reeve ==? Pred: Lovell Augustus Reeve ||| False ||| True ID:  73626.0 ==? Pred: 20166407 ||| False\n",
      "Culpepper 2500 ||| True: Culpeper, Virginia ==? Pred: Nicholas Culpeper ||| False ||| True ID:  137552.0 ==? Pred: 482878 ||| False\n",
      "Virginia 2500 ||| True: Virginia ==? Pred: Virginia ||| True ||| True ID:  32432.0 ==? Pred: 32432 ||| True\n",
      "After congruence, we have achieved 60.0% accuracy comparing title.\n",
      "After congruence, we have achieved 60.0% accuracy comparing page ID.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over subset of dataframe to calculate accuracy and study output results\n",
    "accurate_predictions_title = 0\n",
    "accurate_predictions_id = 0\n",
    "rand_idx = np.random.randint(len(predictions)) # Observe random subsection of predictions\n",
    "window_size = 10\n",
    "for row in range(rand_idx, rand_idx+window_size):\n",
    "    mention_df = predictions.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    page_id = mention_df['wikipedia_page_ID']\n",
    "    pred = dataframe_predictions[sid][fm]\n",
    "    norm_pred_title = normalize_text(pred[0])\n",
    "    pred_page_id = pred[1]\n",
    "    print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred_title, \"|||\", norm_pred_title==title,\\\n",
    "                 \"||| True ID: \", page_id, \"==? Pred:\", pred_page_id, \"|||\", pred_page_id==page_id)\n",
    "    if title == norm_pred_title:\n",
    "        accurate_predictions_title += 1\n",
    "    if page_id == pred_page_id:\n",
    "        accurate_predictions_id += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing title.\".format(round(accurate_predictions_title/window_size*100, 3)))\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing page ID.\".format(round(accurate_predictions_id/window_size*100, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Culpepper 2500 ||| True: Culpeper, Virginia ==? Pred: Nicholas Culpeper ||| False ||| True ID:  137552.0 ==? Pred: 482878 ||| False\n",
    "Virginia 2500 ||| True: Virginia ==? Pred: Virginia ||| True ||| True ID:  32432.0 ==? Pred: 32432 ||| True`\n",
    "\n",
    "Guessing the prior for `culpepper` referring to `Nicholas Culpeper` overwhelmed the otherwise more congruent pairing between `Culpeper, Virginia` and `Virginia`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congruence_measurement = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "sentence_predictions['candidate_pool_likelihoods'][men1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victoria 5393 ||| True: Victorian Bushrangers ==? Pred: Victoria (Australia) ||| False ||| True ID:  1147963.0 ==? Pred: 4689460 ||| False\n",
      "Dean Jones 5393 ||| True: Dean Jones (cricketer) ==? Pred: Dean Jones (cricketer) ||| True ||| True ID:  897229.0 ==? Pred: 897229 ||| True\n",
      "Dean Jones 5393 ||| True: Dean Jones (cricketer) ==? Pred: Dean Jones (cricketer) ||| True ||| True ID:  897229.0 ==? Pred: 897229 ||| True\n",
      "SOUTH KOREA 5394 ||| True: South Korea ==? Pred: South Korea ||| True ||| True ID:  27019.0 ==? Pred: 27019 ||| True\n",
      "SOUTH KOREA 5394 ||| True: South Korea ==? Pred: South Korea ||| True ||| True ID:  27019.0 ==? Pred: 27019 ||| True\n",
      "ABU DHABI 5395 ||| True: Abu Dhabi ==? Pred: Abu Dhabi ||| True ||| True ID:  18950756.0 ==? Pred: 18950756 ||| True\n",
      "ABU DHABI 5395 ||| True: Abu Dhabi ==? Pred: Abu Dhabi ||| True ||| True ID:  18950756.0 ==? Pred: 18950756 ||| True\n",
      "South Korea 5395 ||| True: Korea Republic national football team ==? Pred: South Korea ||| False ||| True ID:  1018627.0 ==? Pred: 27019 ||| False\n",
      "South Korea 5395 ||| True: Korea Republic national football team ==? Pred: South Korea ||| False ||| True ID:  1018627.0 ==? Pred: 27019 ||| False\n",
      "Asian Cup 5395 ||| True: AFC Asian Cup ==? Pred: AFC Asian Cup ||| True ||| True ID:  250683.0 ==? Pred: 250683 ||| True\n",
      "After congruence, we have achieved 70.0% accuracy comparing title.\n",
      "After congruence, we have achieved 70.0% accuracy comparing page ID.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over subset of dataframe to calculate accuracy and study output results\n",
    "accurate_predictions_title = 0\n",
    "accurate_predictions_id = 0\n",
    "rand_idx = np.random.randint(len(predictions)) # Observe random subsection of predictions\n",
    "window_size = 10\n",
    "for row in range(rand_idx, rand_idx+window_size):\n",
    "    mention_df = predictions.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    page_id = mention_df['wikipedia_page_ID']\n",
    "    pred = dataframe_predictions[sid][fm]\n",
    "    norm_pred_title = normalize_text(pred[0])\n",
    "    pred_page_id = pred[1]\n",
    "    print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred_title, \"|||\", norm_pred_title==title,\\\n",
    "                 \"||| True ID: \", page_id, \"==? Pred:\", pred_page_id, \"|||\", pred_page_id==page_id)\n",
    "    if title == norm_pred_title:\n",
    "        accurate_predictions_title += 1\n",
    "    if page_id == pred_page_id:\n",
    "        accurate_predictions_id += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing title.\".format(round(accurate_predictions_title/window_size*100, 3)))\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing page ID.\".format(round(accurate_predictions_id/window_size*100, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Victoria 5393 ||| True: Victorian Bushrangers ==? Pred: Victoria (Australia) ||| False ||| True ID:  1147963.0 ==? Pred: 4689460 ||| False\n",
    "Dean Jones 5393 ||| True: Dean Jones (cricketer) ==? Pred: Dean Jones (cricketer) ||| True ||| True ID:  897229.0 ==? Pred: 897229 ||| True`\n",
    "\n",
    "Prior for `Victoria` referring to `Victoria (Australia)` overwhelms the otherwise more congruent pairing of `Victoria Bushrangers` (cricket team)  with `Dean Jones (cricketer)`, which it did get right.\n",
    "\n",
    "Same for `South Korea` referring to the country (higher prior) rather than the football team, even though `Asian Cup` referring to `AFC Asian Cup` (football cup) was correctly predicted. Again congruence fails to overcome the very high prior for `South Korea` pointing to the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian Weather Service 2820 ||| True: nan ==? Pred: None ||| False ||| True ID:  nan ==? Pred: None ||| False\n",
      "Moscow Newsroom 2821 ||| True: nan ==? Pred: None ||| False ||| True ID:  nan ==? Pred: None ||| False\n",
      "Moscow Newsroom 2821 ||| True: nan ==? Pred: None ||| False ||| True ID:  nan ==? Pred: None ||| False\n",
      "Russian 2821 ||| True: Russia ==? Pred: Russian language ||| False ||| True ID:  25391.0 ==? Pred: 25431 ||| False\n",
      "MOSCOW 2822 ||| True: Moscow ==? Pred: Moscow ||| True ||| True ID:  19004.0 ==? Pred: 19004 ||| True\n",
      "Russian 2822 ||| True: Russia ==? Pred: Russian language ||| False ||| True ID:  25391.0 ==? Pred: 25431 ||| False\n",
      "Urals 2822 ||| True: Ural Mountains ==? Pred: Ural Mountains ||| True ||| True ID:  32152.0 ==? Pred: 32152 ||| True\n",
      "Perm 2822 ||| True: Perm ==? Pred: Perm ||| True ||| True ID:  389777.0 ==? Pred: 389777 ||| True\n",
      "Itar-Tass 2822 ||| True: Information Telegraph Agency of Russia ==? Pred: TASS ||| False ||| True ID:  434852.0 ==? Pred: 434852 ||| True\n",
      "Tass 2824 ||| True: nan ==? Pred: TASS ||| False ||| True ID:  nan ==? Pred: 434852 ||| False\n",
      "After congruence, we have achieved 30.0% accuracy comparing title.\n",
      "After congruence, we have achieved 40.0% accuracy comparing page ID.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over subset of dataframe to calculate accuracy and study output results\n",
    "accurate_predictions_title = 0\n",
    "accurate_predictions_id = 0\n",
    "rand_idx = np.random.randint(len(predictions)) # Observe random subsection of predictions\n",
    "window_size = 10\n",
    "for row in range(rand_idx, rand_idx+window_size):\n",
    "    mention_df = predictions.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    page_id = mention_df['wikipedia_page_ID']\n",
    "    pred = dataframe_predictions[sid][fm]\n",
    "    norm_pred_title = normalize_text(pred[0])\n",
    "    pred_page_id = pred[1]\n",
    "    print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred_title, \"|||\", norm_pred_title==title,\\\n",
    "                 \"||| True ID: \", page_id, \"==? Pred:\", pred_page_id, \"|||\", pred_page_id==page_id)\n",
    "    if title == norm_pred_title:\n",
    "        accurate_predictions_title += 1\n",
    "    if page_id == pred_page_id:\n",
    "        accurate_predictions_id += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing title.\".format(round(accurate_predictions_title/window_size*100, 3)))\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing page ID.\".format(round(accurate_predictions_id/window_size*100, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several of these are just very hard, like `Russian` pointing to `Russia` (country) rather than `Russian` (language), which is what we predicted, and which would arguably have been a better link in that Wikipedia page anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:**\n",
    "\n",
    "The failure modes are usually that a strong prior overwhelms what would otherwise be a more congruent combination of entities. E.g. mapping `Victoria` to `Victoria` (city) rather than `Victorian Bushrangers` (cricket team) when the second mention in the sentence is `Dean Jones` correctly mapped to `Dean Jones` (cricketer).\n",
    "\n",
    "Previously the most common failure mode was the opposite: congruent pairings were almost always chosen, overwhelming more sensible priors. E.g. mapping `Iran` and `Iraq` to their respective national soccer teams rather than countries, languages, or cultures, because the soccer teams were always very close in embedding vector space.\n",
    "\n",
    "So we need to strike the best balance we can between the two. Alternatives are:\n",
    "- Extending beyond pairwise congruence measures, rather considering whole sets. Pairwise is easy as we can take the dot product (find cosine distance) between two vectors (but not between more than two). For sets of vectors, we need a different way, such as for each set of proposed candidates finding the centroid of all vectors in embedding space and picking the set of entities with lowest aggregate distance to that set's centroid (or some other way).\n",
    "- Finding the weighting between priors (currently we take their mean) and congruence; currently we take the mean prior across candidates and multiply that one number with the overall congruence score (right?). Can find the best weighting through cross-validation, for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy only with Full Mentions with Known True\n",
    "\n",
    "This is a better reflection of our success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22257\n"
     ]
    }
   ],
   "source": [
    "# Calculate length of input with known true values\n",
    "known_true = sum(predictions['wikipedia_page_ID'].notnull())\n",
    "print(known_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29312/29312 [00:04<00:00, 6266.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After congruence, we have achieved 70.315% accuracy comparing title.\n",
      "After congruence, we have achieved 75.109% accuracy comparing page ID.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to calculate accuracy\n",
    "accurate_predictions_title = 0\n",
    "accurate_predictions_id = 0\n",
    "for row in tqdm(range(len(predictions))):\n",
    "    mention_df = predictions.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    page_id = mention_df['wikipedia_page_ID']\n",
    "    if page_id is None:\n",
    "        pass\n",
    "    elif isinstance(title, float):\n",
    "        pass\n",
    "    else:\n",
    "        pred = dataframe_predictions[sid][fm]\n",
    "        norm_pred_title = normalize_text(pred[0])\n",
    "        pred_page_id = pred[1]\n",
    "    #     print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred_title, \"|||\", norm_pred_title==title,\\\n",
    "    #                  \"||| True ID: \", page_id, \"==? Pred:\", pred_page_id, \"|||\", pred_page_id==page_id)\n",
    "        if title == norm_pred_title:\n",
    "            accurate_predictions_title += 1\n",
    "        if page_id == pred_page_id:\n",
    "            accurate_predictions_id += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing title.\".format(round(accurate_predictions_title/known_true*100, 3)))\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing page ID.\".format(round(accurate_predictions_id/known_true*100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4363 7\n",
      "U.S. 4363 ||| True: United States 3434750.0 ==? Pred: United States 3434750 ||| True True\n",
      "Federal Reserve 4363 ||| True: Federal Reserve System 10819.0 ==? Pred: Federal Reserve 10819 ||| False True\n",
      "Federal Reserve 4363 ||| True: Federal Reserve System 10819.0 ==? Pred: Federal Reserve 10819 ||| False True\n",
      "Federal Open Market Committee 4363 ||| True: Federal Open Market Committee 593155.0 ==? Pred: Federal Open Market Committee 593155 ||| True True\n",
      "Federal Open Market Committee 4363 ||| True: Federal Open Market Committee 593155.0 ==? Pred: Federal Open Market Committee 593155 ||| True True\n",
      "Federal Open Market Committee 4363 ||| True: Federal Open Market Committee 593155.0 ==? Pred: Federal Open Market Committee 593155 ||| True True\n",
      "Federal Open Market Committee 4363 ||| True: Federal Open Market Committee 593155.0 ==? Pred: Federal Open Market Committee 593155 ||| True True\n",
      "***************************************\n",
      "After congruence, we have achieved 71.429% accuracy comparing title.\n",
      "After congruence, we have achieved 100.0% accuracy comparing page ID.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over subset of dataframe to calculate accuracy and study output results\n",
    "accurate_predictions_title = 0\n",
    "accurate_predictions_id = 0\n",
    "predictions_count = 0\n",
    "\n",
    "# Generate random window\n",
    "rand_idx = np.random.randint(len(predictions)) # Observe random subsection of predictions\n",
    "window_size = 10\n",
    "\n",
    "# Pick random sentence\n",
    "rand_sid = np.random.choice(predictions['sentence_id'])\n",
    "rand_sentence_df = predictions[predictions['sentence_id'] == rand_sid]\n",
    "print(rand_sid, len(rand_sentence_df))\n",
    "for row in range(len(rand_sentence_df)):\n",
    "    mention_df = rand_sentence_df.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    page_id = mention_df['wikipedia_page_ID']\n",
    "    if page_id is None: # This handles calculating over only known true\n",
    "        pass\n",
    "    elif isinstance(title, float):\n",
    "        pass\n",
    "    else:\n",
    "        predictions_count += 1\n",
    "        pred = dataframe_predictions[sid][fm]\n",
    "        norm_pred_title = normalize_text(pred[0])\n",
    "        pred_page_id = pred[1]\n",
    "        print(fm, sid, \"||| True:\", title, page_id, \"==? Pred:\", norm_pred_title, pred_page_id, \"|||\",\\\n",
    "              norm_pred_title==title, pred_page_id==page_id)\n",
    "        if title == norm_pred_title:\n",
    "            accurate_predictions_title += 1\n",
    "        if page_id == pred_page_id:\n",
    "            accurate_predictions_id += 1\n",
    "print(\"***************************************\")\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing title.\".format(round(accurate_predictions_title/predictions_count*100, 3)))\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing page ID.\".format(round(accurate_predictions_id/predictions_count*100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24306</th>\n",
       "      <td>B</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_States</td>\n",
       "      <td>3434750.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>4363</td>\n",
       "      <td>770</td>\n",
       "      <td>[U.S., Federal Reserve, Federal Open Market Co...</td>\n",
       "      <td>u.s.</td>\n",
       "      <td>[3434750, 423161, 730350, 195149, 582488, 2051...</td>\n",
       "      <td>[30, 180072, 188819, 48525, 164134, 11220, 112...</td>\n",
       "      <td>[United_States, Billboard_Hot_100, Billboard_2...</td>\n",
       "      <td>[0.9423457, 0.0061728, 0.0038272, 0.002963, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24307</th>\n",
       "      <td>B</td>\n",
       "      <td>Federal Reserve</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Federal_Reserve_S...</td>\n",
       "      <td>10819.0</td>\n",
       "      <td>Federal Reserve System</td>\n",
       "      <td>4363</td>\n",
       "      <td>770</td>\n",
       "      <td>[U.S., Federal Reserve, Federal Open Market Co...</td>\n",
       "      <td>federal reserve</td>\n",
       "      <td>[10819, 1838417, 178805, 1291858, 360128, 2338...</td>\n",
       "      <td>[53536, 5440396, 4481787, 288835, 786774, 4384...</td>\n",
       "      <td>[Federal_Reserve, Federal_Reserve_Board_of_Gov...</td>\n",
       "      <td>[0.9756573, 0.0077897, 0.006816, 0.0019474, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24308</th>\n",
       "      <td>I</td>\n",
       "      <td>Federal Reserve</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Federal_Reserve_S...</td>\n",
       "      <td>10819.0</td>\n",
       "      <td>Federal Reserve System</td>\n",
       "      <td>4363</td>\n",
       "      <td>770</td>\n",
       "      <td>[U.S., Federal Reserve, Federal Open Market Co...</td>\n",
       "      <td>federal reserve</td>\n",
       "      <td>[10819, 1838417, 178805, 1291858, 360128, 2338...</td>\n",
       "      <td>[53536, 5440396, 4481787, 288835, 786774, 4384...</td>\n",
       "      <td>[Federal_Reserve, Federal_Reserve_Board_of_Gov...</td>\n",
       "      <td>[0.9756573, 0.0077897, 0.006816, 0.0019474, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24309</th>\n",
       "      <td>B</td>\n",
       "      <td>Federal Open Market Committee</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Federal_Open_Mark...</td>\n",
       "      <td>593155.0</td>\n",
       "      <td>Federal Open Market Committee</td>\n",
       "      <td>4363</td>\n",
       "      <td>770</td>\n",
       "      <td>[U.S., Federal Reserve, Federal Open Market Co...</td>\n",
       "      <td>federal open market committee</td>\n",
       "      <td>[593155]</td>\n",
       "      <td>[2289022]</td>\n",
       "      <td>[Federal_Open_Market_Committee]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24310</th>\n",
       "      <td>I</td>\n",
       "      <td>Federal Open Market Committee</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Federal_Open_Mark...</td>\n",
       "      <td>593155.0</td>\n",
       "      <td>Federal Open Market Committee</td>\n",
       "      <td>4363</td>\n",
       "      <td>770</td>\n",
       "      <td>[U.S., Federal Reserve, Federal Open Market Co...</td>\n",
       "      <td>federal open market committee</td>\n",
       "      <td>[593155]</td>\n",
       "      <td>[2289022]</td>\n",
       "      <td>[Federal_Open_Market_Committee]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24311</th>\n",
       "      <td>I</td>\n",
       "      <td>Federal Open Market Committee</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Federal_Open_Mark...</td>\n",
       "      <td>593155.0</td>\n",
       "      <td>Federal Open Market Committee</td>\n",
       "      <td>4363</td>\n",
       "      <td>770</td>\n",
       "      <td>[U.S., Federal Reserve, Federal Open Market Co...</td>\n",
       "      <td>federal open market committee</td>\n",
       "      <td>[593155]</td>\n",
       "      <td>[2289022]</td>\n",
       "      <td>[Federal_Open_Market_Committee]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24312</th>\n",
       "      <td>I</td>\n",
       "      <td>Federal Open Market Committee</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Federal_Open_Mark...</td>\n",
       "      <td>593155.0</td>\n",
       "      <td>Federal Open Market Committee</td>\n",
       "      <td>4363</td>\n",
       "      <td>770</td>\n",
       "      <td>[U.S., Federal Reserve, Federal Open Market Co...</td>\n",
       "      <td>federal open market committee</td>\n",
       "      <td>[593155]</td>\n",
       "      <td>[2289022]</td>\n",
       "      <td>[Federal_Open_Market_Committee]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mention                   full_mention  \\\n",
       "24306       B                           U.S.   \n",
       "24307       B                Federal Reserve   \n",
       "24308       I                Federal Reserve   \n",
       "24309       B  Federal Open Market Committee   \n",
       "24310       I  Federal Open Market Committee   \n",
       "24311       I  Federal Open Market Committee   \n",
       "24312       I  Federal Open Market Committee   \n",
       "\n",
       "                                           wikipedia_URL  wikipedia_page_ID  \\\n",
       "24306         http://en.wikipedia.org/wiki/United_States          3434750.0   \n",
       "24307  http://en.wikipedia.org/wiki/Federal_Reserve_S...            10819.0   \n",
       "24308  http://en.wikipedia.org/wiki/Federal_Reserve_S...            10819.0   \n",
       "24309  http://en.wikipedia.org/wiki/Federal_Open_Mark...           593155.0   \n",
       "24310  http://en.wikipedia.org/wiki/Federal_Open_Mark...           593155.0   \n",
       "24311  http://en.wikipedia.org/wiki/Federal_Open_Mark...           593155.0   \n",
       "24312  http://en.wikipedia.org/wiki/Federal_Open_Mark...           593155.0   \n",
       "\n",
       "                     wikipedia_title  sentence_id  doc_id  \\\n",
       "24306                  United States         4363     770   \n",
       "24307         Federal Reserve System         4363     770   \n",
       "24308         Federal Reserve System         4363     770   \n",
       "24309  Federal Open Market Committee         4363     770   \n",
       "24310  Federal Open Market Committee         4363     770   \n",
       "24311  Federal Open Market Committee         4363     770   \n",
       "24312  Federal Open Market Committee         4363     770   \n",
       "\n",
       "                                      congruent_mentions  \\\n",
       "24306  [U.S., Federal Reserve, Federal Open Market Co...   \n",
       "24307  [U.S., Federal Reserve, Federal Open Market Co...   \n",
       "24308  [U.S., Federal Reserve, Federal Open Market Co...   \n",
       "24309  [U.S., Federal Reserve, Federal Open Market Co...   \n",
       "24310  [U.S., Federal Reserve, Federal Open Market Co...   \n",
       "24311  [U.S., Federal Reserve, Federal Open Market Co...   \n",
       "24312  [U.S., Federal Reserve, Federal Open Market Co...   \n",
       "\n",
       "                   norm_full_mention  \\\n",
       "24306                           u.s.   \n",
       "24307                federal reserve   \n",
       "24308                federal reserve   \n",
       "24309  federal open market committee   \n",
       "24310  federal open market committee   \n",
       "24311  federal open market committee   \n",
       "24312  federal open market committee   \n",
       "\n",
       "                                 candidate_pool_page_ids  \\\n",
       "24306  [3434750, 423161, 730350, 195149, 582488, 2051...   \n",
       "24307  [10819, 1838417, 178805, 1291858, 360128, 2338...   \n",
       "24308  [10819, 1838417, 178805, 1291858, 360128, 2338...   \n",
       "24309                                           [593155]   \n",
       "24310                                           [593155]   \n",
       "24311                                           [593155]   \n",
       "24312                                           [593155]   \n",
       "\n",
       "                                 candidate_pool_item_ids  \\\n",
       "24306  [30, 180072, 188819, 48525, 164134, 11220, 112...   \n",
       "24307  [53536, 5440396, 4481787, 288835, 786774, 4384...   \n",
       "24308  [53536, 5440396, 4481787, 288835, 786774, 4384...   \n",
       "24309                                          [2289022]   \n",
       "24310                                          [2289022]   \n",
       "24311                                          [2289022]   \n",
       "24312                                          [2289022]   \n",
       "\n",
       "                                   candidate_pool_titles  \\\n",
       "24306  [United_States, Billboard_Hot_100, Billboard_2...   \n",
       "24307  [Federal_Reserve, Federal_Reserve_Board_of_Gov...   \n",
       "24308  [Federal_Reserve, Federal_Reserve_Board_of_Gov...   \n",
       "24309                    [Federal_Open_Market_Committee]   \n",
       "24310                    [Federal_Open_Market_Committee]   \n",
       "24311                    [Federal_Open_Market_Committee]   \n",
       "24312                    [Federal_Open_Market_Committee]   \n",
       "\n",
       "                              candidate_pool_likelihoods  \n",
       "24306  [0.9423457, 0.0061728, 0.0038272, 0.002963, 0....  \n",
       "24307  [0.9756573, 0.0077897, 0.006816, 0.0019474, 0....  \n",
       "24308  [0.9756573, 0.0077897, 0.006816, 0.0019474, 0....  \n",
       "24309                                              [1.0]  \n",
       "24310                                              [1.0]  \n",
       "24311                                              [1.0]  \n",
       "24312                                              [1.0]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See original dataframe\n",
    "predictions[predictions['sentence_id'] == rand_sid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Federal_Open_Market_Committee']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See full candidate pool\n",
    "predictions[predictions['sentence_id'] == rand_sid]['candidate_pool_titles'].iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate with *Popularity* Predictions Known True Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 9891, 9472, 10890716, 2780146, 18...</td>\n",
       "      <td>[458, 46, 45003, 4916, 185441, 932442, 8268, 8...</td>\n",
       "      <td>['European_Union', 'Europe', 'Entropy', 'Euro'...</td>\n",
       "      <td>[0.2237612, 0.2008411, 0.0962244, 0.0766248, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 11867, 27318, 21148, 21212, 21212, 269...</td>\n",
       "      <td>[183, 183, 334, 55, 7318, 7318, 40, 12548, 825...</td>\n",
       "      <td>['Germany', 'Germany', 'Singapore', 'Netherlan...</td>\n",
       "      <td>[0.0494115, 0.0494115, 0.0485629, 0.0390487, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>british</td>\n",
       "      <td>[3434750, 31717, 31717, 19344654, 26061, 85699...</td>\n",
       "      <td>[30, 145, 145, 9531, 172771, 1860, 21, 22, 868...</td>\n",
       "      <td>['United_States', 'United_Kingdom', 'United_Ki...</td>\n",
       "      <td>[0.115186, 0.0611816, 0.0611816, 0.0294143, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 56873217, 9643132]</td>\n",
       "      <td>[2073954, 26634508, 7172840]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.6296296, 0.3703704, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 56873217, 9643132]</td>\n",
       "      <td>[2073954, 26634508, 7172840]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.6296296, 0.3703704, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention     full_mention                                wikipedia_URL  \\\n",
       "0       B               EU                                          NaN   \n",
       "1       B           German         http://en.wikipedia.org/wiki/Germany   \n",
       "2       B          British  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "3       B  Peter Blackburn                                          NaN   \n",
       "4       I  Peter Blackburn                                          NaN   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0                NaN             NaN            0       0   \n",
       "1            11867.0         Germany            0       0   \n",
       "2            31717.0  United Kingdom            0       0   \n",
       "3                NaN             NaN            1       0   \n",
       "4                NaN             NaN            1       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0                        ['EU', 'German', 'British']                eu   \n",
       "1                        ['EU', 'German', 'British']            german   \n",
       "2                        ['EU', 'German', 'British']           british   \n",
       "3  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "4  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [9317, 9239, 9891, 9472, 10890716, 2780146, 18...   \n",
       "1  [11867, 11867, 27318, 21148, 21212, 21212, 269...   \n",
       "2  [3434750, 31717, 31717, 19344654, 26061, 85699...   \n",
       "3                      [56783206, 56873217, 9643132]   \n",
       "4                      [56783206, 56873217, 9643132]   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [458, 46, 45003, 4916, 185441, 932442, 8268, 8...   \n",
       "1  [183, 183, 334, 55, 7318, 7318, 40, 12548, 825...   \n",
       "2  [30, 145, 145, 9531, 172771, 1860, 21, 22, 868...   \n",
       "3                       [2073954, 26634508, 7172840]   \n",
       "4                       [2073954, 26634508, 7172840]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  ['European_Union', 'Europe', 'Entropy', 'Euro'...   \n",
       "1  ['Germany', 'Germany', 'Singapore', 'Netherlan...   \n",
       "2  ['United_States', 'United_Kingdom', 'United_Ki...   \n",
       "3  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "4  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.2237612, 0.2008411, 0.0962244, 0.0766248, 0...  \n",
       "1  [0.0494115, 0.0494115, 0.0485629, 0.0390487, 0...  \n",
       "2  [0.115186, 0.0611816, 0.0611816, 0.0294143, 0....  \n",
       "3                        [0.6296296, 0.3703704, 0.0]  \n",
       "4                        [0.6296296, 0.3703704, 0.0]  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "predictions_pop = pd.read_csv(os.path.join(preds_path, \"anchortext_popularity.csv\"), delimiter=\",\")\n",
    "predictions_pop.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "After ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "Before [56783206, 56873217, 9643132]\n",
      "After [56783206, 56873217, 9643132]\n",
      "Before [2073954, 26634508, 7172840]\n",
      "After [2073954, 26634508, 7172840]\n",
      "Before ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(MP)', 'Peter_Blackburn_(bishop)']\n",
      "After ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(MP)', 'Peter_Blackburn_(bishop)']\n",
      "Before [0.6296296, 0.3703704, 0.0]\n",
      "After [0.6296296, 0.3703704, 0.0]\n",
      "CPU times: user 683 ms, sys: 306 ms, total: 990 ms\n",
      "Wall time: 1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Apply defined function to entire dataframe for all candidate pool columns\n",
    "\n",
    "column = 'congruent_mentions'\n",
    "print(\"Before\", predictions_pop[column][3])\n",
    "parsed_candidate_pool = predictions_pop[column].apply(parse_list_string, value_type=str)\n",
    "predictions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions_pop[column][3])\n",
    "\n",
    "column = 'candidate_pool_page_ids'\n",
    "print(\"Before\", predictions_pop[column][3])\n",
    "parsed_candidate_pool = predictions_pop[column].apply(parse_list_string, value_type=int)\n",
    "predictions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions_pop[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_item_ids'\n",
    "print(\"Before\", predictions_pop[column][3])\n",
    "parsed_candidate_pool = predictions_pop[column].apply(parse_list_string, value_type=int)\n",
    "predictions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions_pop[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_titles'\n",
    "print(\"Before\", predictions_pop[column][3])\n",
    "parsed_candidate_pool = predictions_pop[column].apply(parse_list_string, value_type=str)\n",
    "predictions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions_pop[column][3])\n",
    "\n",
    "column = 'candidate_pool_likelihoods'\n",
    "print(\"Before\", predictions_pop[column][3])\n",
    "parsed_candidate_pool = predictions_pop[column].apply(parse_list_string, value_type=float)\n",
    "predictions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", predictions_pop[column][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22257\n"
     ]
    }
   ],
   "source": [
    "# Calculate length of input with known true values\n",
    "known_true = sum(predictions_pop['wikipedia_page_ID'].notnull())\n",
    "print(known_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[EU, German, British]</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 9891, 9472, 10890716, 2780146, 18...</td>\n",
       "      <td>[458, 46, 45003, 4916, 185441, 932442, 8268, 8...</td>\n",
       "      <td>[European_Union, Europe, Entropy, Euro, Member...</td>\n",
       "      <td>[0.2237612, 0.2008411, 0.0962244, 0.0766248, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[EU, German, British]</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 11867, 27318, 21148, 21212, 21212, 269...</td>\n",
       "      <td>[183, 183, 334, 55, 7318, 7318, 40, 12548, 825...</td>\n",
       "      <td>[Germany, Germany, Singapore, Netherlands, Naz...</td>\n",
       "      <td>[0.0494115, 0.0494115, 0.0485629, 0.0390487, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[EU, German, British]</td>\n",
       "      <td>british</td>\n",
       "      <td>[3434750, 31717, 31717, 19344654, 26061, 85699...</td>\n",
       "      <td>[30, 145, 145, 9531, 172771, 1860, 21, 22, 868...</td>\n",
       "      <td>[United_States, United_Kingdom, United_Kingdom...</td>\n",
       "      <td>[0.115186, 0.0611816, 0.0611816, 0.0294143, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention full_mention                                wikipedia_URL  \\\n",
       "0       B           EU                                          NaN   \n",
       "1       B       German         http://en.wikipedia.org/wiki/Germany   \n",
       "2       B      British  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0                NaN             NaN            0       0   \n",
       "1            11867.0         Germany            0       0   \n",
       "2            31717.0  United Kingdom            0       0   \n",
       "\n",
       "      congruent_mentions norm_full_mention  \\\n",
       "0  [EU, German, British]                eu   \n",
       "1  [EU, German, British]            german   \n",
       "2  [EU, German, British]           british   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [9317, 9239, 9891, 9472, 10890716, 2780146, 18...   \n",
       "1  [11867, 11867, 27318, 21148, 21212, 21212, 269...   \n",
       "2  [3434750, 31717, 31717, 19344654, 26061, 85699...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [458, 46, 45003, 4916, 185441, 932442, 8268, 8...   \n",
       "1  [183, 183, 334, 55, 7318, 7318, 40, 12548, 825...   \n",
       "2  [30, 145, 145, 9531, 172771, 1860, 21, 22, 868...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [European_Union, Europe, Entropy, Euro, Member...   \n",
       "1  [Germany, Germany, Singapore, Netherlands, Naz...   \n",
       "2  [United_States, United_Kingdom, United_Kingdom...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.2237612, 0.2008411, 0.0962244, 0.0766248, 0...  \n",
       "1  [0.0494115, 0.0494115, 0.0485629, 0.0390487, 0...  \n",
       "2  [0.115186, 0.0611816, 0.0611816, 0.0294143, 0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congruent Mentions:  ['EU', 'German', 'British']\n",
      "Congruent Mentions as numbers:  [0 1 2]\n",
      "['European_Union', 'Europe', 'Entropy', 'Euro', 'Member_state_of_the_European_Union', 'European_emission_standards', 'Eurozone', 'European_Parliament', 'European_Commission', 'Europe,_the_Middle_East_and_Africa']\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['Germany', 'Germany', 'Singapore', 'Netherlands', 'Nazi_Germany', 'Nazi_Germany', 'Austria', 'Holy_Roman_Empire', 'Bundesliga', 'Age_of_Enlightenment']\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['United_States', 'United_Kingdom', 'United_Kingdom', 'BBC', 'Royal_Navy', 'English_language', 'England', 'Scotland', 'British_Empire', 'Commonwealth_of_Nations']\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "dict_keys([0, 1, 2])\n",
      "10\n",
      "10\n",
      "10\n",
      "Mentions with Vectors:  dict_keys([0, 1, 2])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 1 & 2\n",
      "First-Level Congruence Keys:  dict_keys([0, 1])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 1 & 2\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((1, 3), 0.05630195596694946)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 0), 0.07658241383750587)\n",
      "Current Most Congruent:  (((1, 3), 0.05630195596694946), (0, 1))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.032764191008694474)\n",
      "Current Most Congruent:  (((1, 0), 0.07658241383750587), (0, 2))\n",
      "Final Most Congruent Pair:  (((1, 0), 0.07658241383750587), (0, 2))\n",
      "(((1, 0), 0.07658241383750587), (0, 2))\n",
      "[1] dict_keys([0, 2])\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((1, 3), 0.05630195596694946)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.032764191008694474)\n",
      "Current Most Congruent:  (((1, 3), 0.05630195596694946), (0, 1))\n",
      "Final Most Congruent Pair:  (((1, 3), 0.05630195596694946), (0, 1))\n",
      "[] dict_keys([0, 2, 1])\n",
      "{0: 1, 2: 0, 1: 3}\n",
      "0 1\n",
      "EU Europe 9239\n",
      "2 0\n",
      "British United_States 3434750\n",
      "1 3\n",
      "German Netherlands 21148\n",
      "{'EU': ('Europe', 9239), 'British': ('United_States', 3434750), 'German': ('Netherlands', 21148)}\n",
      "CPU times: user 26.4 ms, sys: 10 ms, total: 36.4 ms\n",
      "Wall time: 36.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test out function\n",
    "congruent_predictions = get_congruent_predictions(sentence_id=0, dataframe=predictions_pop, verbose=True)\n",
    "print(congruent_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4516/4516 [02:34<00:00, 29.14it/s] \n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "dataframe_predictions_pop = {}\n",
    "for sid in tqdm(predictions_pop['sentence_id'].unique()):\n",
    "    dataframe_predictions_pop[sid] = get_congruent_predictions(sid, dataframe=predictions_pop, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29312/29312 [00:03<00:00, 7468.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After congruence, we have achieved 57.959% accuracy comparing title.\n",
      "After congruence, we have achieved 62.066% accuracy comparing page ID.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to calculate accuracy\n",
    "accurate_predictions_title = 0\n",
    "accurate_predictions_id = 0\n",
    "for row in tqdm(range(len(predictions_pop))):\n",
    "    mention_df = predictions_pop.iloc[row]\n",
    "    sid = mention_df['sentence_id']\n",
    "    fm = mention_df['full_mention']\n",
    "    title = mention_df['wikipedia_title']\n",
    "    page_id = mention_df['wikipedia_page_ID']\n",
    "    if page_id is None:\n",
    "        pass\n",
    "    else:\n",
    "        pred = dataframe_predictions_pop[sid][fm]\n",
    "        norm_pred_title = normalize_text(pred[0])\n",
    "        pred_page_id = pred[1]\n",
    "    #     print(fm, sid, \"||| True:\", title, \"==? Pred:\", norm_pred_title, \"|||\", norm_pred_title==title,\\\n",
    "    #                  \"||| True ID: \", page_id, \"==? Pred:\", pred_page_id, \"|||\", pred_page_id==page_id)\n",
    "        if title == norm_pred_title:\n",
    "            accurate_predictions_title += 1\n",
    "        if page_id == pred_page_id:\n",
    "            accurate_predictions_id += 1\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing title.\".format(round(accurate_predictions_title/known_true*100, 3)))\n",
    "print(\"After congruence, we have achieved {}% accuracy comparing page ID.\".format(round(accurate_predictions_id/known_true*100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical Flow Demonstration\n",
    "\n",
    "The cells below have been included as a more easily understood logical flow to understand how we designed the recursive congruence algorithm for an arbitrary length of full mentions in a sentence. We manually select a sentence and work through that. This is identical to the above but with more printed out breaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Known Oddities\n",
    "1. Test with sentence_id == 1. We only return unique mentions in a single sentence. Is that ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Germany</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>germany</td>\n",
       "      <td>[11867, 250204, 21212, 12674, 33685, 662281, 1...</td>\n",
       "      <td>[183, 43310, 7318, 43287, 41304, 154408, 12031...</td>\n",
       "      <td>[Germany, Germany_national_football_team, Nazi...</td>\n",
       "      <td>[0.8896856, 0.021721, 0.0153527, 0.0140082, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>European Union</td>\n",
       "      <td>http://en.wikipedia.org/wiki/European_Union</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>European Union</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>european union</td>\n",
       "      <td>[9317, 1933156, 9317, 10890716, 276436, 265743...</td>\n",
       "      <td>[458, 1376407, 458, 185441, 208202, 319328, 36...</td>\n",
       "      <td>[European_Union, European_Boxing_Union, Europe...</td>\n",
       "      <td>[0.9884673, 0.0026042, 0.0011161, 0.0009566, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Werner Zwingmann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>werner zwingmann</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Britain</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>britain</td>\n",
       "      <td>[31717, 13530298, 152256, 158019, 13525, 4721,...</td>\n",
       "      <td>[145, 23666, 174193, 161885, 185103, 8680, 977...</td>\n",
       "      <td>[United_Kingdom, Great_Britain, United_Kingdom...</td>\n",
       "      <td>[0.5200594, 0.2035661, 0.0822639, 0.0451168, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                wikipedia_URL  \\\n",
       "0       B           Germany         http://en.wikipedia.org/wiki/Germany   \n",
       "1       B    European Union  http://en.wikipedia.org/wiki/European_Union   \n",
       "2       B  Werner Zwingmann                                          NaN   \n",
       "3       B           Britain  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            11867.0         Germany            2       0   \n",
       "1             9317.0  European Union            2       0   \n",
       "2                NaN             NaN            2       0   \n",
       "3            31717.0  United Kingdom            2       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [Germany, European Union, Werner Zwingmann, Br...           germany   \n",
       "1  [Germany, European Union, Werner Zwingmann, Br...    european union   \n",
       "2  [Germany, European Union, Werner Zwingmann, Br...  werner zwingmann   \n",
       "3  [Germany, European Union, Werner Zwingmann, Br...           britain   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [11867, 250204, 21212, 12674, 33685, 662281, 1...   \n",
       "1  [9317, 1933156, 9317, 10890716, 276436, 265743...   \n",
       "2                                                 []   \n",
       "3  [31717, 13530298, 152256, 158019, 13525, 4721,...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [183, 43310, 7318, 43287, 41304, 154408, 12031...   \n",
       "1  [458, 1376407, 458, 185441, 208202, 319328, 36...   \n",
       "2                                                 []   \n",
       "3  [145, 23666, 174193, 161885, 185103, 8680, 977...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [Germany, Germany_national_football_team, Nazi...   \n",
       "1  [European_Union, European_Boxing_Union, Europe...   \n",
       "2                                                 []   \n",
       "3  [United_Kingdom, Great_Britain, United_Kingdom...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.8896856, 0.021721, 0.0153527, 0.0140082, 0....  \n",
       "1  [0.9884673, 0.0026042, 0.0011161, 0.0009566, 0...  \n",
       "2                                                 []  \n",
       "3  [0.5200594, 0.2035661, 0.0822639, 0.0451168, 0...  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on manually selected sentence\n",
    "sentence_predictions = predictions[predictions['sentence_id'] == sentence_id].drop_duplicates(['full_mention', 'wikipedia_page_ID', 'sentence_id']).reset_index(drop=True)\n",
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Germany', 'European Union', 'Werner Zwingmann', 'Britain']\n"
     ]
    }
   ],
   "source": [
    "# Congruent Mention\n",
    "print(sentence_predictions['congruent_mentions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numerical for easier recursive logic later\n",
    "sentence_mention_nums = np.arange(len(sentence_predictions['congruent_mentions'][0]))\n",
    "sentence_mention_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate lists of vectors\n",
    "def get_candidate_pool_vectors(candidate_pool_titles, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return entity vectors from Wikipedia2Vec\n",
    "    Takes as input a list of page titles, representing the candidate pool\n",
    "    Normalizes each page title to match necessary input format\n",
    "    Returns entity vector or empty vector if no match\n",
    "    \"\"\"\n",
    "    # Track failed vector queries\n",
    "    no_vector_count = 0\n",
    "    candidate_pool_vectors = []\n",
    "    for candidate in candidate_pool_titles:\n",
    "        candidate = normalize_text(candidate)\n",
    "        try:\n",
    "            candidate_vectors = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            # Keep empty vector representation to maintain index locations\n",
    "            candidate_vectors = np.zeros(100)\n",
    "            no_vector_count += 1\n",
    "        candidate_pool_vectors.append(candidate_vectors)\n",
    "    \n",
    "    if len(candidate_pool_titles) == 0:\n",
    "        candidate_pool_vectors = [np.zeros(100), np.zeros(100), np.zeros(100)]\n",
    "    \n",
    "    if verbose: print(f\"Failed Wikipedia2Vec Entity Vector Queries: {no_vector_count}\")\n",
    "    return candidate_pool_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n"
     ]
    }
   ],
   "source": [
    "# Save vectors in dictionary\n",
    "vector_dict = {}\n",
    "\n",
    "# For each full mention we are analyzing in the contextual domain (i.e. sentence)\n",
    "for m in sentence_mention_nums:\n",
    "    \n",
    "    # Retrieve candidate pool titles\n",
    "    candidate_pool_titles = sentence_predictions['candidate_pool_titles'][m]\n",
    "    \n",
    "    # Convert candidate pool titles to candidate pool vectors\n",
    "    candidate_pool_vectors = get_candidate_pool_vectors(candidate_pool_titles, verbose=True)\n",
    "    \n",
    "    # Save candidate pool vectors to dictionary\n",
    "    vector_dict[m] = candidate_pool_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display vector_dict output\n",
    "print(vector_dict.keys())\n",
    "# Preview one candidate vector from a candidate pool vectors\n",
    "vector_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "1 2\n",
      "1 3\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# Calculate congruence metric for each candidate vector for each mention's candidate pool\n",
    "# This notebook uses cosine similarity as the congruence metric\n",
    "\n",
    "## Save congruence measurements in a two-level dictionary\n",
    "# Create first-level dictionary\n",
    "congruence_dict = {}\n",
    "\n",
    "# Always work low numbers to high\n",
    "m = 0\n",
    "while m < len(sentence_mention_nums)-1:\n",
    "    \n",
    "    # Save second-level congruence measurement dictionary\n",
    "    m_dict = {}\n",
    "    # Compare each mention against mentions after it\n",
    "    for n in sentence_mention_nums[m+1:]:\n",
    "        print(m, n)\n",
    "        # Calculate congruence measurement - cosine similarity\n",
    "        congruence_measurement = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "        # Save congruence measurement to second-level dictionary\n",
    "        m_dict[n] = congruence_measurement\n",
    "    \n",
    "    # Save second-level dictionary to first-level\n",
    "    congruence_dict[m] = m_dict\n",
    "    \n",
    "    # Increment mention\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2])\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Display congruence_dict output\n",
    "# This should be one less than congruent mention count, since we are comparing low to high\n",
    "# and thus don't compare the highest value to anything\n",
    "print(congruence_dict.keys())\n",
    "# Preview congruence matrix derived from comparing Mention 1 to Mention 2\n",
    "for k in congruence_dict.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Germany</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>germany</td>\n",
       "      <td>[11867, 250204, 21212, 12674, 33685, 662281, 1...</td>\n",
       "      <td>[183, 43310, 7318, 43287, 41304, 154408, 12031...</td>\n",
       "      <td>[Germany, Germany_national_football_team, Nazi...</td>\n",
       "      <td>[0.8896856, 0.021721, 0.0153527, 0.0140082, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>European Union</td>\n",
       "      <td>http://en.wikipedia.org/wiki/European_Union</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>European Union</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>european union</td>\n",
       "      <td>[9317, 1933156, 9317, 10890716, 276436, 265743...</td>\n",
       "      <td>[458, 1376407, 458, 185441, 208202, 319328, 36...</td>\n",
       "      <td>[European_Union, European_Boxing_Union, Europe...</td>\n",
       "      <td>[0.9884673, 0.0026042, 0.0011161, 0.0009566, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Werner Zwingmann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>werner zwingmann</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Britain</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>britain</td>\n",
       "      <td>[31717, 13530298, 152256, 158019, 13525, 4721,...</td>\n",
       "      <td>[145, 23666, 174193, 161885, 185103, 8680, 977...</td>\n",
       "      <td>[United_Kingdom, Great_Britain, United_Kingdom...</td>\n",
       "      <td>[0.5200594, 0.2035661, 0.0822639, 0.0451168, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                wikipedia_URL  \\\n",
       "0       B           Germany         http://en.wikipedia.org/wiki/Germany   \n",
       "1       B    European Union  http://en.wikipedia.org/wiki/European_Union   \n",
       "2       B  Werner Zwingmann                                          NaN   \n",
       "3       B           Britain  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            11867.0         Germany            2       0   \n",
       "1             9317.0  European Union            2       0   \n",
       "2                NaN             NaN            2       0   \n",
       "3            31717.0  United Kingdom            2       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [Germany, European Union, Werner Zwingmann, Br...           germany   \n",
       "1  [Germany, European Union, Werner Zwingmann, Br...    european union   \n",
       "2  [Germany, European Union, Werner Zwingmann, Br...  werner zwingmann   \n",
       "3  [Germany, European Union, Werner Zwingmann, Br...           britain   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [11867, 250204, 21212, 12674, 33685, 662281, 1...   \n",
       "1  [9317, 1933156, 9317, 10890716, 276436, 265743...   \n",
       "2                                                 []   \n",
       "3  [31717, 13530298, 152256, 158019, 13525, 4721,...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [183, 43310, 7318, 43287, 41304, 154408, 12031...   \n",
       "1  [458, 1376407, 458, 185441, 208202, 319328, 36...   \n",
       "2                                                 []   \n",
       "3  [145, 23666, 174193, 161885, 185103, 8680, 977...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [Germany, Germany_national_football_team, Nazi...   \n",
       "1  [European_Union, European_Boxing_Union, Europe...   \n",
       "2                                                 []   \n",
       "3  [United_Kingdom, Great_Britain, United_Kingdom...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.8896856, 0.021721, 0.0153527, 0.0140082, 0....  \n",
       "1  [0.9884673, 0.0026042, 0.0011161, 0.0009566, 0...  \n",
       "2                                                 []  \n",
       "3  [0.5200594, 0.2035661, 0.0822639, 0.0451168, 0...  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "# Pick mentions to compare\n",
    "men1 = np.random.choice(sentence_mention_nums[:-1])\n",
    "men2 = np.random.choice(sentence_mention_nums)\n",
    "while men1 == men2:\n",
    "    men2 = np.random.choice(sentence_mention_nums)\n",
    "men1, men2 = np.sort([men1, men2])\n",
    "print(men1, men2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_8f508dee_35ae_11eb_bf35_acde48001122row0_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row0_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row0_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row1_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row1_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row1_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row2_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row2_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row2_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row3_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row3_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row3_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row4_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row4_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row4_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row5_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row5_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row5_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row6_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row6_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row6_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row7_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row7_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row7_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row8_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row8_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row8_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row9_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row9_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row9_col2{\n",
       "            background:  skyblue;\n",
       "        }</style><table id=\"T_8f508dee_35ae_11eb_bf35_acde48001122\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row0_col2\" class=\"data row0 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row3_col0\" class=\"data row3 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row3_col1\" class=\"data row3 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row3_col2\" class=\"data row3 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row4_col0\" class=\"data row4 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row4_col1\" class=\"data row4 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row4_col2\" class=\"data row4 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row5_col0\" class=\"data row5 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row5_col1\" class=\"data row5 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row5_col2\" class=\"data row5 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row6_col0\" class=\"data row6 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row6_col1\" class=\"data row6 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row6_col2\" class=\"data row6 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row7_col0\" class=\"data row7 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row7_col1\" class=\"data row7 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row7_col2\" class=\"data row7 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row8_col0\" class=\"data row8 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row8_col1\" class=\"data row8 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row8_col2\" class=\"data row8 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row9_col0\" class=\"data row9 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row9_col1\" class=\"data row9 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row9_col2\" class=\"data row9 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f8bbee74f50>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congruence_test_df = pd.DataFrame(congruence_dict[men1][men2])\n",
    "max_num = max(np.max(congruence_test_df))\n",
    "congruence_test_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-2ee20f6677ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmatrix_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmatrix_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmax_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmatrix_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"background: skyblue\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_num\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "matrix_test = []\n",
    "for a in sentence_predictions['candidate_pool_likelihoods'][men1]:\n",
    "    row_test = []\n",
    "    for b in sentence_predictions['candidate_pool_likelihoods'][men2]:\n",
    "        row_test.append(np.mean([a, b]))\n",
    "    matrix_test.append(row_test)\n",
    "matrix_df = pd.DataFrame(matrix_test)\n",
    "max_num = max(np.max(matrix_df))\n",
    "matrix_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,0) (10,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-fe69072df3f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweighted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcongruence_test_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mweighted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweighted_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"background: skyblue\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_num\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,0) (10,3) "
     ]
    }
   ],
   "source": [
    "weighted_df = np.array(matrix_df) * np.array(congruence_test_df)\n",
    "weighted_df = pd.DataFrame(weighted_df)\n",
    "max_num = max(np.max(weighted_df))\n",
    "weighted_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  4\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "Length:  4\n",
      "1 2\n",
      "1 3\n",
      "Length:  4\n",
      "2 3\n",
      "Length:  4\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of logic to ensure unique mention congruence only calculated once\n",
    "m = 0\n",
    "while m < len(sentence_mention_nums):\n",
    "    print(\"Length: \", len(sentence_mention_nums))\n",
    "    for i in sentence_mention_nums[m+1:]:\n",
    "        print(m, i)\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 1), 0.8189154129945448)\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 2), 0.6412995855581739)\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((1, 1), 0.6947784687002414)\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((1, 1), 0.6947784687002414), 1, 2)\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "most_congruent_pair = (None, 0.0, 0, 0) # Most Congruent Candidates, Congruence Metric, Mention A, Mention B\n",
    "\n",
    "for m in sentence_mention_nums:\n",
    "    for n in sentence_mention_nums[m+1:]:\n",
    "        print(f\"Comparing {m} & {n}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m][n])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, m, n\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save max congruent pair estimates for each mention\n",
    "mention_predictions = {}\n",
    "mention_predictions[most_congruent_pair[1]] = most_congruent_pair[0][0][0]\n",
    "mention_predictions[most_congruent_pair[2]] = most_congruent_pair[0][0][1]\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have predictions for  dict_keys([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# With two mentions set in their predictions, we must filter the other congruent matrices to find the next most\n",
    "print(\"We have predictions for \", mention_predictions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(sentence_mention_nums) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "MAX: (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "MAX: (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 1), 0.8189154129945448)\n",
      "MAX: (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "MAX: (((1, 1), 0.8189154129945448), (0, 2))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((1, 1), 0.8189154129945448), (0, 2))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1, 0: 1}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "MAX: (((None, None), 0.0), (0, 0))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "MAX: (((0, 4), 0.6645409154870272), (1, 3))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 2), 0.6412995855581739)\n",
      "MAX: (((0, 6), 0.78401095), (2, 3))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((0, 6), 0.78401095), (2, 3))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1, 0: 1, 3: 6}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've predicted everything!\n"
     ]
    }
   ],
   "source": [
    "if len(mentions_remaining) == 0:\n",
    "    print(\"You've predicted everything!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
