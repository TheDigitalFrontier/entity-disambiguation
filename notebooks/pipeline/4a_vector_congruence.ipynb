{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congruence via Entity Vector Similarity\n",
    "\n",
    "In this notebook, we calculate the cosine similarity measure between vectors of each candidate. The vectors will be retrieved from Wikipedia2vec's pre-trained API, which creates vectors for the entire Wikipedia page including its relational links. Comparing two vectors in this way thus lets us make a statement about similar pages and update our prior confidence based on that.\n",
    "\n",
    "#### (i) Using Prior Confidence\n",
    "\n",
    "In Phase 4, we calculate congruence between candidates in pools for mentions in the same sentence. We then update these congruence metrics using prior confidence from Phase 3. This should adjust our pool by discounting congruence using the prior knowledge we have of the calculated similarity. In this notebook, we import prior knowledge generated from Anchor Link statistics and weight those with our numerical calculations from congruence. We demonstrate the performance gains of directly incorporating prior confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 21347120, 9477, 1882861]</td>\n",
       "      <td>[458, 46, 211593, 1396, 363404]</td>\n",
       "      <td>['European_Union', 'Europe', 'Eu,_Seine-Mariti...</td>\n",
       "      <td>[0.9227799, 0.024651, 0.020196, 0.005346, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 11884, 152735, 21212, 12674]</td>\n",
       "      <td>[183, 188, 42884, 7318, 43287]</td>\n",
       "      <td>['Germany', 'German_language', 'Germans', 'Naz...</td>\n",
       "      <td>[0.4192066, 0.2893363, 0.1470461, 0.03832, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>british</td>\n",
       "      <td>[31717, 19097669, 13530298, 4721, 158019]</td>\n",
       "      <td>[145, 842438, 23666, 8680, 161885]</td>\n",
       "      <td>['United_Kingdom', 'British_people', 'Great_Br...</td>\n",
       "      <td>[0.6101256, 0.1146913, 0.0681775, 0.0366451, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.5, 0.3, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 9643132, 56873217]</td>\n",
       "      <td>[2073954, 7172840, 26634508]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.5, 0.3, 0.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention     full_mention                                wikipedia_URL  \\\n",
       "0       B               EU                                          NaN   \n",
       "1       B           German         http://en.wikipedia.org/wiki/Germany   \n",
       "2       B          British  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "3       B  Peter Blackburn                                          NaN   \n",
       "4       I  Peter Blackburn                                          NaN   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0                NaN             NaN            0       0   \n",
       "1            11867.0         Germany            0       0   \n",
       "2            31717.0  United Kingdom            0       0   \n",
       "3                NaN             NaN            1       0   \n",
       "4                NaN             NaN            1       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0                        ['EU', 'German', 'British']                eu   \n",
       "1                        ['EU', 'German', 'British']            german   \n",
       "2                        ['EU', 'German', 'British']           british   \n",
       "3  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "4  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "\n",
       "                     candidate_pool_page_ids  \\\n",
       "0      [9317, 9239, 21347120, 9477, 1882861]   \n",
       "1       [11867, 11884, 152735, 21212, 12674]   \n",
       "2  [31717, 19097669, 13530298, 4721, 158019]   \n",
       "3              [56783206, 9643132, 56873217]   \n",
       "4              [56783206, 9643132, 56873217]   \n",
       "\n",
       "              candidate_pool_item_ids  \\\n",
       "0     [458, 46, 211593, 1396, 363404]   \n",
       "1      [183, 188, 42884, 7318, 43287]   \n",
       "2  [145, 842438, 23666, 8680, 161885]   \n",
       "3        [2073954, 7172840, 26634508]   \n",
       "4        [2073954, 7172840, 26634508]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  ['European_Union', 'Europe', 'Eu,_Seine-Mariti...   \n",
       "1  ['Germany', 'German_language', 'Germans', 'Naz...   \n",
       "2  ['United_Kingdom', 'British_people', 'Great_Br...   \n",
       "3  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "4  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "\n",
       "                               candidate_pool_priors  \n",
       "0  [0.9227799, 0.024651, 0.020196, 0.005346, 0.00...  \n",
       "1  [0.4192066, 0.2893363, 0.1470461, 0.03832, 0.0...  \n",
       "2  [0.6101256, 0.1146913, 0.0681775, 0.0366451, 0...  \n",
       "3                                    [0.5, 0.3, 0.2]  \n",
       "4                                    [0.5, 0.3, 0.2]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base path to input\n",
    "preds_path = '../../predictions/'\n",
    "\n",
    "# Load data\n",
    "full_mentions = pd.read_csv(os.path.join(preds_path, \"anchortext_frequency_5x5.csv\"), delimiter=\",\")\n",
    "full_mentions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Saved Candidate Pool\n",
    "\n",
    "Candidate pools when exported to csv are stored as the string of the list. The below function parses the string back into a list with proper formatted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate that list is string\n",
    "type(full_mentions['candidate_pool_page_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse list as string\n",
    "def parse_list_string(list_string, value_type=int):\n",
    "    \n",
    "    parsed_list = []\n",
    "    \n",
    "    # If candidate pool is empty\n",
    "    if list_string == \"[]\" or isinstance(list_string, float):\n",
    "        pass\n",
    "    # Else parse\n",
    "    else:\n",
    "        # Parses lists of titles as strings\n",
    "        if value_type==str:\n",
    "            # Eliminate bracket and parenthesis on either side, split by comma pattern\n",
    "            parsed_list = re.split(\"', '|\\\", \\\"|', \\\"|\\\", \\'\", list_string[2:-2])\n",
    "\n",
    "        # Parses lists of IDs as ints\n",
    "        elif value_type==int:\n",
    "            # Eliminate brackets and convert each number from string to int\n",
    "            parsed_list = list(map(int, list_string[1:-1].split(', ')))\n",
    "        elif value_type==float:\n",
    "            # Eliminate brackets and convert each number from string to int\n",
    "            parsed_list = list(map(float, list_string[1:-1].split(', ')))\n",
    "            \n",
    "        \n",
    "    return parsed_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['European_Union',\n",
       " 'Europe',\n",
       " 'Eu,_Seine-Maritime',\n",
       " 'Europium',\n",
       " 'Citizenship_of_the_European_Union']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "# 0 is the hard one. See how some value is stored with '' and some with \"\". Unsure why.\n",
    "parse_list_string(full_mentions['candidate_pool_titles'][0], value_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9317, 9239, 21347120, 9477, 1882861]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "# 0 is the hard one. See how some value is stored with '' and some with \"\". Unsure why.\n",
    "parse_list_string(full_mentions['candidate_pool_page_ids'][0], value_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "parse_list_string(full_mentions['candidate_pool_page_ids'][13], value_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9227799, 0.024651, 0.020196, 0.005346, 0.002079]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually test function\n",
    "parse_list_string(full_mentions['candidate_pool_priors'][0], value_type=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "After ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "Before [56783206, 9643132, 56873217]\n",
      "After [56783206, 9643132, 56873217]\n",
      "Before [2073954, 7172840, 26634508]\n",
      "After [2073954, 7172840, 26634508]\n",
      "Before ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(bishop)', 'Peter_Blackburn_(MP)']\n",
      "After ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(bishop)', 'Peter_Blackburn_(MP)']\n",
      "Before [0.5, 0.3, 0.2]\n",
      "After [0.5, 0.3, 0.2]\n",
      "CPU times: user 189 ms, sys: 75.8 ms, total: 264 ms\n",
      "Wall time: 263 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Apply defined function to entire dataframe for all candidate pool columns\n",
    "\n",
    "column = 'congruent_mentions'\n",
    "print(\"Before\", full_mentions[column][3])\n",
    "parsed_candidate_pool = full_mentions[column].apply(parse_list_string, value_type=str)\n",
    "full_mentions[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions[column][3])\n",
    "\n",
    "column = 'candidate_pool_page_ids'\n",
    "print(\"Before\", full_mentions[column][3])\n",
    "parsed_candidate_pool = full_mentions[column].apply(parse_list_string, value_type=int)\n",
    "full_mentions[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_item_ids'\n",
    "print(\"Before\", full_mentions[column][3])\n",
    "parsed_candidate_pool = full_mentions[column].apply(parse_list_string, value_type=int)\n",
    "full_mentions[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_titles'\n",
    "print(\"Before\", full_mentions[column][3])\n",
    "parsed_candidate_pool = full_mentions[column].apply(parse_list_string, value_type=str)\n",
    "full_mentions[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions[column][3])\n",
    "\n",
    "column = 'candidate_pool_priors'\n",
    "print(\"Before\", full_mentions[column][3])\n",
    "parsed_candidate_pool = full_mentions[column].apply(parse_list_string, value_type=float)\n",
    "full_mentions[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions[column][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Entity Vectors from Wikipedia2Vec\n",
    "\n",
    "For provided wikipedia pages, we retrieve a representative entity vector from Wikipedia2vec. This involves passing the normalized title into their get_entity_vector() function. We default to using 100d pre-trained embeddings due to the easier computational requirements, but tested results using higher dimensions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package\n",
    "from wikipedia2vec import Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 102 ms, sys: 122 ms, total: 224 ms\n",
      "Wall time: 277 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load unzipped pkl file with word embeddings\n",
    "w2v = Wikipedia2Vec.load(\"../../embeddings/enwiki_20180420_100d.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Coverage of Candidate Pools in Wikipedia2vec\n",
    "\n",
    "We need to measure what percent of candidates in our candidate pools successfully return a vector from Wikipedia2vec. This should conceivably be 100% given we're passing known Wikipedia pages into this package trained over Wikipedia pages, but there may be some drop-off due to different creation dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text normalization function\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    We define normalized in this notebook as:\n",
    "    - strip whitespace\n",
    "    - Spaces, not underlines\n",
    "    \"\"\"\n",
    "    return str(text).strip().replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13781/13781 [00:01<00:00, 10736.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia2vec returned an entity vector for 94.792% of 42,453 searches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over candidate pool titles to see what can be returned\n",
    "\n",
    "found_entity = 0\n",
    "searched_entity = 0\n",
    "\n",
    "for i in tqdm(range(len(full_mentions))):\n",
    "    \n",
    "    # Retrieve candidate pool\n",
    "    candidate_pool = full_mentions['candidate_pool_titles'][i]\n",
    "    \n",
    "    # Query for each candidate\n",
    "    for candidate in candidate_pool:\n",
    "        # Normalize candidate title to form necessary to input into Wikipedia2vec\n",
    "        candidate = normalize_text(candidate)\n",
    "        \n",
    "        # Query Wikipedia2vec get_entity_vector()\n",
    "        try:\n",
    "            entity_vector = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            entity_vector = None\n",
    "        \n",
    "        # Check if result\n",
    "        if entity_vector is not None:\n",
    "            found_entity += 1\n",
    "        \n",
    "        # Increment count\n",
    "        searched_entity += 1\n",
    "\n",
    "print(f\"Wikipedia2vec returned an entity vector for {round(found_entity/searched_entity*100,3)}% of {searched_entity:,} searches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Sentence Properties\n",
    "\n",
    "First, let's get a sense for what the upper bound of congruent calculations might be per sentence as well as the distribution of counts and average priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the maximum number of congruent entities in a single sentence\n",
    "max(full_mentions['congruent_mentions'].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What is the distribution of congruent mention counts\n",
    "plt.figure(figsize=(20,5))\n",
    "full_mentions['congruent_mentions'].apply(len).hist(bins=50)\n",
    "plt.title(\"distribution of congruent mention counts\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get a sense for the average ranking of the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13781/13781 [00:01<00:00, 7412.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer not present in candidate pool 6.9% of the time.\n",
      "Correct answer null 29.0% of the time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare tracking metrics\n",
    "correct_answer_rank = []\n",
    "correct_answer_prior = {0:[],1:[],2:[],3:[],4:[]}\n",
    "correct_answer_not_present = 0\n",
    "correct_answer_null = 0\n",
    "\n",
    "# Iterate over whole dataframe\n",
    "for i in tqdm(range(len(full_mentions))):\n",
    "    row = full_mentions.iloc[i]\n",
    "    correct_answer = row['wikipedia_page_ID']\n",
    "    if isinstance(row['wikipedia_title'], float):\n",
    "        correct_answer_null += 1\n",
    "    else:\n",
    "        try:\n",
    "            correct_rank = row['candidate_pool_page_ids'].index(correct_answer)\n",
    "            correct_answer_rank.append(correct_rank)\n",
    "            correct_answer_prior[correct_rank].append(row['candidate_pool_priors'][correct_rank])\n",
    "        except ValueError:\n",
    "            correct_answer_not_present += 1\n",
    "\n",
    "print(f\"Correct answer not present in candidate pool {round(correct_answer_not_present/len(full_mentions)*100,1)}% of the time.\")\n",
    "print(f\"Correct answer null {round(correct_answer_null/len(full_mentions)*100,1)}% of the time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIoAAAHiCAYAAACHjidlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAo3ElEQVR4nO3dfdRl1V0n+O8vkBA0IhAKhCre1Jr0AE6IlEgm3XY6qBRGAzMtrtI2YCZaNoOajG8N6W5NWrEZV7cd0UBLXqQwKl2d1zIJUYY2ydiSkCImIhAmlUBCNQVUSEiIiUTIb/64u/T68FTVraqH5yme+nzWuuueu88+5+xz9j2r6vmuffat7g4AAAAAPG2pGwAAAADA/kFQBAAAAEASQREAAAAAg6AIAAAAgCSCIgAAAAAGQREAAAAASQRFALDfqKpXV9Wb92H7G6rqooVs04zHfUFVfaKqvlRV5y/28dl/VNVJVdVVdfD4vNPv5Ny6T1XL5TwAYAdBEQAsgKr6saq6raq+XFX3V9XVVXX4Yrahu8/t7g2Leczh3yX57e5+Vne/Y74KVfUjVbV5hEnbRoDwjxe3mfO2656q+u4Z6p1cVV+rqqsWo10Lbamu/0J9J6vqhVW1dR+3/9o4/0eq6q6qetm+tgsAliNBEQDso6r6uST/d5JfSPKNSc5KcmKSG6vqGTvZZsFGH9TEUv6bfmKS23e2sqp+Nslrk/xakmOSnJDkqiTn7emB5rtuizSS48Ikn0+yrqoOWYTj7bGdfQ8W8vo/xd3X3c9KcliS/yvJ66vqOUvcJgDY7wiKAGAfVNVhSV6T5Ke7+73d/bfdfU+SH8okQPnRUe/VVfWWqnpzVX0xyY+NUSrvHyMcbkxy1Jx9n1VVf15VD1fVx6rqhVPr3ldVl1fVf0/y5STfPMp+fKz/sar6s6r6D1X1+aq6u6rOndr+5Kr6wDj2/1NVr9vVY29V9RNVtaWqPldVm6rquFH+ySTfnOSPxmiNQ+Zs942ZjDi6pLvf1t1/Pa7RH3X3L4w6h1TVa6vqvvF67Y797BhJUlX/qqruT/K7O7mW31hVbxyjZf5HVf1qVR00p/13jvO9o6q+vap+L5PQZEfbf3EXXX1hkn+T5G+T/MCcc+yq+pc1efzu8+Na1lj3raOPv1BVn62q/zLKX1NVvzWWn15Vf11Vvz4+H1pVf1NVR+zN92Avrv+ZVXXz2P+2qvrt6YBzN+d30PiOfbaqPpXkxXOOP/2d3F3dl0310aeq6idH+dcnuSHJcaOfvlRVx1XV06rq0qr6ZFU9VFUbq+rIXfRhkqQn3pPkc0n+l3GMI6rqXVW1fZzju6pq1Zzz+JWq+u+jfX9SVUfNt/+q+uc1Gal22u7aAgD7I0ERAOyb/zXJM5O8bbqwu7+UyR+33zNVfF6StyQ5PMnvJ/mDJLdmEhD9SpK/m8ulqlYmeXeSX01yZJKfT/LWqloxtb+XJlmf5BuSfHqetn1nkrvG/n89yRt3/IE/jn1LkmcnefXY17yq6kVJ/n0m4dex41jXj/P8liSfSfID49GzR+ds/vxMrs/bd7b/JP86k1FYpyd5bpIzMwlldvimTK7BieN8kydeyw1JHkvyrUmel+R7k+wIKC4Y53hhJqNJXpLkoe5+6Zy2//pOzv+fJFk1znnj2M9c35/kO0b7fyjJOaP8V5L8SZIjxj5+a5S/P8kLx/J3JLk/yT8dn5+f5K7u/vwCfA9muf6PZzLC5qhR/+wk/+eM5/cTY93zkqxJ8oO7OM7u6j441h+W5GVJ/lNVfXt3/3WSczNGBI3XfUl+Jsn5mVy34zIZ8fW6XRw/STICppeM890yip+W5Hcz+Y6dkOQrSX57zqY/Mtp1dJJnZNIXc/f9skxGF353d//V7toCAPsjQREA7Jujkny2ux+bZ922/MNRQjd39zu6+2tJVmTyh/e/7e5Hu/sDSf5oqu6PJnlPd7+nu7/W3Tcm2Zzk+6bqXNvdt3f3Y939t/Mc/9Pd/frufjyTIOXYJMdU1Qnj2L/U3V/t7j9LsmkX5/gvkrypuz8ygqDLkjy/qk7axTY7PDs7vz7T+/933f1gd2/PZITWdHD1tSS/PK7TV0bZ9LU8LJMg4ZVjxMyDSf5TknWj7o8n+fXu/vAYTbKlu+cL1nbmoiQ3dPfnMwnYzq2qo+fUuaK7H+7uzyT500xCr2QyAunEJMd199+Ma50kNydZXVXPTvJdSd6YZGVVPSuT4OP9o96+fg92e/27+9bu/uDY/p4kv5O/D612d34/lOS13X1vd38uk0BxZ3ZZt7vf3d2fHH30/kwCtn+yi/39ZJJ/3d1bx/fy1Ul+sHb+KOJxVfVwJiHQ25P8bHf/xTj2Q9391u7+cnc/kuTyea7B73b3/ze+gxunrsEOr8zk8dMXdveWAMBTlKAIAPbNZ5MctZM/To8d63e4d2r5uCSfH6MldpgOL05McsF4HOjh8QfuPx77nG9/87l/x0J3f3ksPmsc+3NTZbvb13HTbRujpR5KsnI3x8+ot7PrM+/+x/JxU5+3d/ffzNlmur0nJnl6km1T1+p3Mhn5kSTHJ/nkDG19gqo6NMkFmYxaSnffnMkopB+ZU/X+qeUvZ3Kdk+QXk1SSW6rq9qr6P8Z+vpJJ4PNPMwmK3p/kz5O8IP8wKNrX78Fur39V/U/jUav7a/Io369lzmOQuzi/4+Ycf1cB3C7rVtW5VfXBmjze+HAmYdi8j3cNJyZ5+9R1uTOT0VHH7KT+fd19eCbB4pVJXjR17K+rqt+pqk+Pa/CBJIfX1OOL2fk12OEXkryuu/d60m0A2B8IigBg39yc5NEk//t04ZhX5dwkN00V99TytiRHjHo7nDC1fG+S3+vuw6deX9/dV+xkf3tiW5Ijq+rrpsqO30X9+zL5ozzJ353bs5P8jxmOdXOSv8nkEaGZ9p/Jdbhv6vN85zlddm8mfXDU1LU6rLtPnVr/LTs59u6u4f+WSbBw1QhS7s8kIJvv8bMn7rz7/u7+ie4+LpMRMFdV1beO1e/PJKx4XpIPj8/nZPLo3Qem2r4v34NZrv/VST6eZHV3H5bkVZmEW7PYln/43TlhZxV3Vbcmc1K9Ncl/SHLMCHTeM9WO+c7x3iTnzrk2z+zuXX4vx+ijf5Xk26rq/FH8c0mek+Q7xzX4rh1N29W+5vjeJP+mqv75HmwDAPsdQREA7IPu/kImj0r9VlWtrcnExCcl+a9Jtib5vZ1s9+lMRpS8pqqeUZOfKp+eJPnNSX6gqs4ZkwA/syYTO6+ab3972OYdx371OPbz5xx7rj9I8rKqOn38Qf9rST40HlPa3bG+kOSXkryuqs4fIzeePkaP7JgT6A8z+QN7xZgg+JcyOf9Zz2dbJo8p/ceqOmzMQfMtVbXj0aE3JPn5qjqjJr61qnYEUw9kzgTQc1yU5E1Jvi2TR41Oz2TUz+lV9W27a1tVXTDVZ5/PJPB4fHx+fyaB0x3d/dUk78vkMbm7xyN4yT5+D2a8/t+Q5ItJvlRV/yjJxbPse9iY5GeqalVNJt++dC/rPiPJIUm2J3msJhOvf+/U+geSPLsmk3Pv8J+TXL6jL8f3Z6ZfchvX+z9mcm2SyTX4SpKHazIh9i/Psp85bk+yNpNr/ZK92B4A9guCIgDYR2MS5FdlMhrii0k+lMloh7P7iZM7T/uRTCac/lwmf5heN7XPezOZsPlVmfzxfG8mj7Ys1L/d/yKTiYsfymSi5P+SyaicJ+jum5L820xGfGzLZHTOuvnq7mT730jys5lMUL3jXH4qyTtGlV/NJLj6yyS3JfnIKNsTF2YSNtyRSSDzlozHs7r7v2Yy58wfJHlkHHfHr2P9+0xCqoer6h9MTjwmkj47k3l17p963ZrkvZmafHwXviPJh6rqS5nMA/WK7r57rPvzJIfm70cP3ZHJ6J8dnxfkezDD9f/5TL6LjyR5fSbfhVm9PskfJ/lYJv32tr2pO+YF+plMwqTPj/Zsmlr/8UwCxU+NvjouyW+OOn9SVY8k+WAm99Os3pTkhKr6gSSvzaQvPjv289492M/f6e6PZTIh9+tr6lcGAeCppLr3dtQ6ALBc1ORn2z/e3XszkgIAgGXCiCIAOABV1XeMx7OeVlVrMxm18o4lbhYAAEtsV79AAgAsX9+UyaM/z85kLqWLd/xUOAAABy6PngEAAACQxKNnAAAAAAyCIgAAAACSPAXmKDrqqKP6pJNOWupmAAAAACwbt95662e7e8Xc8v0+KDrppJOyefPmpW4GAAAAwLJRVZ+er9yjZwAAAAAkERQBAAAAMAiKAAAAAEgiKAIAAABgEBQBAAAAkERQBAAAAMAgKAIAAAAgiaAIAAAAgEFQBAAAAEASQREAAAAAg6AIAAAAgCSCIgAAAAAGQREAAAAASQRFAAAAAAyCIgAAAACSzBAUVdVzquqjU68vVtUrq+rIqrqxqj4x3o+Y2uayqtpSVXdV1TlT5WdU1W1j3ZVVVU/WiQEAAACwZ3YbFHX3Xd19enefnuSMJF9O8vYklya5qbtXJ7lpfE5VnZJkXZJTk6xNclVVHTR2d3WS9UlWj9faBT0bAAAAAPbanj56dnaST3b3p5Ocl2TDKN+Q5PyxfF6S67v70e6+O8mWJGdW1bFJDuvum7u7k1w3tQ0AAAAAS2xPg6J1Sf5wLB/T3duSZLwfPcpXJrl3aputo2zlWJ5bDgAAAMB+4OBZK1bVM5K8JMllu6s6T1nvony+Y63P5BG1nHDCCbM2cb920qXvXuomLJh7rnjxUjcBAAAAeBLsyYiic5N8pLsfGJ8fGI+TZbw/OMq3Jjl+artVSe4b5avmKX+C7r6mu9d095oVK1bsQRMBAAAA2Ft7EhT9cP7+sbMk2ZTkorF8UZJ3TpWvq6pDqurkTCatvmU8nvZIVZ01fu3swqltAAAAAFhiMz16VlVfl+R7kvzkVPEVSTZW1cuTfCbJBUnS3bdX1cYkdyR5LMkl3f342ObiJNcmOTTJDeMFAAAAwH5gpqCou7+c5Nlzyh7K5FfQ5qt/eZLL5ynfnOS0PW8mAAAAAE+2Pf3VMwAAAACWKUERAAAAAEkERQAAAAAMgiIAAAAAkgiKAAAAABgERQAAAAAkERQBAAAAMAiKAAAAAEgiKAIAAABgEBQBAAAAkERQBAAAAMAgKAIAAAAgiaAIAAAAgEFQBAAAAEASQREAAAAAg6AIAAAAgCSCIgAAAAAGQREAAAAASQRFAAAAAAyCIgAAAACSCIoAAAAAGARFAAAAACQRFAEAAAAwCIoAAAAASCIoAgAAAGAQFAEAAACQRFAEAAAAwCAoAgAAACCJoAgAAACAQVAEAAAAQBJBEQAAAACDoAgAAACAJIIiAAAAAAZBEQAAAABJBEUAAAAADIIiAAAAAJIIigAAAAAYBEUAAAAAJBEUAQAAADAIigAAAABIIigCAAAAYBAUAQAAAJBEUAQAAADAICgCAAAAIImgCAAAAIBBUAQAAABAEkERAAAAAIOgCAAAAIAkgiIAAAAABkERAAAAAElmDIqq6vCqektVfbyq7qyq51fVkVV1Y1V9YrwfMVX/sqraUlV3VdU5U+VnVNVtY92VVVVPxkkBAAAAsOdmHVH0m0ne293/KMlzk9yZ5NIkN3X36iQ3jc+pqlOSrEtyapK1Sa6qqoPGfq5Osj7J6vFau0DnAQAAAMA+2m1QVFWHJfmuJG9Mku7+anc/nOS8JBtGtQ1Jzh/L5yW5vrsf7e67k2xJcmZVHZvksO6+ubs7yXVT2wAAAACwxGYZUfTNSbYn+d2q+ouqekNVfX2SY7p7W5KM96NH/ZVJ7p3afusoWzmW55YDAAAAsB+YJSg6OMm3J7m6u5+X5K8zHjPbifnmHepdlD9xB1Xrq2pzVW3evn37DE0EAAAAYF/NEhRtTbK1uz80Pr8lk+DogfE4Wcb7g1P1j5/aflWS+0b5qnnKn6C7r+nuNd29ZsWKFbOeCwAAAAD7YLdBUXffn+TeqnrOKDo7yR1JNiW5aJRdlOSdY3lTknVVdUhVnZzJpNW3jMfTHqmqs8avnV04tQ0AAAAAS+zgGev9dJLfr6pnJPlUkpdlEjJtrKqXJ/lMkguSpLtvr6qNmYRJjyW5pLsfH/u5OMm1SQ5NcsN4AQAAALAfmCko6u6PJlkzz6qzd1L/8iSXz1O+Oclpe9A+AAAAABbJLHMUAQAAAHAAEBQBAAAAkERQBAAAAMAgKAIAAAAgiaAIAAAAgEFQBAAAAEASQREAAAAAg6AIAAAAgCSCIgAAAAAGQREAAAAASQRFAAAAAAyCIgAAAACSCIoAAAAAGARFAAAAACQRFAEAAAAwCIoAAAAASCIoAgAAAGAQFAEAAACQRFAEAAAAwCAoAgAAACCJoAgAAACAQVAEAAAAQBJBEQAAAACDoAgAAACAJIIiAAAAAAZBEQAAAABJBEUAAAAADIIiAAAAAJIIigAAAAAYBEUAAAAAJBEUAQAAADAIigAAAABIIigCAAAAYBAUAQAAAJBEUAQAAADAICgCAAAAIImgCAAAAIBBUAQAAABAEkERAAAAAIOgCAAAAIAkgiIAAAAABkERAAAAAEkERQAAAAAMgiIAAAAAkgiKAAAAABgERQAAAAAkERQBAAAAMAiKAAAAAEgiKAIAAABgEBQBAAAAkERQBAAAAMAwU1BUVfdU1W1V9dGq2jzKjqyqG6vqE+P9iKn6l1XVlqq6q6rOmSo/Y+xnS1VdWVW18KcEAAAAwN7YkxFF/6y7T+/uNePzpUlu6u7VSW4an1NVpyRZl+TUJGuTXFVVB41trk6yPsnq8Vq776cAAAAAwELYl0fPzkuyYSxvSHL+VPn13f1od9+dZEuSM6vq2CSHdffN3d1JrpvaBgAAAIAlNmtQ1En+pKpurar1o+yY7t6WJOP96FG+Msm9U9tuHWUrx/Lc8ieoqvVVtbmqNm/fvn3GJgIAAACwLw6esd4Luvu+qjo6yY1V9fFd1J1v3qHeRfkTC7uvSXJNkqxZs2beOgAAAAAsrJlGFHX3feP9wSRvT3JmkgfG42QZ7w+O6luTHD+1+aok943yVfOUAwAAALAf2G1QVFVfX1XfsGM5yfcm+askm5JcNKpdlOSdY3lTknVVdUhVnZzJpNW3jMfTHqmqs8avnV04tQ0AAAAAS2yWR8+OSfL28Uv2Byf5g+5+b1V9OMnGqnp5ks8kuSBJuvv2qtqY5I4kjyW5pLsfH/u6OMm1SQ5NcsN4AQAAALAf2G1Q1N2fSvLcecofSnL2Tra5PMnl85RvTnLanjcTAAAAgCfbrL96BgAAAMAyJygCAAAAIImgCAAAAIBBUAQAAABAEkERAAAAAIOgCAAAAIAkgiIAAAAABkERAAAAAEkERQAAAAAMgiIAAAAAkgiKAAAAABgERQAAAAAkERQBAAAAMAiKAAAAAEgiKAIAAABgEBQBAAAAkERQBAAAAMAgKAIAAAAgiaAIAAAAgEFQBAAAAEASQREAAAAAg6AIAAAAgCSCIgAAAAAGQREAAAAASQRFAAAAAAyCIgAAAACSCIoAAAAAGARFAAAAACQRFAEAAAAwCIoAAAAASCIoAgAAAGAQFAEAAACQRFAEAAAAwCAoAgAAACCJoAgAAACAQVAEAAAAQBJBEQAAAACDoAgAAACAJIIiAAAAAAZBEQAAAABJBEUAAAAADIIiAAAAAJIIigAAAAAYBEUAAAAAJBEUAQAAADAIigAAAABIIigCAAAAYBAUAQAAAJBEUAQAAADAICgCAAAAIMkeBEVVdVBV/UVVvWt8PrKqbqyqT4z3I6bqXlZVW6rqrqo6Z6r8jKq6bay7sqpqYU8HAAAAgL21JyOKXpHkzqnPlya5qbtXJ7lpfE5VnZJkXZJTk6xNclVVHTS2uTrJ+iSrx2vtPrUeAAAAgAUzU1BUVauSvDjJG6aKz0uyYSxvSHL+VPn13f1od9+dZEuSM6vq2CSHdffN3d1JrpvaBgAAAIAlNuuIotcm+cUkX5sqO6a7tyXJeD96lK9Mcu9Uva2jbOVYnlv+BFW1vqo2V9Xm7du3z9hEAAAAAPbFboOiqvr+JA92960z7nO+eYd6F+VPLOy+prvXdPeaFStWzHhYAAAAAPbFwTPUeUGSl1TV9yV5ZpLDqurNSR6oqmO7e9t4rOzBUX9rkuOntl+V5L5RvmqecgAAAAD2A7sdUdTdl3X3qu4+KZNJqv9bd/9okk1JLhrVLkryzrG8Kcm6qjqkqk7OZNLqW8bjaY9U1Vnj184unNoGAAAAgCU2y4iinbkiycaqenmSzyS5IEm6+/aq2pjkjiSPJbmkux8f21yc5Nokhya5YbwAAAAA2A/sUVDU3e9L8r6x/FCSs3dS7/Ikl89TvjnJaXvaSAAAAACefLP+6hkAAAAAy5ygCAAAAIAkgiIAAAAABkERAAAAAEkERQAAAAAMgiIAAAAAkgiKAAAAABgERQAAAAAkERQBAAAAMAiKAAAAAEgiKAIAAABgEBQBAAAAkERQBAAAAMAgKAIAAAAgiaAIAAAAgEFQBAAAAEASQREAAAAAg6AIAAAAgCSCIgAAAAAGQREAAAAASQRFAAAAAAyCIgAAAACSCIoAAAAAGARFAAAAACQRFAEAAAAwCIoAAAAASCIoAgAAAGAQFAEAAACQRFAEAAAAwCAoAgAAACCJoAgAAACAQVAEAAAAQBJBEQAAAACDoAgAAACAJIIiAAAAAAZBEQAAAABJBEUAAAAADIIiAAAAAJIIigAAAAAYBEUAAAAAJBEUAQAAADAIigAAAABIIigCAAAAYBAUAQAAAJBEUAQAAADAICgCAAAAIImgCAAAAIBBUAQAAABAEkERAAAAAIOgCAAAAIAkMwRFVfXMqrqlqj5WVbdX1WtG+ZFVdWNVfWK8HzG1zWVVtaWq7qqqc6bKz6iq28a6K6uqnpzTAgAAAGBPzTKi6NEkL+ru5yY5PcnaqjoryaVJburu1UluGp9TVackWZfk1CRrk1xVVQeNfV2dZH2S1eO1duFOBQAAAIB9sdugqCe+ND4+fbw6yXlJNozyDUnOH8vnJbm+ux/t7ruTbElyZlUdm+Sw7r65uzvJdVPbAAAAALDEZpqjqKoOqqqPJnkwyY3d/aEkx3T3tiQZ70eP6iuT3Du1+dZRtnIszy0HAAAAYD8wU1DU3Y939+lJVmUyOui0XVSfb96h3kX5E3dQtb6qNlfV5u3bt8/SRAAAAAD20R796ll3P5zkfZnMLfTAeJws4/3BUW1rkuOnNluV5L5Rvmqe8vmOc013r+nuNStWrNiTJgIAAACwl2b51bMVVXX4WD40yXcn+XiSTUkuGtUuSvLOsbwpybqqOqSqTs5k0upbxuNpj1TVWePXzi6c2gYAAACAJXbwDHWOTbJh/HLZ05Js7O53VdXNSTZW1cuTfCbJBUnS3bdX1cYkdyR5LMkl3f342NfFSa5NcmiSG8YLAAAAgP3AboOi7v7LJM+bp/yhJGfvZJvLk1w+T/nmJLua3wgAAACAJbJHcxQBAAAAsHwJigAAAABIIigCAAAAYBAUAQAAAJBEUAQAAADAICgCAAAAIImgCAAAAIBBUAQAAABAEkERAAAAAIOgCAAAAIAkgiIAAAAABkERAAAAAEkERQAAAAAMgiIAAAAAkgiKAAAAABgERQAAAAAkERQBAAAAMAiKAAAAAEgiKAIAAABgEBQBAAAAkERQBAAAAMAgKAIAAAAgiaAIAAAAgEFQBAAAAEASQREAAAAAg6AIAAAAgCSCIgAAAAAGQREAAAAASQRFAAAAAAyCIgAAAACSCIoAAAAAGARFAAAAACQRFAEAAAAwCIoAAAAASCIoAgAAAGAQFAEAAACQRFAEAAAAwCAoAgAAACCJoAgAAACAQVAEAAAAQBJBEQAAAACDoAgAAACAJIIiAAAAAAZBEQAAAABJBEUAAAAADIIiAAAAAJIIigAAAAAYBEUAAAAAJBEUAQAAADAIigAAAABIMkNQVFXHV9WfVtWdVXV7Vb1ilB9ZVTdW1SfG+xFT21xWVVuq6q6qOmeq/Iyqum2su7Kq6sk5LQAAAAD21Cwjih5L8nPd/T8nOSvJJVV1SpJLk9zU3auT3DQ+Z6xbl+TUJGuTXFVVB419XZ1kfZLV47V2Ac8FAAAAgH2w26Cou7d190fG8iNJ7kyyMsl5STaMahuSnD+Wz0tyfXc/2t13J9mS5MyqOjbJYd19c3d3kuumtgEAAABgie3RHEVVdVKS5yX5UJJjuntbMgmTkhw9qq1Mcu/UZltH2cqxPLccAAAAgP3AzEFRVT0ryVuTvLK7v7irqvOU9S7K5zvW+qraXFWbt2/fPmsTAQAAANgHMwVFVfX0TEKi3+/ut43iB8bjZBnvD47yrUmOn9p8VZL7RvmqecqfoLuv6e413b1mxYoVs54LAAAAAPtgll89qyRvTHJnd//G1KpNSS4ayxcleedU+bqqOqSqTs5k0upbxuNpj1TVWWOfF05tAwAAAMASO3iGOi9I8tIkt1XVR0fZq5JckWRjVb08yWeSXJAk3X17VW1Mckcmv5h2SXc/Pra7OMm1SQ5NcsN4AQAAALAf2G1Q1N1/lvnnF0qSs3eyzeVJLp+nfHOS0/akgQAAAAAsjj361TMAAAAAli9BEQAAAABJBEUAAAAADIIiAAAAAJIIigAAAAAYBEUAAAAAJBEUAQAAADAIigAAAABIIigCAAAAYBAUAQAAAJBEUAQAAADAICgCAAAAIImgCAAAAIBBUAQAAABAEkERAAAAAIOgCAAAAIAkgiIAAAAABkERAAAAAEkERQAAAAAMgiIAAAAAkgiKAAAAABgERQAAAAAkERQBAAAAMAiKAAAAAEgiKAIAAABgEBQBAAAAkERQBAAAAMAgKAIAAAAgiaAIAAAAgEFQBAAAAEASQREAAAAAg6AIAAAAgCSCIgAAAAAGQREAAAAASQRFAAAAAAyCIgAAAACSCIoAAAAAGARFAAAAACQRFAEAAAAwCIoAAAAASCIoAgAAAGAQFAEAAACQJDl4qRsAsFyddOm7l7oJC+aeK1681E0AAAAWgRFFAAAAACQRFAEAAAAwCIoAAAAASCIoAgAAAGAQFAEAAACQRFAEAAAAwCAoAgAAACDJDEFRVb2pqh6sqr+aKjuyqm6sqk+M9yOm1l1WVVuq6q6qOmeq/Iyqum2su7KqauFPBwAAAIC9NcuIomuTrJ1TdmmSm7p7dZKbxudU1SlJ1iU5dWxzVVUdNLa5Osn6JKvHa+4+AQAAAFhCuw2KuvsDST43p/i8JBvG8oYk50+VX9/dj3b33Um2JDmzqo5Nclh339zdneS6qW0AAAAA2A/s7RxFx3T3tiQZ70eP8pVJ7p2qt3WUrRzLc8sBAAAA2E8s9GTW88071Lson38nVeuranNVbd6+ffuCNQ4AAACAndvboOiB8ThZxvuDo3xrkuOn6q1Kct8oXzVP+by6+5ruXtPda1asWLGXTQQAAABgT+xtULQpyUVj+aIk75wqX1dVh1TVyZlMWn3LeDztkao6a/za2YVT2wAAAACwHzh4dxWq6g+TvDDJUVW1NckvJ7kiycaqenmSzyS5IEm6+/aq2pjkjiSPJbmkux8fu7o4k19QOzTJDeMFAAAAwH5it0FRd//wTladvZP6lye5fJ7yzUlO26PWAQAAALBoFnoyawAAAACeogRFAAAAACQRFAEAAAAwCIoAAAAASCIoAgAAAGAQFAEAAACQRFAEAAAAwCAoAgAAACCJoAgAAACAQVAEAAAAQBJBEQAAAACDoAgAAACAJIIiAAAAAAZBEQAAAABJBEUAAAAADIIiAAAAAJIIigAAAAAYBEUAAAAAJBEUAQAAADAIigAAAABIIigCAAAAYBAUAQAAAJBEUAQAAADAICgCAAAAIImgCAAAAIBBUAQAAABAEkERAAAAAIOgCAAAAIAkgiIAAAAABkERAAAAAEkERQAAAAAMgiIAAAAAkgiKAAAAABgOXuoGAAAsFydd+u6lbsKCuOeKFy91EwCAJWJEEQAAAABJBEUAAAAADIIiAAAAAJIIigAAAAAYBEUAAAAAJBEUAQAAADAIigAAAABIIigCAAAAYDh4qRsAAADwVHTSpe9e6iYsmHuuePFSNwHYTxhRBAAAAEASQREAAAAAg6AIAAAAgCSCIgAAAAAGQREAAAAASQRFAAAAAAyCIgAAAACSJAcv9gGram2S30xyUJI3dPcVi90GAAAAYHGcdOm7l7oJC+aeK1681E140i3qiKKqOijJ65Kcm+SUJD9cVacsZhsAAAAAmN9iP3p2ZpIt3f2p7v5qkuuTnLfIbQAAAABgHosdFK1Mcu/U562jDAAAAIAlVt29eAeruiDJOd394+PzS5Oc2d0/Pafe+iTrx8fnJLlr0Rr55DkqyWeXuhEsCX1/4NL3By59f+DS9wcufX9g0u8HLn1/4FpOfX9id6+YW7jYk1lvTXL81OdVSe6bW6m7r0lyzWI1ajFU1ebuXrPU7WDx6fsDl74/cOn7A5e+P3Dp+wOTfj9w6fsD14HQ94v96NmHk6yuqpOr6hlJ1iXZtMhtAAAAAGAeizqiqLsfq6qfSvLHSQ5K8qbuvn0x2wAAAADA/Bb70bN093uSvGexj7sfWFaP0rFH9P2BS98fuPT9gUvfH7j0/YFJvx+49P2Ba9n3/aJOZg0AAADA/mux5ygCAAAAYD8lKFpgVbW2qu6qqi1Vdek866uqrhzr/7Kqvn0p2snCm6HvX1hVX6iqj47XLy1FO1lYVfWmqnqwqv5qJ+vd88vUDH3vnl+Gqur4qvrTqrqzqm6vqlfMU8d9vwzN2Pfu+2Woqp5ZVbdU1cdG379mnjru+2Voxr533y9TVXVQVf1FVb1rnnXL+p5f9DmKlrOqOijJ65J8T5KtST5cVZu6+46paucmWT1e35nk6vHOU9iMfZ8k/293f/+iN5An07VJfjvJdTtZ755fvq7Nrvs+cc8vR48l+bnu/khVfUOSW6vqRv/WHxBm6fvEfb8cPZrkRd39pap6epI/q6obuvuDU3Xc98vTLH2fuO+Xq1ckuTPJYfOsW9b3vBFFC+vMJFu6+1Pd/dUk1yc5b06d85Jc1xMfTHJ4VR272A1lwc3S9yxD3f2BJJ/bRRX3/DI1Q9+zDHX3tu7+yFh+JJP/QK6cU819vwzN2PcsQ+Ne/tL4+PTxmjvRq/t+GZqx71mGqmpVkhcnecNOqizre15QtLBWJrl36vPWPPE/ELPU4aln1n59/hi6ekNVnbo4TWOJuecPbO75ZayqTkryvCQfmrPKfb/M7aLvE/f9sjQeQflokgeT3Njd7vsDxAx9n7jvl6PXJvnFJF/byfplfc8LihZWzVM2N3GepQ5PPbP060eSnNjdz03yW0ne8WQ3iv2Ce/7A5Z5fxqrqWUnemuSV3f3Fuavn2cR9v0zspu/d98tUdz/e3acnWZXkzKo6bU4V9/0yNUPfu++Xmar6/iQPdvetu6o2T9myuecFRQtra5Ljpz6vSnLfXtThqWe3/drdX9wxdLW735Pk6VV11OI1kSXinj9AueeXrzFPxVuT/H53v22eKu77ZWp3fe++X/66++Ek70uyds4q9/0yt7O+d98vSy9I8pKquieTKUVeVFVvnlNnWd/zgqKF9eEkq6vq5Kp6RpJ1STbNqbMpyYVjlvSzknyhu7ctdkNZcLvt+6r6pqqqsXxmJvffQ4veUhabe/4A5Z5fnkafvjHJnd39Gzup5r5fhmbpe/f98lRVK6rq8LF8aJLvTvLxOdXc98vQLH3vvl9+uvuy7l7V3Sdl8nfdf+vuH51TbVnf8371bAF192NV9VNJ/jjJQUne1N23V9W/HOv/c5L3JPm+JFuSfDnJy5aqvSycGfv+B5NcXFWPJflKknXdvWyGJx6oquoPk7wwyVFVtTXJL2cy0aF7fpmboe/d88vTC5K8NMltY86KJHlVkhMS9/0yN0vfu++Xp2OTbBi/cvu0JBu7+13+j39AmKXv3fcHiAPpni/fYQAAAAASj54BAAAAMAiKAAAAAEgiKAIAAABgEBQBAAAAkERQBAAAAMAgKAIAAAAgiaAIAAAAgEFQBAAAAECS5P8HHTdBHtui5woAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot rank\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.hist(correct_answer_rank, bins=20)\n",
    "plt.title(\"Ordering of Correct Answer Candidate Rank\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7008\n",
       "1    1193\n",
       "2     357\n",
       "3     207\n",
       "4      68\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Provide rank numbers\n",
    "pd.Series(correct_answer_rank).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Candidate Rank: 0 - Average Prior Confidence: 0.88239 - Median Prior Confidence: 0.95273\n",
      "Correct Candidate Rank: 1 - Average Prior Confidence: 0.17663 - Median Prior Confidence: 0.15083\n",
      "Correct Candidate Rank: 2 - Average Prior Confidence: 0.07626 - Median Prior Confidence: 0.07267\n",
      "Correct Candidate Rank: 3 - Average Prior Confidence: 0.04184 - Median Prior Confidence: 0.04132\n",
      "Correct Candidate Rank: 4 - Average Prior Confidence: 0.02394 - Median Prior Confidence: 0.01005\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean prior confidence for each rank\n",
    "for k,v in correct_answer_prior.items():\n",
    "    print(f\"Correct Candidate Rank: {k} - Average Prior Confidence: {round(np.mean(v),5)} - Median Prior Confidence: {round(np.median(v),5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When limited to sentences with 5 congruent mentions or less, 79.3% are in First Place and 13.5% are in Second, totaling 92.8% in our top two positions. Our first place candidates have median prior confidence of 0.95, but mean confidence of 0.99, suggesting that for most mentions we are highly confident in its first option, but a few are much lower and more uncertain, bringing the mean down (like a 50/50 estimate). Conversely, in second place, median is 0.15 but mean is 0.17, further suggesting those 50/50 estimates for some mentions bringing the mean up.\n",
    "\n",
    "Our hope is that congruence will be able to clarify those more evenly split prior values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Congruence Metric between Congruent Entities\n",
    "\n",
    "In order to allow this to become recursive for N many tables, we will need to capture a congruence table for every candidate pool to every other candidate pool in a two-level dictionary so you can retrieve values using `matrix[3][1]`. This would entail duplication except to save that, we sort by value so you always search [small][large]. Saves us computation and storage. To see a sequential example of our logic, scroll to the end of this notebook. Below is our function-based implementation of our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to Create Modular Congruent Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate lists of vectors\n",
    "def get_candidate_pool_vectors(candidate_pool_titles, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return entity vectors from Wikipedia2Vec\n",
    "    Takes as input a list of page titles, representing the candidate pool\n",
    "    Normalizes each page title to match necessary input format\n",
    "    Returns entity vector or empty vector if no match\n",
    "    \"\"\"\n",
    "    # Track failed vector queries\n",
    "    no_vector_count = 0\n",
    "    candidate_pool_vectors = []\n",
    "    for candidate in candidate_pool_titles:\n",
    "        candidate = normalize_text(candidate)\n",
    "        if verbose: print(candidate)\n",
    "        try:\n",
    "            candidate_vectors = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            # Keep empty vector representation to maintain index locations\n",
    "            candidate_vectors = np.zeros(100)\n",
    "            no_vector_count += 1\n",
    "        candidate_pool_vectors.append(candidate_vectors)\n",
    "    \n",
    "    # Handle case where candidate pool is empty from Phase 3\n",
    "    # Add arbitrarily chosen 3 arrays of zeros\n",
    "    if len(candidate_pool_titles) == 0:\n",
    "        candidate_pool_vectors = [np.zeros(100), np.zeros(100), np.zeros(100)]\n",
    "    \n",
    "    if verbose: print(f\"Failed Wikipedia2Vec Entity Vector Queries: {no_vector_count}\")\n",
    "    return candidate_pool_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to retrieve entity vectors\n",
    "def create_entity_vector_dict(sentence_mention_ids, single_sentence_df, verbose=False):\n",
    "    \"\"\"\n",
    "    Function iterates over a provided list of congruent mentions,\n",
    "    finds the associated candidate pool for each mention\n",
    "    and returns the candidate pool vector\n",
    "    \"\"\"\n",
    "    # Save vectors in dictionary\n",
    "    vector_dict = {}\n",
    "    \n",
    "    # For each full mention we are analyzing in the contextual domain\n",
    "    for m in sentence_mention_ids:\n",
    "        \n",
    "        # Retrieve candidate pool titles\n",
    "        candidate_pool_titles = single_sentence_df['candidate_pool_titles'][m]\n",
    "        if verbose: print(candidate_pool_titles)\n",
    "        \n",
    "        # Convert candidate pool titles to candidate pool vectors\n",
    "        candidate_pool_vectors = get_candidate_pool_vectors(candidate_pool_titles, verbose=verbose)\n",
    "        \n",
    "        # Save candidate pool vectors to dictionary\n",
    "        vector_dict[m] = candidate_pool_vectors\n",
    "    \n",
    "    if verbose:\n",
    "        print(vector_dict.keys())\n",
    "        for k in vector_dict.keys():\n",
    "            print(len(vector_dict[k]))\n",
    "    return vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate congruent metric for each candidate in every mention's candidate pool\n",
    "def get_congruence_dict(vector_dict, sentence_mention_ids, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return congruence metric calculations between all candidates for all mentions\n",
    "    Input:\n",
    "    - vector_dict: todo this could be generalized to allow comparison between text/ints/vectors\n",
    "    - Sentence Mentions Numerical Representation: Integers representing congruent mentions in a context domain\n",
    "    Outputs:\n",
    "    - Dictionary with congruence metric calculations for everyone\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Save congruence metrics in a two-level dictionary\n",
    "    # Create first-level dictionary to be returned\n",
    "    congruence_dict = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level congruence metric dictionary\n",
    "        m_dict = {}\n",
    "\n",
    "        # Compare eaech mention against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            if verbose: print(f\"Comparing mentions {m} & {n}\")\n",
    "            # Calculate congruence metric - cosine similarity\n",
    "            congruence_metric = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "            # Save congruence metric to second-level dictionary\n",
    "            m_dict[n] = congruence_metric\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        congruence_dict[m] = m_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "    \n",
    "#     if verbose: print(congruence_dict)\n",
    "    return congruence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to combine mention probabilities\n",
    "def combine_probabilities(mid_1, mid_2, sentence_probabilities):\n",
    "    \"\"\"\n",
    "    Function takes two mentions (numerically represented),\n",
    "    finds their candidate pool probabilities,\n",
    "    and combines them using the chosen logic,\n",
    "    returning a matrix of combined pair-wise probabilities\n",
    "    \"\"\"\n",
    "    # Prepare matrix to return\n",
    "    weights_matrix = []\n",
    "    \n",
    "    # Combine every candidate's probability for one mention with each for the second\n",
    "    for a in sentence_probabilities[mid_1]:\n",
    "        weights_row = []\n",
    "        for b in sentence_probabilities[mid_2]:\n",
    "            \n",
    "            ## Combination logic\n",
    "            weights_row.append(np.mean([a, b])) # Take mean of two prior probabilities\n",
    "        \n",
    "        weights_matrix.append(weights_row)\n",
    "        \n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create a weighted dictionary from priors\n",
    "def create_weighted_dict(sentence_mention_ids, single_sentence_df, verbose=False):\n",
    "    \"\"\"\n",
    "    Function iterates over a provided list of congruent mentions,\n",
    "    finds the associated candidate pool prior probabilities for each mention\n",
    "    and returns a dictionary for the combined probability of every pair of candidates\n",
    "    \"\"\"\n",
    "    # Save weights in dictionary\n",
    "    weights_dict = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level weights dictionary\n",
    "        w_dict = {}\n",
    "        \n",
    "        # Compare each mention probabilities against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            if verbose: print(f\"Comparing mentions {m} & {n}\")\n",
    "            # Return candidate pool probabilities - todo replace likelihood with probabilities\n",
    "            weights_matrix = combine_probabilities(m, n, single_sentence_df['candidate_pool_priors'])\n",
    "            if not weights_matrix: # Handles error where candidate pool is empty\n",
    "                weights_matrix = [0.0]\n",
    "            # Save weights matrix to second-level dictionary\n",
    "            w_dict[n] = weights_matrix\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        weights_dict[m] = w_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "                \n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to combine congruence and prior weights\n",
    "def combine_congruence_priors(sentence_mention_ids, congruence_dict, weights_dict, verbose=False):\n",
    "    \"\"\"\n",
    "    Function takes two dictionaries representing recursive metric calculations,\n",
    "    Multiplies them element-wise together,\n",
    "    Returns the updated table\n",
    "    \"\"\"\n",
    "   # Combine congruence with prior weights in first-level dictionary\n",
    "    weighted_congruence = {}\n",
    "    \n",
    "    # Always work low numbers to high without duplicate comparison\n",
    "    m = 0\n",
    "    while m < len(sentence_mention_ids)-1:\n",
    "        \n",
    "        # Create second-level dictionary\n",
    "        w_c_dict = {}\n",
    "        \n",
    "        # Compare each mention probabilities against mentions after it\n",
    "        for n in sentence_mention_ids[m+1:]:\n",
    "            try:\n",
    "                weighted_table = congruence_dict[m][n] * np.array(weights_dict[m][n])\n",
    "            except ValueError: # Handles error when candidate pool is empty\n",
    "                weighted_table = congruence_dict[m][n]\n",
    "            w_c_dict[n] = weighted_table\n",
    "        \n",
    "        # Save second-level dictionary to first-level\n",
    "        weighted_congruence[m] = w_c_dict\n",
    "        \n",
    "        # Increment mention\n",
    "        m += 1\n",
    "    \n",
    "    return weighted_congruence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standardize on form lookup always row then column\n",
    "def get_most_congruent_pair(congruence_matrix, verbose=False):\n",
    "    \"\"\"\n",
    "    This function takes a congruence matrix and returns the indices\n",
    "    of the two most congruent candidates using your chosen metric.\n",
    "    These indices can be plugged back into the candidate pool lists\n",
    "    to determine which candidates are most similar.\n",
    "    \"\"\"\n",
    "    # Get max values for every row\n",
    "    max_row_values = congruence_matrix.max(axis=1)\n",
    "    max_row_idxs = congruence_matrix.argmax(axis=1)\n",
    "    \n",
    "    # Get overall max value and the row it is in\n",
    "    max_value = max_row_values.max()\n",
    "    max_row_idx = max_row_values.argmax()\n",
    "    \n",
    "    # Get column max value is in\n",
    "    max_column_idx = max_row_idxs[max_row_idx]\n",
    "    \n",
    "    return (max_row_idx, max_column_idx), max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the most congruent pair amongst remaining mentions\n",
    "def find_most_congruent_pair(mention_predictions, mentions_remaining, congruence_dict, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to search the congruence matrices for mentions without predictions\n",
    "    and return the most congruent pair of candidates and associated mentions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with empty most_congruent_pair\n",
    "    # (Candidate Pair, Congruence Metric), (Mention A, Mention B)\n",
    "    most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "    \n",
    "    # Assess whether first pass or recursive pass\n",
    "    if len(mention_predictions) == 0:\n",
    "\n",
    "        # First pass        \n",
    "        for m in mentions_remaining:\n",
    "            for n in mentions_remaining[m+1:]:\n",
    "\n",
    "                # Get most congruent pair in one matrix\n",
    "                congruent_pair = get_most_congruent_pair(congruence_dict[m][n], verbose=verbose)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Comparing {m} & {n}\")\n",
    "                    print(\"Congruent Pair: \", congruent_pair)\n",
    "                    print(\"Current Most Congruent: \", most_congruent_pair)\n",
    "                \n",
    "                if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "                    # Save most congruent candidate pair and mentions\n",
    "                    most_congruent_pair = congruent_pair, (m, n)\n",
    "\n",
    "    elif len(mention_predictions) > 0:\n",
    "        \n",
    "        # Second+, recursive pass\n",
    "        for m in mention_predictions.keys():\n",
    "            for n in mentions_remaining:\n",
    "                \n",
    "                # Becauase we always assume search small mention to large to save computation/storage,\n",
    "                # we must sort incrementing variables to be in increasing order for query\n",
    "                m_tmp, n_tmp = np.sort((m, n))\n",
    "                \n",
    "                # Get most congruent pair in one matrix\n",
    "                congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp], verbose=verbose)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "                    print(\"Congruent Pair: \", congruent_pair)\n",
    "                    print(\"Current Most Congruent: \", most_congruent_pair)\n",
    "                    \n",
    "                if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "                    # Save most congruent candidate pair and mentions\n",
    "                    most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "                \n",
    "    if verbose: print(\"Final Most Congruent Pair: \", most_congruent_pair)\n",
    "    return most_congruent_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congruent Predictions Function\n",
    "\n",
    "This is our main function that takes a sentence ID, calculates congruence for all candidates, updates the prior confidence from Phase 3 with that congruence and selects predictions iteratively based on that final number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate congruent predictions\n",
    "def get_congruent_predictions(sentence_id, dataframe, with_priors=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to calculate congruence metrics over a set of entity full mentions\n",
    "    and return the predicted candidates based on the congruent metric\n",
    "    Input:\n",
    "    - Sentence ID used to filter dataframe\n",
    "    - Dataframe over which to process\n",
    "    Output:\n",
    "    - Prediction for each entity mention\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to dataframe representing single sentence\n",
    "    # Drop duplicates necessary for sentences with the same mention included twice\n",
    "    single_sentence_df = dataframe[dataframe['sentence_id'] == sentence_id]\\\n",
    "                        .drop_duplicates(['full_mention', 'wikipedia_URL', 'wikipedia_page_ID', 'wikipedia_title'])\\\n",
    "                        .reset_index(drop=True)\n",
    "    if verbose: display(single_sentence_df)\n",
    "    \n",
    "    # Define numerical representation of congruent mention list\n",
    "    sentence_congruent_mentions = single_sentence_df['congruent_mentions'][0]\n",
    "    sentence_mention_ids = np.arange(len(sentence_congruent_mentions))\n",
    "    if verbose:\n",
    "        print(\"Congruent Mentions: \", sentence_congruent_mentions)\n",
    "        print(\"Congruent Mentions as numbers: \", sentence_mention_ids)\n",
    "    \n",
    "    # Retrieve dictionary of candidate pool vectors for each mention\n",
    "    vectors_dict = create_entity_vector_dict(sentence_mention_ids, single_sentence_df, verbose=verbose)\n",
    "    if verbose: print(\"Mentions with Vectors: \", vectors_dict.keys())\n",
    "        \n",
    "    # Calculate congruence metric for each candidate vector for each mention's candidate pool\n",
    "    # This notebook uses cosine similarity as the congruence metric\n",
    "    congruence_dict = get_congruence_dict(vectors_dict, sentence_mention_ids, verbose=verbose)\n",
    "    if verbose: print(\"First-Level Congruence Keys: \", congruence_dict.keys())\n",
    "    # This should be one less than congruent mention count, since we are comparing low to high\n",
    "    # and thus don't compare the highest value to anything\n",
    "    \n",
    "    # Add flag for including priors\n",
    "    if with_priors:\n",
    "        \n",
    "        # Calculate weights matric for every pairwise combination of candidates for every mention\n",
    "        weights_dict = create_weighted_dict(sentence_mention_ids, single_sentence_df, verbose=verbose)\n",
    "\n",
    "        # Combine congruence with prior weights\n",
    "        vector_congruence = combine_congruence_priors(sentence_mention_ids, congruence_dict, weights_dict, verbose=verbose)\n",
    "\n",
    "    else:\n",
    "        vector_congruence = congruence_dict\n",
    "\n",
    "    # Create predictions dictionary\n",
    "    mention_predictions = {}\n",
    "    \n",
    "    # Create copy of sentence_mention_ids to iterate through\n",
    "    mentions_remaining = sentence_mention_ids.copy()\n",
    "    \n",
    "    # Iterate through congruent entity mentions to retrieve predictions\n",
    "    # where a prediction is the most congruent candidate between two mentions\n",
    "    while len(mentions_remaining) > 0:\n",
    "        \n",
    "        # Analyze congruence matrices to identify the most congruent pair\n",
    "        most_congruent_pair = find_most_congruent_pair(mention_predictions, mentions_remaining, vector_congruence, verbose=verbose)\n",
    "        \n",
    "        # Save most congrent pair prediction for associated mentions\n",
    "        if len(mention_predictions) == 0:\n",
    "            # First pass\n",
    "            if verbose: print(most_congruent_pair)\n",
    "            # Handles error when nearly all candidate pools are empty\n",
    "            if most_congruent_pair == (((None, None), 0.0), (0, 0)):\n",
    "                mention_predictions[0] = 0\n",
    "            else:\n",
    "                mention_predictions[most_congruent_pair[1][0]] = most_congruent_pair[0][0][0]\n",
    "                mention_predictions[most_congruent_pair[1][1]] = most_congruent_pair[0][0][1]\n",
    "        elif len(mention_predictions) > 0:\n",
    "            # Second_, recursive pass\n",
    "            \n",
    "            # Find new mention you're predicting for\n",
    "            try:\n",
    "                # The number left over in the mention tuple if you remove anything in the prediction dict\n",
    "                new_mention_num = most_congruent_pair[1].index(\\\n",
    "                                                               list(set(most_congruent_pair[1])\\\n",
    "                                                                    - set(mention_predictions.keys())))\n",
    "\n",
    "                # Save new prediction\n",
    "                mention_predictions[most_congruent_pair[1][new_mention_num]] = most_congruent_pair[0][0][new_mention_num]\n",
    "            except ValueError:\n",
    "                for mention in mentions_remaining:\n",
    "                    mention_predictions[mention] = 0 # Zero produces error in final section to produce None prediction\n",
    "            \n",
    "        # Update remaining mentions to mentions without a prediction stored in the dictionary\n",
    "        mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "        if verbose: print(mentions_remaining, mention_predictions.keys())\n",
    "    \n",
    "    if verbose: print(mention_predictions)\n",
    "    \n",
    "    # Use mention predictions to return titles\n",
    "    readable_predictions = {}\n",
    "    for k, v in mention_predictions.items():\n",
    "        if verbose: print(k, v)\n",
    "        readable_key = sentence_congruent_mentions[k]\n",
    "        try:\n",
    "            readable_value = single_sentence_df['candidate_pool_titles'][k][v]\n",
    "            readable_id = single_sentence_df['candidate_pool_page_ids'][k][v]\n",
    "        except IndexError: # Handles case where no candidate pool was provided from Phase 3\n",
    "            readable_value = None \n",
    "            readable_id = None\n",
    "        except TypeError:\n",
    "            # Handles case where no congruence can be calculated\n",
    "            # Either due to one mention in sentence or two mentions but one with no candidate pool\n",
    "            readable_value = single_sentence_df['candidate_pool_titles'][0][0] # Just return top value from Phase 3\n",
    "            readable_id = single_sentence_df['candidate_pool_page_ids'][0][0]\n",
    "        if verbose: print(readable_key, readable_value, readable_id)\n",
    "        readable_predictions[readable_key] = (readable_value, readable_id)\n",
    "    \n",
    "    # Output dictionary with predictions for each entity mention based on congruence\n",
    "    return readable_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>YORK</td>\n",
       "      <td>http://en.wikipedia.org/wiki/York</td>\n",
       "      <td>34361.0</td>\n",
       "      <td>York</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>york</td>\n",
       "      <td>[34361, 134305, 974198, 992970, 2436511]</td>\n",
       "      <td>[42462, 821105, 8055519, 21008612, 994000]</td>\n",
       "      <td>[York, York,_Pennsylvania, York_Racecourse, Yo...</td>\n",
       "      <td>[0.5884024, 0.0486891, 0.0466228, 0.0396487, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>England</td>\n",
       "      <td>http://en.wikipedia.org/wiki/England</td>\n",
       "      <td>9316.0</td>\n",
       "      <td>England</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>england</td>\n",
       "      <td>[9316, 9904, 759125, 691024, 990422]</td>\n",
       "      <td>[21, 47762, 1321565, 378628, 3589698]</td>\n",
       "      <td>[England, England_national_football_team, Engl...</td>\n",
       "      <td>[0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mark_Prescott</td>\n",
       "      <td>25674876.0</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>mark prescott</td>\n",
       "      <td>[25674876]</td>\n",
       "      <td>[6769327]</td>\n",
       "      <td>[Mark_Prescott]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Pivotal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>pivotal</td>\n",
       "      <td>[39421721, 39467663, 34382035, 25015249, 2768719]</td>\n",
       "      <td>[11331695, 16335785, 7180990, 5503397, 15148967]</td>\n",
       "      <td>[Pivotal_(horse), Pivotal_Software, Phases_of_...</td>\n",
       "      <td>[0.6304348, 0.2608696, 0.0217391, 0.0217391, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Nunthorpe_Stakes</td>\n",
       "      <td>727606.0</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>nunthorpe stakes</td>\n",
       "      <td>[727606]</td>\n",
       "      <td>[3346442]</td>\n",
       "      <td>[Nunthorpe_Stakes]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                  wikipedia_URL  \\\n",
       "0       B              YORK              http://en.wikipedia.org/wiki/York   \n",
       "1       B           England           http://en.wikipedia.org/wiki/England   \n",
       "2       B     Mark Prescott     http://en.wikipedia.org/wiki/Mark_Prescott   \n",
       "3       B           Pivotal                                            NaN   \n",
       "4       B  Nunthorpe Stakes  http://en.wikipedia.org/wiki/Nunthorpe_Stakes   \n",
       "\n",
       "   wikipedia_page_ID   wikipedia_title  sentence_id  doc_id  \\\n",
       "0            34361.0              York          210      31   \n",
       "1             9316.0           England          210      31   \n",
       "2         25674876.0     Mark Prescott          210      31   \n",
       "3                NaN               NaN          210      31   \n",
       "4           727606.0  Nunthorpe Stakes          210      31   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [YORK, England, Mark Prescott, Pivotal, Nuntho...              york   \n",
       "1  [YORK, England, Mark Prescott, Pivotal, Nuntho...           england   \n",
       "2  [YORK, England, Mark Prescott, Pivotal, Nuntho...     mark prescott   \n",
       "3  [YORK, England, Mark Prescott, Pivotal, Nuntho...           pivotal   \n",
       "4  [YORK, England, Mark Prescott, Pivotal, Nuntho...  nunthorpe stakes   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0           [34361, 134305, 974198, 992970, 2436511]   \n",
       "1               [9316, 9904, 759125, 691024, 990422]   \n",
       "2                                         [25674876]   \n",
       "3  [39421721, 39467663, 34382035, 25015249, 2768719]   \n",
       "4                                           [727606]   \n",
       "\n",
       "                            candidate_pool_item_ids  \\\n",
       "0        [42462, 821105, 8055519, 21008612, 994000]   \n",
       "1             [21, 47762, 1321565, 378628, 3589698]   \n",
       "2                                         [6769327]   \n",
       "3  [11331695, 16335785, 7180990, 5503397, 15148967]   \n",
       "4                                         [3346442]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [York, York,_Pennsylvania, York_Racecourse, Yo...   \n",
       "1  [England, England_national_football_team, Engl...   \n",
       "2                                    [Mark_Prescott]   \n",
       "3  [Pivotal_(horse), Pivotal_Software, Phases_of_...   \n",
       "4                                 [Nunthorpe_Stakes]   \n",
       "\n",
       "                               candidate_pool_priors  \n",
       "0  [0.5884024, 0.0486891, 0.0466228, 0.0396487, 0...  \n",
       "1  [0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...  \n",
       "2                                              [1.0]  \n",
       "3  [0.6304348, 0.2608696, 0.0217391, 0.0217391, 0...  \n",
       "4                                              [1.0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congruent Mentions:  ['YORK', 'England', 'Mark Prescott', 'Pivotal', 'Nunthorpe Stakes']\n",
      "Congruent Mentions as numbers:  [0 1 2 3 4]\n",
      "['York', 'York,_Pennsylvania', 'York_Racecourse', 'York_Wasps', 'York,_Western_Australia']\n",
      "York\n",
      "York, Pennsylvania\n",
      "York Racecourse\n",
      "York Wasps\n",
      "York, Western Australia\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['England', 'England_national_football_team', 'England_cricket_team', 'England_national_rugby_union_team', 'England_national_rugby_league_team']\n",
      "England\n",
      "England national football team\n",
      "England cricket team\n",
      "England national rugby union team\n",
      "England national rugby league team\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['Mark_Prescott']\n",
      "Mark Prescott\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "['Pivotal_(horse)', 'Pivotal_Software', 'Phases_of_clinical_research', 'Fricke_v._Lynch', 'Pivotal']\n",
      "Pivotal (horse)\n",
      "Pivotal Software\n",
      "Phases of clinical research\n",
      "Fricke v. Lynch\n",
      "Pivotal\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 1\n",
      "['Nunthorpe_Stakes']\n",
      "Nunthorpe Stakes\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "dict_keys([0, 1, 2, 3, 4])\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "Mentions with Vectors:  dict_keys([0, 1, 2, 3, 4])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 0 & 3\n",
      "Comparing mentions 0 & 4\n",
      "Comparing mentions 1 & 2\n",
      "Comparing mentions 1 & 3\n",
      "Comparing mentions 1 & 4\n",
      "Comparing mentions 2 & 3\n",
      "Comparing mentions 2 & 4\n",
      "Comparing mentions 3 & 4\n",
      "First-Level Congruence Keys:  dict_keys([0, 1, 2, 3])\n",
      "Comparing mentions 0 & 1\n",
      "Comparing mentions 0 & 2\n",
      "Comparing mentions 0 & 3\n",
      "Comparing mentions 0 & 4\n",
      "Comparing mentions 1 & 2\n",
      "Comparing mentions 1 & 3\n",
      "Comparing mentions 1 & 4\n",
      "Comparing mentions 2 & 3\n",
      "Comparing mentions 2 & 4\n",
      "Comparing mentions 3 & 4\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((0, 0), 0.25858797358032914)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((0, 0), 0.37014722010928397)\n",
      "Current Most Congruent:  (((0, 0), 0.25858797358032914), (0, 1))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((2, 0), 0.23136763545781952)\n",
      "Current Most Congruent:  (((0, 0), 0.37014722010928397), (0, 2))\n",
      "Comparing 0 & 4\n",
      "Congruent Pair:  ((2, 0), 0.40144850716092584)\n",
      "Current Most Congruent:  (((0, 0), 0.37014722010928397), (0, 2))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.261977061977385)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 0), 0.20199003368640245)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Comparing 1 & 4\n",
      "Congruent Pair:  ((0, 0), 0.24040039331027865)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.5795273173607773)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Comparing 2 & 4\n",
      "Congruent Pair:  ((0, 0), 0.6659345030784607)\n",
      "Current Most Congruent:  (((0, 0), 0.5795273173607773), (2, 3))\n",
      "Comparing 3 & 4\n",
      "Congruent Pair:  ((0, 0), 0.6558879848280214)\n",
      "Current Most Congruent:  (((0, 0), 0.6659345030784607), (2, 4))\n",
      "Final Most Congruent Pair:  (((0, 0), 0.6659345030784607), (2, 4))\n",
      "(((0, 0), 0.6659345030784607), (2, 4))\n",
      "[0, 1, 3] dict_keys([2, 4])\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((0, 0), 0.37014722010928397)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.261977061977385)\n",
      "Current Most Congruent:  (((0, 0), 0.37014722010928397), (0, 2))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 0), 0.5795273173607773)\n",
      "Current Most Congruent:  (((0, 0), 0.37014722010928397), (0, 2))\n",
      "Comparing 0 & 4\n",
      "Congruent Pair:  ((2, 0), 0.40144850716092584)\n",
      "Current Most Congruent:  (((0, 0), 0.5795273173607773), (2, 3))\n",
      "Comparing 1 & 4\n",
      "Congruent Pair:  ((0, 0), 0.24040039331027865)\n",
      "Current Most Congruent:  (((0, 0), 0.5795273173607773), (2, 3))\n",
      "Comparing 3 & 4\n",
      "Congruent Pair:  ((0, 0), 0.6558879848280214)\n",
      "Current Most Congruent:  (((0, 0), 0.5795273173607773), (2, 3))\n",
      "Final Most Congruent Pair:  (((0, 0), 0.6558879848280214), (3, 4))\n",
      "[0, 1] dict_keys([2, 4, 3])\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((0, 0), 0.37014722010928397)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.261977061977385)\n",
      "Current Most Congruent:  (((0, 0), 0.37014722010928397), (0, 2))\n",
      "Comparing 0 & 4\n",
      "Congruent Pair:  ((2, 0), 0.40144850716092584)\n",
      "Current Most Congruent:  (((0, 0), 0.37014722010928397), (0, 2))\n",
      "Comparing 1 & 4\n",
      "Congruent Pair:  ((0, 0), 0.24040039331027865)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((2, 0), 0.23136763545781952)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 0), 0.20199003368640245)\n",
      "Current Most Congruent:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "Final Most Congruent Pair:  (((2, 0), 0.40144850716092584), (0, 4))\n",
      "[1] dict_keys([2, 4, 3, 0])\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((0, 0), 0.261977061977385)\n",
      "Current Most Congruent:  (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 4\n",
      "Congruent Pair:  ((0, 0), 0.24040039331027865)\n",
      "Current Most Congruent:  (((0, 0), 0.261977061977385), (1, 2))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 0), 0.20199003368640245)\n",
      "Current Most Congruent:  (((0, 0), 0.261977061977385), (1, 2))\n",
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((0, 0), 0.25858797358032914)\n",
      "Current Most Congruent:  (((0, 0), 0.261977061977385), (1, 2))\n",
      "Final Most Congruent Pair:  (((0, 0), 0.261977061977385), (1, 2))\n",
      "[] dict_keys([2, 4, 3, 0, 1])\n",
      "{2: 0, 4: 0, 3: 0, 0: 2, 1: 0}\n",
      "2 0\n",
      "Mark Prescott Mark_Prescott 25674876\n",
      "4 0\n",
      "Nunthorpe Stakes Nunthorpe_Stakes 727606\n",
      "3 0\n",
      "Pivotal Pivotal_(horse) 39421721\n",
      "0 2\n",
      "YORK York_Racecourse 974198\n",
      "1 0\n",
      "England England 9316\n",
      "{'Mark Prescott': ('Mark_Prescott', 25674876), 'Nunthorpe Stakes': ('Nunthorpe_Stakes', 727606), 'Pivotal': ('Pivotal_(horse)', 39421721), 'YORK': ('York_Racecourse', 974198), 'England': ('England', 9316)}\n",
      "CPU times: user 29.2 ms, sys: 4.21 ms, total: 33.4 ms\n",
      "Wall time: 30.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test out function\n",
    "sentence_id = 210\n",
    "congruent_predictions = get_congruent_predictions(sentence_id=sentence_id, dataframe=full_mentions, with_priors=True, verbose=True)\n",
    "print(congruent_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>YORK</td>\n",
       "      <td>http://en.wikipedia.org/wiki/York</td>\n",
       "      <td>34361.0</td>\n",
       "      <td>York</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>york</td>\n",
       "      <td>[34361, 134305, 974198, 992970, 2436511]</td>\n",
       "      <td>[42462, 821105, 8055519, 21008612, 994000]</td>\n",
       "      <td>[York, York,_Pennsylvania, York_Racecourse, Yo...</td>\n",
       "      <td>[0.5884024, 0.0486891, 0.0466228, 0.0396487, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>England</td>\n",
       "      <td>http://en.wikipedia.org/wiki/England</td>\n",
       "      <td>9316.0</td>\n",
       "      <td>England</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>england</td>\n",
       "      <td>[9316, 9904, 759125, 691024, 990422]</td>\n",
       "      <td>[21, 47762, 1321565, 378628, 3589698]</td>\n",
       "      <td>[England, England_national_football_team, Engl...</td>\n",
       "      <td>[0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mark_Prescott</td>\n",
       "      <td>25674876.0</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>mark prescott</td>\n",
       "      <td>[25674876]</td>\n",
       "      <td>[6769327]</td>\n",
       "      <td>[Mark_Prescott]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mark_Prescott</td>\n",
       "      <td>25674876.0</td>\n",
       "      <td>Mark Prescott</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>mark prescott</td>\n",
       "      <td>[25674876]</td>\n",
       "      <td>[6769327]</td>\n",
       "      <td>[Mark_Prescott]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>Pivotal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>pivotal</td>\n",
       "      <td>[39421721, 39467663, 34382035, 25015249, 2768719]</td>\n",
       "      <td>[11331695, 16335785, 7180990, 5503397, 15148967]</td>\n",
       "      <td>[Pivotal_(horse), Pivotal_Software, Phases_of_...</td>\n",
       "      <td>[0.6304348, 0.2608696, 0.0217391, 0.0217391, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Nunthorpe_Stakes</td>\n",
       "      <td>727606.0</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>nunthorpe stakes</td>\n",
       "      <td>[727606]</td>\n",
       "      <td>[3346442]</td>\n",
       "      <td>[Nunthorpe_Stakes]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Nunthorpe_Stakes</td>\n",
       "      <td>727606.0</td>\n",
       "      <td>Nunthorpe Stakes</td>\n",
       "      <td>210</td>\n",
       "      <td>31</td>\n",
       "      <td>[YORK, England, Mark Prescott, Pivotal, Nuntho...</td>\n",
       "      <td>nunthorpe stakes</td>\n",
       "      <td>[727606]</td>\n",
       "      <td>[3346442]</td>\n",
       "      <td>[Nunthorpe_Stakes]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                  wikipedia_URL  \\\n",
       "0       B              YORK              http://en.wikipedia.org/wiki/York   \n",
       "1       B           England           http://en.wikipedia.org/wiki/England   \n",
       "2       B     Mark Prescott     http://en.wikipedia.org/wiki/Mark_Prescott   \n",
       "3       I     Mark Prescott     http://en.wikipedia.org/wiki/Mark_Prescott   \n",
       "4       B           Pivotal                                            NaN   \n",
       "5       B  Nunthorpe Stakes  http://en.wikipedia.org/wiki/Nunthorpe_Stakes   \n",
       "6       I  Nunthorpe Stakes  http://en.wikipedia.org/wiki/Nunthorpe_Stakes   \n",
       "\n",
       "   wikipedia_page_ID   wikipedia_title  sentence_id  doc_id  \\\n",
       "0            34361.0              York          210      31   \n",
       "1             9316.0           England          210      31   \n",
       "2         25674876.0     Mark Prescott          210      31   \n",
       "3         25674876.0     Mark Prescott          210      31   \n",
       "4                NaN               NaN          210      31   \n",
       "5           727606.0  Nunthorpe Stakes          210      31   \n",
       "6           727606.0  Nunthorpe Stakes          210      31   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [YORK, England, Mark Prescott, Pivotal, Nuntho...              york   \n",
       "1  [YORK, England, Mark Prescott, Pivotal, Nuntho...           england   \n",
       "2  [YORK, England, Mark Prescott, Pivotal, Nuntho...     mark prescott   \n",
       "3  [YORK, England, Mark Prescott, Pivotal, Nuntho...     mark prescott   \n",
       "4  [YORK, England, Mark Prescott, Pivotal, Nuntho...           pivotal   \n",
       "5  [YORK, England, Mark Prescott, Pivotal, Nuntho...  nunthorpe stakes   \n",
       "6  [YORK, England, Mark Prescott, Pivotal, Nuntho...  nunthorpe stakes   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0           [34361, 134305, 974198, 992970, 2436511]   \n",
       "1               [9316, 9904, 759125, 691024, 990422]   \n",
       "2                                         [25674876]   \n",
       "3                                         [25674876]   \n",
       "4  [39421721, 39467663, 34382035, 25015249, 2768719]   \n",
       "5                                           [727606]   \n",
       "6                                           [727606]   \n",
       "\n",
       "                            candidate_pool_item_ids  \\\n",
       "0        [42462, 821105, 8055519, 21008612, 994000]   \n",
       "1             [21, 47762, 1321565, 378628, 3589698]   \n",
       "2                                         [6769327]   \n",
       "3                                         [6769327]   \n",
       "4  [11331695, 16335785, 7180990, 5503397, 15148967]   \n",
       "5                                         [3346442]   \n",
       "6                                         [3346442]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [York, York,_Pennsylvania, York_Racecourse, Yo...   \n",
       "1  [England, England_national_football_team, Engl...   \n",
       "2                                    [Mark_Prescott]   \n",
       "3                                    [Mark_Prescott]   \n",
       "4  [Pivotal_(horse), Pivotal_Software, Phases_of_...   \n",
       "5                                 [Nunthorpe_Stakes]   \n",
       "6                                 [Nunthorpe_Stakes]   \n",
       "\n",
       "                               candidate_pool_priors  \n",
       "0  [0.5884024, 0.0486891, 0.0466228, 0.0396487, 0...  \n",
       "1  [0.7461579, 0.0736803, 0.0415712, 0.0328608, 0...  \n",
       "2                                              [1.0]  \n",
       "3                                              [1.0]  \n",
       "4  [0.6304348, 0.2608696, 0.0217391, 0.0217391, 0...  \n",
       "5                                              [1.0]  \n",
       "6                                              [1.0]  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define testing sentence_id\n",
    "single_sentence_df = full_mentions[full_mentions['sentence_id'] == sentence_id].reset_index(drop=True)\n",
    "single_sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YORK', 'England', 'Mark Prescott', 'Pivotal', 'Nunthorpe Stakes']\n"
     ]
    }
   ],
   "source": [
    "print(single_sentence_df['congruent_mentions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Prescott -> Mark Prescott =? Mark Prescott\n",
      "Nunthorpe Stakes -> Nunthorpe Stakes =? Nunthorpe Stakes\n",
      "Pivotal -> nan =? Pivotal (horse)\n",
      "YORK -> York =? York Racecourse\n",
      "England -> England =? England\n",
      "*************************************************\n",
      "This congruent experiment is 60.0% accurate comparing page titles.\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for mention, pred in congruent_predictions.items():\n",
    "    pred_title = normalize_text(pred[0])\n",
    "    true_title = single_sentence_df[single_sentence_df['full_mention'] == mention]['wikipedia_title'].values[0]\n",
    "    print(mention, \"->\", true_title, \"=?\", pred_title)\n",
    "    if single_sentence_df[single_sentence_df['full_mention'] == mention]['wikipedia_title'].values[0] == pred_title:\n",
    "        accuracy += 1\n",
    "print(\"*************************************************\")\n",
    "print(f\"This congruent experiment is {round(accuracy/len(congruent_predictions)*100,3)}% accurate comparing page titles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Prescott -> 25674876 =? 25674876\n",
      "Nunthorpe Stakes -> 727606 =? 727606\n",
      "Pivotal -> None =? 39421721\n",
      "YORK -> 34361 =? 974198\n",
      "England -> 9316 =? 9316\n",
      "*************************************************\n",
      "This congruent experiment is 60.0% accurate comparing page IDs.\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for mention, pred in congruent_predictions.items():\n",
    "    pred_page_id = pred[1]\n",
    "    try:\n",
    "        true_page_id = int(single_sentence_df[single_sentence_df['full_mention'] == mention]['wikipedia_page_ID'].values[0])\n",
    "    except ValueError:\n",
    "        true_page_id = None\n",
    "    print(mention, \"->\", true_page_id, \"=?\", pred_page_id)\n",
    "    if single_sentence_df[single_sentence_df['full_mention'] == mention]['wikipedia_page_ID'].values[0] == pred_page_id:\n",
    "        accuracy += 1\n",
    "print(\"*************************************************\")\n",
    "print(f\"This congruent experiment is {round(accuracy/len(congruent_predictions)*100,3)}% accurate comparing page IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Congruence Predictions and Assess Accuracy over Entire Dataframe\n",
    "\n",
    "We now apply the per-sentence structure over the whole ACY dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3,935 sentences to predict.\n"
     ]
    }
   ],
   "source": [
    "# Max sentence_id in dataframe\n",
    "max_sentence_id = len(full_mentions['sentence_id'].unique())\n",
    "print(\"We have {:,} sentences to predict.\".format(max_sentence_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate accuracy\n",
    "def calculate_accuracy(predictions, dataframe=full_mentions, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to calculate accuracy over generated predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize tracker metrics\n",
    "    accurate_predictions = 0\n",
    "    accurate_present = 0\n",
    "    \n",
    "    # Iterate through each full mention\n",
    "    for row in tqdm(range(len(dataframe))):\n",
    "        mention_df = dataframe.iloc[row]\n",
    "        \n",
    "        # Save key values\n",
    "        sid = mention_df['sentence_id']\n",
    "        fm = mention_df['full_mention']\n",
    "        title = mention_df['wikipedia_title']\n",
    "        page_id = mention_df['wikipedia_page_ID']\n",
    "        candidate_pool_page_ids = mention_df['candidate_pool_page_ids']\n",
    "        \n",
    "        # Retrieve prediction\n",
    "        pred = predictions[sid][fm]\n",
    "        norm_pred_title = normalize_text(pred[0])\n",
    "        pred_page_id = pred[1]\n",
    "        \n",
    "        # Print comparison (useful for subset review)\n",
    "        if verbose:\n",
    "            print(fm, sid, \"||| True:\", title, page_id, \"==? Pred:\", norm_pred_title, pred_page_id, \"|||\",\\\n",
    "            norm_pred_title==title, pred_page_id==page_id, \" ||| Present? \", (page_id in candidate_pool_page_ids))\n",
    "        \n",
    "        # Compare true vs prediction\n",
    "        if page_id == pred_page_id:\n",
    "            accurate_predictions += 1\n",
    "        if page_id in candidate_pool_page_ids:\n",
    "            accurate_present += 1\n",
    "        \n",
    "    # Print results\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"Predictive Accuracy: {}%\".format(round(accurate_predictions/len(dataframe)*100, 3)))\n",
    "    print(\"Answer Present: {}%\".format(round(accurate_present/len(dataframe)*100, 3)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congruence Accuracy without considering Prior Confidence\n",
    "\n",
    "We first experiment with calculating congruence accuracy without directly incorporating prior confidence. We still indirectly incorporate it since we took the Top N most confidence (highest ranked) values in Phase 3, but after creating the Top N list, we don't explicitly incorporate it in our calculation for final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3935/3935 [00:11<00:00, 338.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "congruent_predictions_freq_nopr = {}\n",
    "for sid in tqdm(full_mentions['sentence_id'].unique()):\n",
    "    congruent_predictions_freq_nopr[sid] = get_congruent_predictions(sid, dataframe=full_mentions,\n",
    "                                                                     with_priors=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13781/13781 [00:01<00:00, 6978.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predictive Accuracy: 40.737%\n",
      "Answer Present: 64.095%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_freq_nopr, dataframe=full_mentions, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy only with Full Mentions with Known True\n",
    "\n",
    "This is a better reflection of our success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mentions remaining:  9785\n"
     ]
    }
   ],
   "source": [
    "# Filter dataframe to only full mentions with known true values\n",
    "known_true_mentions = full_mentions[full_mentions['wikipedia_page_ID'].notnull()].reset_index()\n",
    "print(\"Full Mentions remaining: \", len(known_true_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9785/9785 [00:01<00:00, 6938.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predictive Accuracy: 57.374%\n",
      "Answer Present: 90.271%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_freq_nopr, dataframe=known_true_mentions, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 1620.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 4607, Number of Mentions: 7\n",
      "------------------------------------------\n",
      "Nelson Mandela 4607 ||| True: Nelson Mandela 21492751.0 ==? Pred: Nelson Mandela 21492751 ||| True True  ||| Present?  True\n",
      "Nelson Mandela 4607 ||| True: Nelson Mandela 21492751.0 ==? Pred: Nelson Mandela 21492751 ||| True True  ||| Present?  True\n",
      "African National Congress 4607 ||| True: African National Congress 2503.0 ==? Pred: African National Congress 2503 ||| True True  ||| Present?  True\n",
      "African National Congress 4607 ||| True: African National Congress 2503.0 ==? Pred: African National Congress 2503 ||| True True  ||| Present?  True\n",
      "African National Congress 4607 ||| True: African National Congress 2503.0 ==? Pred: African National Congress 2503 ||| True True  ||| Present?  True\n",
      "ANC 4607 ||| True: African National Congress 2503.0 ==? Pred: African National Congress 2503 ||| True True  ||| Present?  True\n",
      "KwaZulu-Natal 4607 ||| True: KwaZulu-Natal 193596.0 ==? Pred: University of KwaZulu-Natal 670734 ||| False False  ||| Present?  True\n",
      "------------------------------------------\n",
      "Predictive Accuracy: 85.714%\n",
      "Answer Present: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to random sentence for manual review\n",
    "rand_sid = np.random.choice(known_true_mentions['sentence_id'])\n",
    "rand_sentence_df = known_true_mentions[known_true_mentions['sentence_id'] == rand_sid]\n",
    "print(f\"Sentence ID: {rand_sid}, Number of Mentions: {len(rand_sentence_df)}\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_freq_nopr, dataframe=rand_sentence_df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congruence Accuracy considering Prior Confidence\n",
    "\n",
    "Now, we directly incorporate prior confidence by combining it during the prediction process with our calculated congruent metric. This involves \"discounting\" congruence by the prior confidence we had in each combination's component candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3935/3935 [00:13<00:00, 302.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "congruent_predictions_freq_pr = {}\n",
    "for sid in tqdm(full_mentions['sentence_id'].unique()):\n",
    "    congruent_predictions_freq_pr[sid] = get_congruent_predictions(sid, dataframe=full_mentions,\n",
    "                                                                   with_priors=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13781/13781 [00:01<00:00, 7165.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predictive Accuracy: 53.044%\n",
      "Answer Present: 64.095%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_freq_pr, dataframe=full_mentions, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy only with Full Mentions with Known True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9785/9785 [00:01<00:00, 6951.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predictive Accuracy: 74.706%\n",
      "Answer Present: 90.271%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_freq_pr, dataframe=known_true_mentions, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 1976.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 4279, Number of Mentions: 5\n",
      "------------------------------------------\n",
      "Gail Devers 4279 ||| True: Gail Devers 72654.0 ==? Pred: Gail Devers 72654 ||| True True  ||| Present?  True\n",
      "Gail Devers 4279 ||| True: Gail Devers 72654.0 ==? Pred: Gail Devers 72654 ||| True True  ||| Present?  True\n",
      "Jamaican 4279 ||| True: Jamaica 15660.0 ==? Pred: Jamaicans 28972206 ||| False False  ||| Present?  True\n",
      "Merlene Ottey 4279 ||| True: Merlene Ottey 558060.0 ==? Pred: Merlene Ottey 558060 ||| True True  ||| Present?  True\n",
      "Merlene Ottey 4279 ||| True: Merlene Ottey 558060.0 ==? Pred: Merlene Ottey 558060 ||| True True  ||| Present?  True\n",
      "------------------------------------------\n",
      "Predictive Accuracy: 80.0%\n",
      "Answer Present: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to random sentence for manual review\n",
    "rand_sid = np.random.choice(known_true_mentions['sentence_id'])\n",
    "rand_sentence_df = known_true_mentions[known_true_mentions['sentence_id'] == rand_sid]\n",
    "print(f\"Sentence ID: {rand_sid}, Number of Mentions: {len(rand_sentence_df)}\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_freq_pr, dataframe=rand_sentence_df, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11046</th>\n",
       "      <td>B</td>\n",
       "      <td>American Olympic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4279</td>\n",
       "      <td>755</td>\n",
       "      <td>[American Olympic, Gail Devers, Jamaican, Merl...</td>\n",
       "      <td>american olympic</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11047</th>\n",
       "      <td>I</td>\n",
       "      <td>American Olympic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4279</td>\n",
       "      <td>755</td>\n",
       "      <td>[American Olympic, Gail Devers, Jamaican, Merl...</td>\n",
       "      <td>american olympic</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11048</th>\n",
       "      <td>B</td>\n",
       "      <td>Gail Devers</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Gail_Devers</td>\n",
       "      <td>72654.0</td>\n",
       "      <td>Gail Devers</td>\n",
       "      <td>4279</td>\n",
       "      <td>755</td>\n",
       "      <td>[American Olympic, Gail Devers, Jamaican, Merl...</td>\n",
       "      <td>gail devers</td>\n",
       "      <td>[72654]</td>\n",
       "      <td>[217354]</td>\n",
       "      <td>[Gail_Devers]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11049</th>\n",
       "      <td>I</td>\n",
       "      <td>Gail Devers</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Gail_Devers</td>\n",
       "      <td>72654.0</td>\n",
       "      <td>Gail Devers</td>\n",
       "      <td>4279</td>\n",
       "      <td>755</td>\n",
       "      <td>[American Olympic, Gail Devers, Jamaican, Merl...</td>\n",
       "      <td>gail devers</td>\n",
       "      <td>[72654]</td>\n",
       "      <td>[217354]</td>\n",
       "      <td>[Gail_Devers]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11050</th>\n",
       "      <td>B</td>\n",
       "      <td>Jamaican</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jamaica</td>\n",
       "      <td>15660.0</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>4279</td>\n",
       "      <td>755</td>\n",
       "      <td>[American Olympic, Gail Devers, Jamaican, Merl...</td>\n",
       "      <td>jamaican</td>\n",
       "      <td>[28972206, 15660, 172379, 11214212, 9297589]</td>\n",
       "      <td>[6127469, 766, 572717, 4970177, 6127409]</td>\n",
       "      <td>[Jamaicans, Jamaica, Music_of_Jamaica, British...</td>\n",
       "      <td>[0.6266094, 0.0944206, 0.037196, 0.0343348, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11051</th>\n",
       "      <td>B</td>\n",
       "      <td>Merlene Ottey</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Merlene_Ottey</td>\n",
       "      <td>558060.0</td>\n",
       "      <td>Merlene Ottey</td>\n",
       "      <td>4279</td>\n",
       "      <td>755</td>\n",
       "      <td>[American Olympic, Gail Devers, Jamaican, Merl...</td>\n",
       "      <td>merlene ottey</td>\n",
       "      <td>[558060]</td>\n",
       "      <td>[217358]</td>\n",
       "      <td>[Merlene_Ottey]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11052</th>\n",
       "      <td>I</td>\n",
       "      <td>Merlene Ottey</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Merlene_Ottey</td>\n",
       "      <td>558060.0</td>\n",
       "      <td>Merlene Ottey</td>\n",
       "      <td>4279</td>\n",
       "      <td>755</td>\n",
       "      <td>[American Olympic, Gail Devers, Jamaican, Merl...</td>\n",
       "      <td>merlene ottey</td>\n",
       "      <td>[558060]</td>\n",
       "      <td>[217358]</td>\n",
       "      <td>[Merlene_Ottey]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mention      full_mention                               wikipedia_URL  \\\n",
       "11046       B  American Olympic                                         NaN   \n",
       "11047       I  American Olympic                                         NaN   \n",
       "11048       B       Gail Devers    http://en.wikipedia.org/wiki/Gail_Devers   \n",
       "11049       I       Gail Devers    http://en.wikipedia.org/wiki/Gail_Devers   \n",
       "11050       B          Jamaican        http://en.wikipedia.org/wiki/Jamaica   \n",
       "11051       B     Merlene Ottey  http://en.wikipedia.org/wiki/Merlene_Ottey   \n",
       "11052       I     Merlene Ottey  http://en.wikipedia.org/wiki/Merlene_Ottey   \n",
       "\n",
       "       wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "11046                NaN             NaN         4279     755   \n",
       "11047                NaN             NaN         4279     755   \n",
       "11048            72654.0     Gail Devers         4279     755   \n",
       "11049            72654.0     Gail Devers         4279     755   \n",
       "11050            15660.0         Jamaica         4279     755   \n",
       "11051           558060.0   Merlene Ottey         4279     755   \n",
       "11052           558060.0   Merlene Ottey         4279     755   \n",
       "\n",
       "                                      congruent_mentions norm_full_mention  \\\n",
       "11046  [American Olympic, Gail Devers, Jamaican, Merl...  american olympic   \n",
       "11047  [American Olympic, Gail Devers, Jamaican, Merl...  american olympic   \n",
       "11048  [American Olympic, Gail Devers, Jamaican, Merl...       gail devers   \n",
       "11049  [American Olympic, Gail Devers, Jamaican, Merl...       gail devers   \n",
       "11050  [American Olympic, Gail Devers, Jamaican, Merl...          jamaican   \n",
       "11051  [American Olympic, Gail Devers, Jamaican, Merl...     merlene ottey   \n",
       "11052  [American Olympic, Gail Devers, Jamaican, Merl...     merlene ottey   \n",
       "\n",
       "                            candidate_pool_page_ids  \\\n",
       "11046                                            []   \n",
       "11047                                            []   \n",
       "11048                                       [72654]   \n",
       "11049                                       [72654]   \n",
       "11050  [28972206, 15660, 172379, 11214212, 9297589]   \n",
       "11051                                      [558060]   \n",
       "11052                                      [558060]   \n",
       "\n",
       "                        candidate_pool_item_ids  \\\n",
       "11046                                        []   \n",
       "11047                                        []   \n",
       "11048                                  [217354]   \n",
       "11049                                  [217354]   \n",
       "11050  [6127469, 766, 572717, 4970177, 6127409]   \n",
       "11051                                  [217358]   \n",
       "11052                                  [217358]   \n",
       "\n",
       "                                   candidate_pool_titles  \\\n",
       "11046                                                 []   \n",
       "11047                                                 []   \n",
       "11048                                      [Gail_Devers]   \n",
       "11049                                      [Gail_Devers]   \n",
       "11050  [Jamaicans, Jamaica, Music_of_Jamaica, British...   \n",
       "11051                                    [Merlene_Ottey]   \n",
       "11052                                    [Merlene_Ottey]   \n",
       "\n",
       "                                   candidate_pool_priors  \n",
       "11046                                                 []  \n",
       "11047                                                 []  \n",
       "11048                                              [1.0]  \n",
       "11049                                              [1.0]  \n",
       "11050  [0.6266094, 0.0944206, 0.037196, 0.0343348, 0....  \n",
       "11051                                              [1.0]  \n",
       "11052                                              [1.0]  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See original dataframe\n",
    "full_mentions[full_mentions['sentence_id'] == rand_sid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zoom in on a full candidate pool\n",
    "full_mentions[full_mentions['sentence_id'] == rand_sid]['candidate_pool_titles'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate with *Popularity* Predictions - Known True Only\n",
    "\n",
    "We also want to see how much of an impact congruence can have on candidate pools produced via page popularity anchor link statistics. These are considered less effective because of large skews that can happen if a word is linked to a very popular page just a single time. Our hypothesis is that congruence may have a larger impact on popularity mentions and to assess this, we compare accuracy with and without prior incorporation for known trues only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>eu</td>\n",
       "      <td>[9317, 9239, 9891, 9472, 10890716]</td>\n",
       "      <td>[458, 46, 45003, 4916, 185441]</td>\n",
       "      <td>['European_Union', 'Europe', 'Entropy', 'Euro'...</td>\n",
       "      <td>[0.2245141, 0.2015168, 0.0965482, 0.0768826, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>german</td>\n",
       "      <td>[11867, 27318, 21148, 21212, 26964606]</td>\n",
       "      <td>[183, 334, 55, 7318, 40]</td>\n",
       "      <td>['Germany', 'Singapore', 'Netherlands', 'Nazi_...</td>\n",
       "      <td>[0.0558084, 0.0548498, 0.044104, 0.0268639, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>British</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['EU', 'German', 'British']</td>\n",
       "      <td>british</td>\n",
       "      <td>[3434750, 31717, 19344654, 26061, 8569916]</td>\n",
       "      <td>[30, 145, 9531, 172771, 1860]</td>\n",
       "      <td>['United_States', 'United_Kingdom', 'BBC', 'Ro...</td>\n",
       "      <td>[0.1243038, 0.0660245, 0.0317426, 0.0291264, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 56873217, 9643132]</td>\n",
       "      <td>[2073954, 26634508, 7172840]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.6296296, 0.3703704, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['Peter Blackburn', 'BRUSSELS', 'European Comm...</td>\n",
       "      <td>peter blackburn</td>\n",
       "      <td>[56783206, 56873217, 9643132]</td>\n",
       "      <td>[2073954, 26634508, 7172840]</td>\n",
       "      <td>['Peter_Blackburn_(badminton)', 'Peter_Blackbu...</td>\n",
       "      <td>[0.6296296, 0.3703704, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention     full_mention                                wikipedia_URL  \\\n",
       "0       B               EU                                          NaN   \n",
       "1       B           German         http://en.wikipedia.org/wiki/Germany   \n",
       "2       B          British  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "3       B  Peter Blackburn                                          NaN   \n",
       "4       I  Peter Blackburn                                          NaN   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0                NaN             NaN            0       0   \n",
       "1            11867.0         Germany            0       0   \n",
       "2            31717.0  United Kingdom            0       0   \n",
       "3                NaN             NaN            1       0   \n",
       "4                NaN             NaN            1       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0                        ['EU', 'German', 'British']                eu   \n",
       "1                        ['EU', 'German', 'British']            german   \n",
       "2                        ['EU', 'German', 'British']           british   \n",
       "3  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "4  ['Peter Blackburn', 'BRUSSELS', 'European Comm...   peter blackburn   \n",
       "\n",
       "                      candidate_pool_page_ids         candidate_pool_item_ids  \\\n",
       "0          [9317, 9239, 9891, 9472, 10890716]  [458, 46, 45003, 4916, 185441]   \n",
       "1      [11867, 27318, 21148, 21212, 26964606]        [183, 334, 55, 7318, 40]   \n",
       "2  [3434750, 31717, 19344654, 26061, 8569916]   [30, 145, 9531, 172771, 1860]   \n",
       "3               [56783206, 56873217, 9643132]    [2073954, 26634508, 7172840]   \n",
       "4               [56783206, 56873217, 9643132]    [2073954, 26634508, 7172840]   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  ['European_Union', 'Europe', 'Entropy', 'Euro'...   \n",
       "1  ['Germany', 'Singapore', 'Netherlands', 'Nazi_...   \n",
       "2  ['United_States', 'United_Kingdom', 'BBC', 'Ro...   \n",
       "3  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "4  ['Peter_Blackburn_(badminton)', 'Peter_Blackbu...   \n",
       "\n",
       "                               candidate_pool_priors  \n",
       "0  [0.2245141, 0.2015168, 0.0965482, 0.0768826, 0...  \n",
       "1  [0.0558084, 0.0548498, 0.044104, 0.0268639, 0....  \n",
       "2  [0.1243038, 0.0660245, 0.0317426, 0.0291264, 0...  \n",
       "3                        [0.6296296, 0.3703704, 0.0]  \n",
       "4                        [0.6296296, 0.3703704, 0.0]  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "full_mentions_pop = pd.read_csv(os.path.join(preds_path, \"anchortext_popularity_5x5.csv\"), delimiter=\",\")\n",
    "full_mentions_pop.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "After ['Peter Blackburn', 'BRUSSELS', 'European Commission', 'German', 'British']\n",
      "Before [56783206, 56873217, 9643132]\n",
      "After [56783206, 56873217, 9643132]\n",
      "Before [2073954, 26634508, 7172840]\n",
      "After [2073954, 26634508, 7172840]\n",
      "Before ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(MP)', 'Peter_Blackburn_(bishop)']\n",
      "After ['Peter_Blackburn_(badminton)', 'Peter_Blackburn_(MP)', 'Peter_Blackburn_(bishop)']\n",
      "Before [0.6296296, 0.3703704, 0.0]\n",
      "After [0.6296296, 0.3703704, 0.0]\n",
      "CPU times: user 208 ms, sys: 10.7 ms, total: 219 ms\n",
      "Wall time: 218 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Apply defined function to entire dataframe for all candidate pool columns\n",
    "\n",
    "column = 'congruent_mentions'\n",
    "print(\"Before\", full_mentions_pop[column][3])\n",
    "parsed_candidate_pool = full_mentions_pop[column].apply(parse_list_string, value_type=str)\n",
    "full_mentions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions_pop[column][3])\n",
    "\n",
    "column = 'candidate_pool_page_ids'\n",
    "print(\"Before\", full_mentions_pop[column][3])\n",
    "parsed_candidate_pool = full_mentions_pop[column].apply(parse_list_string, value_type=int)\n",
    "full_mentions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions_pop[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_item_ids'\n",
    "print(\"Before\", full_mentions_pop[column][3])\n",
    "parsed_candidate_pool = full_mentions_pop[column].apply(parse_list_string, value_type=int)\n",
    "full_mentions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions_pop[column][3])\n",
    "\n",
    "\n",
    "column = 'candidate_pool_titles'\n",
    "print(\"Before\", full_mentions_pop[column][3])\n",
    "parsed_candidate_pool = full_mentions_pop[column].apply(parse_list_string, value_type=str)\n",
    "full_mentions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions_pop[column][3])\n",
    "\n",
    "column = 'candidate_pool_priors'\n",
    "print(\"Before\", full_mentions_pop[column][3])\n",
    "parsed_candidate_pool = full_mentions_pop[column].apply(parse_list_string, value_type=float)\n",
    "full_mentions_pop[column] = parsed_candidate_pool\n",
    "print(\"After\", full_mentions_pop[column][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congruence Accuracy without considering Prior Confidence\n",
    "\n",
    "We first experiment with calculating congruence accuracy without directly incorporating prior confidence. We still indirectly incorporate it since we took the Top N most confidence (highest ranked) values in Phase 3, but after creating the Top N list, we don't explicitly incorporate it in our calculation for final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3935/3935 [00:11<00:00, 348.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "congruent_predictions_pop_nopr = {}\n",
    "for sid in tqdm(full_mentions['sentence_id'].unique()):\n",
    "    congruent_predictions_pop_nopr[sid] = get_congruent_predictions(sid, dataframe=full_mentions,\n",
    "                                                                     with_priors=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9785/9785 [00:01<00:00, 6962.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predictive Accuracy: 57.374%\n",
      "Answer Present: 90.271%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_pop_nopr, dataframe=known_true_mentions, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1374.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 1491, Number of Mentions: 2\n",
      "------------------------------------------\n",
      "Simon Culhane 1491 ||| True: Simon Culhane 5210833.0 ==? Pred: Simon Culhane 5210833 ||| True True  ||| Present?  True\n",
      "Simon Culhane 1491 ||| True: Simon Culhane 5210833.0 ==? Pred: Simon Culhane 5210833 ||| True True  ||| Present?  True\n",
      "------------------------------------------\n",
      "Predictive Accuracy: 100.0%\n",
      "Answer Present: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to random sentence for manual review\n",
    "rand_sid = np.random.choice(known_true_mentions['sentence_id'])\n",
    "rand_sentence_df = known_true_mentions[known_true_mentions['sentence_id'] == rand_sid]\n",
    "print(f\"Sentence ID: {rand_sid}, Number of Mentions: {len(rand_sentence_df)}\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_pop_nopr, dataframe=rand_sentence_df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congruence Accuracy considering Prior Confidence\n",
    "\n",
    "Now, we directly incorporate prior confidence by combining it during the prediction process with our calculated congruent metric. This involves \"discounting\" congruence by the prior confidence we had in each combination's component candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3935/3935 [00:12<00:00, 306.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over whole dataframe to produce mention predictions for each sentence\n",
    "congruent_predictions_pop_pr = {}\n",
    "for sid in tqdm(full_mentions['sentence_id'].unique()):\n",
    "    congruent_predictions_pop_pr[sid] = get_congruent_predictions(sid, dataframe=full_mentions,\n",
    "                                                                   with_priors=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9785/9785 [00:01<00:00, 6922.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Predictive Accuracy: 74.706%\n",
      "Answer Present: 90.271%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_pop_pr, dataframe=known_true_mentions, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 1431.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 333, Number of Mentions: 7\n",
      "------------------------------------------\n",
      "Detroit 333 ||| True: Detroit 8687.0 ==? Pred: Detroit Pistons 72871 ||| False False  ||| Present?  True\n",
      "Brad Ausmus 333 ||| True: Brad Ausmus 1838414.0 ==? Pred: Brad Ausmus 1838414 ||| True True  ||| Present?  True\n",
      "Brad Ausmus 333 ||| True: Brad Ausmus 1838414.0 ==? Pred: Brad Ausmus 1838414 ||| True True  ||| Present?  True\n",
      "Tigers 333 ||| True: Detroit Tigers 8579.0 ==? Pred: Detroit Tigers 8579 ||| True True  ||| Present?  True\n",
      "Chicago White Sox 333 ||| True: Chicago White Sox 5945.0 ==? Pred: Chicago White Sox 5945 ||| True True  ||| Present?  True\n",
      "Chicago White Sox 333 ||| True: Chicago White Sox 5945.0 ==? Pred: Chicago White Sox 5945 ||| True True  ||| Present?  True\n",
      "Chicago White Sox 333 ||| True: Chicago White Sox 5945.0 ==? Pred: Chicago White Sox 5945 ||| True True  ||| Present?  True\n",
      "------------------------------------------\n",
      "Predictive Accuracy: 85.714%\n",
      "Answer Present: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to random sentence for manual review\n",
    "rand_sid = np.random.choice(known_true_mentions['sentence_id'])\n",
    "rand_sentence_df = known_true_mentions[known_true_mentions['sentence_id'] == rand_sid]\n",
    "print(f\"Sentence ID: {rand_sid}, Number of Mentions: {len(rand_sentence_df)}\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Calculate accuracy\n",
    "calculate_accuracy(predictions=congruent_predictions_pop_pr, dataframe=rand_sentence_df, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical Flow Demonstration\n",
    "\n",
    "The cells below have been included as a more easily understood logical flow to understand how we designed the recursive congruence algorithm for an arbitrary length of full mentions in a sentence. We manually select a sentence and work through that. This is identical to the above but with more printed out breaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Known Oddities\n",
    "1. Test with sentence_id == 1. We only return unique mentions in a single sentence. Is that ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Germany</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>germany</td>\n",
       "      <td>[11867, 250204, 21212, 12674, 33685, 662281, 1...</td>\n",
       "      <td>[183, 43310, 7318, 43287, 41304, 154408, 12031...</td>\n",
       "      <td>[Germany, Germany_national_football_team, Nazi...</td>\n",
       "      <td>[0.8896856, 0.021721, 0.0153527, 0.0140082, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>European Union</td>\n",
       "      <td>http://en.wikipedia.org/wiki/European_Union</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>European Union</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>european union</td>\n",
       "      <td>[9317, 1933156, 9317, 10890716, 276436, 265743...</td>\n",
       "      <td>[458, 1376407, 458, 185441, 208202, 319328, 36...</td>\n",
       "      <td>[European_Union, European_Boxing_Union, Europe...</td>\n",
       "      <td>[0.9884673, 0.0026042, 0.0011161, 0.0009566, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Werner Zwingmann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>werner zwingmann</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Britain</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>britain</td>\n",
       "      <td>[31717, 13530298, 152256, 158019, 13525, 4721,...</td>\n",
       "      <td>[145, 23666, 174193, 161885, 185103, 8680, 977...</td>\n",
       "      <td>[United_Kingdom, Great_Britain, United_Kingdom...</td>\n",
       "      <td>[0.5200594, 0.2035661, 0.0822639, 0.0451168, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                wikipedia_URL  \\\n",
       "0       B           Germany         http://en.wikipedia.org/wiki/Germany   \n",
       "1       B    European Union  http://en.wikipedia.org/wiki/European_Union   \n",
       "2       B  Werner Zwingmann                                          NaN   \n",
       "3       B           Britain  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            11867.0         Germany            2       0   \n",
       "1             9317.0  European Union            2       0   \n",
       "2                NaN             NaN            2       0   \n",
       "3            31717.0  United Kingdom            2       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [Germany, European Union, Werner Zwingmann, Br...           germany   \n",
       "1  [Germany, European Union, Werner Zwingmann, Br...    european union   \n",
       "2  [Germany, European Union, Werner Zwingmann, Br...  werner zwingmann   \n",
       "3  [Germany, European Union, Werner Zwingmann, Br...           britain   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [11867, 250204, 21212, 12674, 33685, 662281, 1...   \n",
       "1  [9317, 1933156, 9317, 10890716, 276436, 265743...   \n",
       "2                                                 []   \n",
       "3  [31717, 13530298, 152256, 158019, 13525, 4721,...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [183, 43310, 7318, 43287, 41304, 154408, 12031...   \n",
       "1  [458, 1376407, 458, 185441, 208202, 319328, 36...   \n",
       "2                                                 []   \n",
       "3  [145, 23666, 174193, 161885, 185103, 8680, 977...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [Germany, Germany_national_football_team, Nazi...   \n",
       "1  [European_Union, European_Boxing_Union, Europe...   \n",
       "2                                                 []   \n",
       "3  [United_Kingdom, Great_Britain, United_Kingdom...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.8896856, 0.021721, 0.0153527, 0.0140082, 0....  \n",
       "1  [0.9884673, 0.0026042, 0.0011161, 0.0009566, 0...  \n",
       "2                                                 []  \n",
       "3  [0.5200594, 0.2035661, 0.0822639, 0.0451168, 0...  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on manually selected sentence\n",
    "single_sentence_df = predictions[predictions['sentence_id'] == sentence_id].drop_duplicates(['full_mention', 'wikipedia_page_ID', 'sentence_id']).reset_index(drop=True)\n",
    "single_sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Germany', 'European Union', 'Werner Zwingmann', 'Britain']\n"
     ]
    }
   ],
   "source": [
    "# Congruent Mention\n",
    "print(single_sentence_df['congruent_mentions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numerical for easier recursive logic later\n",
    "sentence_mention_nums = np.arange(len(single_sentence_df['congruent_mentions'][0]))\n",
    "sentence_mention_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate lists of vectors\n",
    "def get_candidate_pool_vectors(candidate_pool_titles, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to return entity vectors from Wikipedia2Vec\n",
    "    Takes as input a list of page titles, representing the candidate pool\n",
    "    Normalizes each page title to match necessary input format\n",
    "    Returns entity vector or empty vector if no match\n",
    "    \"\"\"\n",
    "    # Track failed vector queries\n",
    "    no_vector_count = 0\n",
    "    candidate_pool_vectors = []\n",
    "    for candidate in candidate_pool_titles:\n",
    "        candidate = normalize_text(candidate)\n",
    "        try:\n",
    "            candidate_vectors = w2v.get_entity_vector(candidate)\n",
    "        except KeyError:\n",
    "            # Keep empty vector representation to maintain index locations\n",
    "            candidate_vectors = np.zeros(100)\n",
    "            no_vector_count += 1\n",
    "        candidate_pool_vectors.append(candidate_vectors)\n",
    "    \n",
    "    if len(candidate_pool_titles) == 0:\n",
    "        candidate_pool_vectors = [np.zeros(100), np.zeros(100), np.zeros(100)]\n",
    "    \n",
    "    if verbose: print(f\"Failed Wikipedia2Vec Entity Vector Queries: {no_vector_count}\")\n",
    "    return candidate_pool_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n",
      "Failed Wikipedia2Vec Entity Vector Queries: 0\n"
     ]
    }
   ],
   "source": [
    "# Save vectors in dictionary\n",
    "vector_dict = {}\n",
    "\n",
    "# For each full mention we are analyzing in the contextual domain (i.e. sentence)\n",
    "for m in sentence_mention_nums:\n",
    "    \n",
    "    # Retrieve candidate pool titles\n",
    "    candidate_pool_titles = single_sentence_df['candidate_pool_titles'][m]\n",
    "    \n",
    "    # Convert candidate pool titles to candidate pool vectors\n",
    "    candidate_pool_vectors = get_candidate_pool_vectors(candidate_pool_titles, verbose=True)\n",
    "    \n",
    "    # Save candidate pool vectors to dictionary\n",
    "    vector_dict[m] = candidate_pool_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display vector_dict output\n",
    "print(vector_dict.keys())\n",
    "# Preview one candidate vector from a candidate pool vectors\n",
    "vector_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "1 2\n",
      "1 3\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# Calculate congruence metric for each candidate vector for each mention's candidate pool\n",
    "# This notebook uses cosine similarity as the congruence metric\n",
    "\n",
    "## Save congruence measurements in a two-level dictionary\n",
    "# Create first-level dictionary\n",
    "congruence_dict = {}\n",
    "\n",
    "# Always work low numbers to high\n",
    "m = 0\n",
    "while m < len(sentence_mention_nums)-1:\n",
    "    \n",
    "    # Save second-level congruence measurement dictionary\n",
    "    m_dict = {}\n",
    "    # Compare each mention against mentions after it\n",
    "    for n in sentence_mention_nums[m+1:]:\n",
    "        print(m, n)\n",
    "        # Calculate congruence measurement - cosine similarity\n",
    "        congruence_measurement = cosine_similarity(vector_dict[m], vector_dict[n])\n",
    "        # Save congruence measurement to second-level dictionary\n",
    "        m_dict[n] = congruence_measurement\n",
    "    \n",
    "    # Save second-level dictionary to first-level\n",
    "    congruence_dict[m] = m_dict\n",
    "    \n",
    "    # Increment mention\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2])\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Display congruence_dict output\n",
    "# This should be one less than congruent mention count, since we are comparing low to high\n",
    "# and thus don't compare the highest value to anything\n",
    "print(congruence_dict.keys())\n",
    "# Preview congruence matrix derived from comparing Mention 1 to Mention 2\n",
    "for k in congruence_dict.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wikipedia_URL</th>\n",
       "      <th>wikipedia_page_ID</th>\n",
       "      <th>wikipedia_title</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>congruent_mentions</th>\n",
       "      <th>norm_full_mention</th>\n",
       "      <th>candidate_pool_page_ids</th>\n",
       "      <th>candidate_pool_item_ids</th>\n",
       "      <th>candidate_pool_titles</th>\n",
       "      <th>candidate_pool_likelihoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>Germany</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>11867.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>germany</td>\n",
       "      <td>[11867, 250204, 21212, 12674, 33685, 662281, 1...</td>\n",
       "      <td>[183, 43310, 7318, 43287, 41304, 154408, 12031...</td>\n",
       "      <td>[Germany, Germany_national_football_team, Nazi...</td>\n",
       "      <td>[0.8896856, 0.021721, 0.0153527, 0.0140082, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>European Union</td>\n",
       "      <td>http://en.wikipedia.org/wiki/European_Union</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>European Union</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>european union</td>\n",
       "      <td>[9317, 1933156, 9317, 10890716, 276436, 265743...</td>\n",
       "      <td>[458, 1376407, 458, 185441, 208202, 319328, 36...</td>\n",
       "      <td>[European_Union, European_Boxing_Union, Europe...</td>\n",
       "      <td>[0.9884673, 0.0026042, 0.0011161, 0.0009566, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Werner Zwingmann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>werner zwingmann</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Britain</td>\n",
       "      <td>http://en.wikipedia.org/wiki/United_Kingdom</td>\n",
       "      <td>31717.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germany, European Union, Werner Zwingmann, Br...</td>\n",
       "      <td>britain</td>\n",
       "      <td>[31717, 13530298, 152256, 158019, 13525, 4721,...</td>\n",
       "      <td>[145, 23666, 174193, 161885, 185103, 8680, 977...</td>\n",
       "      <td>[United_Kingdom, Great_Britain, United_Kingdom...</td>\n",
       "      <td>[0.5200594, 0.2035661, 0.0822639, 0.0451168, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mention      full_mention                                wikipedia_URL  \\\n",
       "0       B           Germany         http://en.wikipedia.org/wiki/Germany   \n",
       "1       B    European Union  http://en.wikipedia.org/wiki/European_Union   \n",
       "2       B  Werner Zwingmann                                          NaN   \n",
       "3       B           Britain  http://en.wikipedia.org/wiki/United_Kingdom   \n",
       "\n",
       "   wikipedia_page_ID wikipedia_title  sentence_id  doc_id  \\\n",
       "0            11867.0         Germany            2       0   \n",
       "1             9317.0  European Union            2       0   \n",
       "2                NaN             NaN            2       0   \n",
       "3            31717.0  United Kingdom            2       0   \n",
       "\n",
       "                                  congruent_mentions norm_full_mention  \\\n",
       "0  [Germany, European Union, Werner Zwingmann, Br...           germany   \n",
       "1  [Germany, European Union, Werner Zwingmann, Br...    european union   \n",
       "2  [Germany, European Union, Werner Zwingmann, Br...  werner zwingmann   \n",
       "3  [Germany, European Union, Werner Zwingmann, Br...           britain   \n",
       "\n",
       "                             candidate_pool_page_ids  \\\n",
       "0  [11867, 250204, 21212, 12674, 33685, 662281, 1...   \n",
       "1  [9317, 1933156, 9317, 10890716, 276436, 265743...   \n",
       "2                                                 []   \n",
       "3  [31717, 13530298, 152256, 158019, 13525, 4721,...   \n",
       "\n",
       "                             candidate_pool_item_ids  \\\n",
       "0  [183, 43310, 7318, 43287, 41304, 154408, 12031...   \n",
       "1  [458, 1376407, 458, 185441, 208202, 319328, 36...   \n",
       "2                                                 []   \n",
       "3  [145, 23666, 174193, 161885, 185103, 8680, 977...   \n",
       "\n",
       "                               candidate_pool_titles  \\\n",
       "0  [Germany, Germany_national_football_team, Nazi...   \n",
       "1  [European_Union, European_Boxing_Union, Europe...   \n",
       "2                                                 []   \n",
       "3  [United_Kingdom, Great_Britain, United_Kingdom...   \n",
       "\n",
       "                          candidate_pool_likelihoods  \n",
       "0  [0.8896856, 0.021721, 0.0153527, 0.0140082, 0....  \n",
       "1  [0.9884673, 0.0026042, 0.0011161, 0.0009566, 0...  \n",
       "2                                                 []  \n",
       "3  [0.5200594, 0.2035661, 0.0822639, 0.0451168, 0...  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "# Pick mentions to compare\n",
    "men1 = np.random.choice(sentence_mention_nums[:-1])\n",
    "men2 = np.random.choice(sentence_mention_nums)\n",
    "while men1 == men2:\n",
    "    men2 = np.random.choice(sentence_mention_nums)\n",
    "men1, men2 = np.sort([men1, men2])\n",
    "print(men1, men2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_8f508dee_35ae_11eb_bf35_acde48001122row0_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row0_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row0_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row1_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row1_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row1_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row2_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row2_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row2_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row3_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row3_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row3_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row4_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row4_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row4_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row5_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row5_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row5_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row6_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row6_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row6_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row7_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row7_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row7_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row8_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row8_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row8_col2,#T_8f508dee_35ae_11eb_bf35_acde48001122row9_col0,#T_8f508dee_35ae_11eb_bf35_acde48001122row9_col1,#T_8f508dee_35ae_11eb_bf35_acde48001122row9_col2{\n",
       "            background:  skyblue;\n",
       "        }</style><table id=\"T_8f508dee_35ae_11eb_bf35_acde48001122\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row0_col2\" class=\"data row0 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row3_col0\" class=\"data row3 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row3_col1\" class=\"data row3 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row3_col2\" class=\"data row3 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row4_col0\" class=\"data row4 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row4_col1\" class=\"data row4 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row4_col2\" class=\"data row4 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row5_col0\" class=\"data row5 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row5_col1\" class=\"data row5 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row5_col2\" class=\"data row5 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row6_col0\" class=\"data row6 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row6_col1\" class=\"data row6 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row6_col2\" class=\"data row6 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row7_col0\" class=\"data row7 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row7_col1\" class=\"data row7 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row7_col2\" class=\"data row7 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row8_col0\" class=\"data row8 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row8_col1\" class=\"data row8 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row8_col2\" class=\"data row8 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8f508dee_35ae_11eb_bf35_acde48001122level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row9_col0\" class=\"data row9 col0\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row9_col1\" class=\"data row9 col1\" >0.000000</td>\n",
       "                        <td id=\"T_8f508dee_35ae_11eb_bf35_acde48001122row9_col2\" class=\"data row9 col2\" >0.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f8bbee74f50>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congruence_test_df = pd.DataFrame(congruence_dict[men1][men2])\n",
    "max_num = max(np.max(congruence_test_df))\n",
    "congruence_test_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-2ee20f6677ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmatrix_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmatrix_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmax_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmatrix_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"background: skyblue\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_num\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "matrix_test = []\n",
    "for a in single_sentence_df['candidate_pool_priors'][men1]:\n",
    "    row_test = []\n",
    "    for b in single_sentence_df['candidate_pool_priors'][men2]:\n",
    "        row_test.append(np.mean([a, b]))\n",
    "    matrix_test.append(row_test)\n",
    "matrix_df = pd.DataFrame(matrix_test)\n",
    "max_num = max(np.max(matrix_df))\n",
    "matrix_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,0) (10,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-fe69072df3f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweighted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcongruence_test_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mweighted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweighted_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"background: skyblue\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_num\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,0) (10,3) "
     ]
    }
   ],
   "source": [
    "weighted_df = np.array(matrix_df) * np.array(congruence_test_df)\n",
    "weighted_df = pd.DataFrame(weighted_df)\n",
    "max_num = max(np.max(weighted_df))\n",
    "weighted_df.style.apply(lambda x: [\"background: skyblue\" if v == max_num else \"\" for v in x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  4\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "Length:  4\n",
      "1 2\n",
      "1 3\n",
      "Length:  4\n",
      "2 3\n",
      "Length:  4\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of logic to ensure unique mention congruence only calculated once\n",
    "m = 0\n",
    "while m < len(sentence_mention_nums):\n",
    "    print(\"Length: \", len(sentence_mention_nums))\n",
    "    for i in sentence_mention_nums[m+1:]:\n",
    "        print(m, i)\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 1), 0.8189154129945448)\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 2), 0.6412995855581739)\n",
      "Comparing 1 & 2\n",
      "Congruent Pair:  ((1, 1), 0.6947784687002414)\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((1, 1), 0.6947784687002414), 1, 2)\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "most_congruent_pair = (None, 0.0, 0, 0) # Most Congruent Candidates, Congruence Metric, Mention A, Mention B\n",
    "\n",
    "for m in sentence_mention_nums:\n",
    "    for n in sentence_mention_nums[m+1:]:\n",
    "        print(f\"Comparing {m} & {n}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m][n])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, m, n\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save max congruent pair estimates for each mention\n",
    "mention_predictions = {}\n",
    "mention_predictions[most_congruent_pair[1]] = most_congruent_pair[0][0][0]\n",
    "mention_predictions[most_congruent_pair[2]] = most_congruent_pair[0][0][1]\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have predictions for  dict_keys([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# With two mentions set in their predictions, we must filter the other congruent matrices to find the next most\n",
    "print(\"We have predictions for \", mention_predictions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(sentence_mention_nums) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 0 & 1\n",
      "Congruent Pair:  ((3, 7), 0.7314480614025645)\n",
      "MAX: (((None, None), 0.0), (0, 0))\n",
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "MAX: (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 0 & 2\n",
      "Congruent Pair:  ((1, 1), 0.8189154129945448)\n",
      "MAX: (((3, 7), 0.7314480614025645), (0, 1))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "MAX: (((1, 1), 0.8189154129945448), (0, 2))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((1, 1), 0.8189154129945448), (0, 2))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1, 0: 1}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 1 & 3\n",
      "Congruent Pair:  ((0, 4), 0.6645409154870272)\n",
      "MAX: (((None, None), 0.0), (0, 0))\n",
      "Comparing 2 & 3\n",
      "Congruent Pair:  ((0, 6), 0.78401095)\n",
      "MAX: (((0, 4), 0.6645409154870272), (1, 3))\n",
      "Comparing 0 & 3\n",
      "Congruent Pair:  ((0, 2), 0.6412995855581739)\n",
      "MAX: (((0, 6), 0.78401095), (2, 3))\n",
      "*****************************************\n",
      "Max Congruent Pair:  (((0, 6), 0.78401095), (2, 3))\n"
     ]
    }
   ],
   "source": [
    "## Analyze congruence matrices to identify the most similar pair\n",
    "\n",
    "# (Candidate Pair, Congruence), (Mention A, Mention B) \n",
    "most_congruent_pair = (((None, None), 0.0), (0, 0))\n",
    "\n",
    "for m in mention_predictions.keys():\n",
    "    for n in mentions_remaining:\n",
    "        \n",
    "        # Because we always assume search small mention to large, must sort order\n",
    "        # Update incrementing variables to be in increasing order\n",
    "        m_tmp, n_tmp = np.sort((m, n))\n",
    "        \n",
    "        print(f\"Comparing {m_tmp} & {n_tmp}\")\n",
    "        congruent_pair = get_most_congruent_pair(congruence_dict[m_tmp][n_tmp])\n",
    "        print(\"Congruent Pair: \", congruent_pair)\n",
    "        print(\"MAX:\", most_congruent_pair)\n",
    "        if congruent_pair[1] > most_congruent_pair[0][1]:\n",
    "            # Save most congruent candidate pair and mentions\n",
    "            most_congruent_pair = congruent_pair, (m_tmp, n_tmp)\n",
    "            \n",
    "print(\"*****************************************\")\n",
    "print(\"Max Congruent Pair: \", most_congruent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index location of the mention we are predicting for, the one that branches off the prior predicted mentions\n",
    "# AKA the one that isn't in the prediction keys\n",
    "new_mention_int = most_congruent_pair[1].index(list(set(most_congruent_pair[1]) - set(mention_predictions.keys())))\n",
    "new_mention_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return candidate from candidate pair using that index position\n",
    "most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save that mention and that candidate to the predictions dict\n",
    "mention_predictions[most_congruent_pair[1][new_mention_int]] = most_congruent_pair[0][0][new_mention_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 1, 0: 1, 3: 6}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what we got done\n",
    "mention_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate remaining mentions to search\n",
    "mentions_remaining = list(set(mentions_remaining) - set(mention_predictions.keys()))\n",
    "mentions_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've predicted everything!\n"
     ]
    }
   ],
   "source": [
    "if len(mentions_remaining) == 0:\n",
    "    print(\"You've predicted everything!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
